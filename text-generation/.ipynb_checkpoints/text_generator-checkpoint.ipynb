{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Updates after discussion\n",
    "\n",
    "* TODO:\n",
    "    * Clean this section and save comments for report\n",
    "    * clear doubts about network. Give credit to blog posts whenever choices are not clear\n",
    "    \n",
    "\n",
    "* cleaning the 5% least frequent symbols from the vocab was a disaster since several steps can't handle unknown words. It's not impossible to fix this, but for the time being it's easier to work with the full corpus (~115k sentences instead of ~20k) and assume that this will make the weird symbols/words waaay less likely to appear. It's not an unfounded assumption (denke ich).\n",
    "\n",
    "* when expanding the input_words set I won't include antonyms. I think syn, hyper and hyponyms are enough, and an incorrectly used antonym is riskier.\n",
    "\n",
    "* should we also leave out hyponyms?\n",
    "\n",
    "* top 3 seeds by freq of appearance in the corpus \"is\" >> \"means\" >> \"can be found\"\n",
    "\n",
    "* if part i of the sentence is already covered (includes one of the input words), any part i+1 will take part i into account while generating. Doesn't sound exciting but it is a smart feature :P\n",
    "\n",
    "* Now running on all trigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Generating our descriptive sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Importing everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#is this the best choice of autograd?\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg #modified version that only returns the set of MW and TWs, not the nice drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Getting our text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "#we will be using the full version of the descriptive corpus we made ~115k sentences\n",
    "f = open('description-corpus-115k.txt', 'r')\n",
    "#text = f.read()\n",
    "text = f.readlines() #if we want a list with sentences as elements\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#getting the avg length of a descriptive sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Cleaning (NOT USED)\n",
    "* Removing stop words, punctuation symbols and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# found that in some tutorials they do 3 extra cleaning steps before applying N-grams\n",
    "\n",
    "# getting rid of stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_free = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "# justification --> it's like getting rid of the long tail in the word frequency plot. Oder?\n",
    "\n",
    "# getting rid of punctuation\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "#punct_free = \"\".join(word for word in stop_free if word not in punctuation_symbols)\n",
    "# makes sense.. I think\n",
    "\n",
    "# lemmatizing?\n",
    "#lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#normalized = ' '.join(lemma.lemmatize(word) for word in punct_free.split())\n",
    "#if it is what I think it is, then it makes sense too\n",
    "\n",
    "#last step, lower case and splitting\n",
    "#cleaned_text = normalized.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stop_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "punct_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "normalized[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Only normalizing (NOT USED)\n",
    "* is it worth it having different tokens for cookie and cookies (in terms of having a correct sentence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Implementing trigrams and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating trigram sets EASY WAY\n",
    "#trigrams = [([cleaned_text[i], cleaned_text[i+1]], cleaned_text[i+2])for i in range(len(cleaned_text) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# better way: sentence by sentence, not the whole text in one go\n",
    "# this structure allows us to create context/target sets for each word. \n",
    "trigrams = []\n",
    "for sentence in sentences:\n",
    "    trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['it', 'does'], 'not')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[0]\n",
    "#context for target word 'text' --> 'a' and 'cookie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2921828"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#using all trigrams led to kernel death every time\n",
    "# we will randomly sample 100000 of them\n",
    "trigrams = random.sample(trigrams, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting set of words in vocab, it's length and the frequency of each word\n",
    "# our vocab consists of the words appearing in trigrams, so no need to take the vocab over the whole text\n",
    "# if we are not using all trigrams\n",
    "voc = set()\n",
    "for tri in trigrams:\n",
    "    voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "voc_length = len(voc) #34174\n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16506"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#the detected some errors on how our corpus is tokenized (\"binoculars\" is imported as bi - no -cu-lars) we will remove the 5% less frequent found words in the dictionary.\n",
    "# THIS AFFECTED THE CELL BELOW. INSTEAD OF USING THIS CODE TO CLEAN THE POSSIBLE OUTPUTS WE WILL ASSUME THAT USING OUR BIGGER VERSION OF THE CORPUS SHOULD MINIMIZE \n",
    "# THE LIKELIHOOD OF FINDING THESE WEIRD TOKENS DURING THE GENERATION STEP\n",
    "\n",
    "\n",
    "# freq values from dictionary to array\n",
    "#freq_values = np.array([v for k, v in word_to_freq.items()])\n",
    "#np.percentile(freq_values, 5) # ~1709\n",
    "\n",
    "#getting rid of those unusual values\n",
    "# creating list of keys to delete\n",
    "#to_del = [k for k, v in word_to_freq.items() if v < 1709]\n",
    "\n",
    "# deleting those elements from the word_to_freq dict\n",
    "#for k in to_del: del word_to_freq[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to inp\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bad, training on CPU; dont exagerate with number of epochs.\n"
     ]
    }
   ],
   "source": [
    "#Cheking if we have access to training on GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Fancy setup!')\n",
    "else: \n",
    "    print('Too bad, training on CPU; dont exagerate with number of epochs.')\n",
    "\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train(context, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(trigrams)):\n",
    "        output, hidden = decoder(context[t], hidden)\n",
    "        loss += criterion(output, target[t])\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data.item() / len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30m 22s (10 5%) 6.6195]\n",
      "[60m 3s (20 10%) 6.1644]\n",
      "[88m 50s (30 15%) 5.7134]\n",
      "[117m 50s (40 20%) 5.2620]\n",
      "[151m 39s (50 25%) 4.8277]\n",
      "[196m 39s (60 30%) 4.4122]\n",
      "[246m 32s (70 35%) 4.0158]\n",
      "[295m 9s (80 40%) 3.6429]\n",
      "[331m 5s (90 45%) 3.3166]\n",
      "[380m 12s (100 50%) 3.0145]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "print_every = 10\n",
    "plot_every = 10\n",
    "hidden_s = 75\n",
    "n_layers = 2\n",
    "lr = 0.015\n",
    "\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(cont,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# saving the model\n",
    "# instructions found here: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "#saving model for inference --> save state_dict\n",
    "path1 = os.getcwd()+'/test4_trained_inference.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(),path1)\n",
    "#to load\n",
    "# decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "# decoder.load_stat_dict(torch.load(path))\n",
    "# decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlpa/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "#saving entire model\n",
    "path2 = os.getcwd()+'/test4_trained1_entire.pt'\n",
    "torch.save(decoder, path1)\n",
    "\n",
    "#loading\n",
    "# Model class must be defined somewhere\n",
    "#decoder = torch.load(path)\n",
    "#decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "f= open('test1_losses.csv', 'w+')\n",
    "f.write(str(all_losses)[1:-1])\n",
    "f.close()\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.savefig('test_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in prime_str.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main way . made wearing treated the 10 trigger butt types\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('the main', 10, temperature = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Generating descriptive sentence\n",
    "* input = main word + taboo words\n",
    "\n",
    "The main idea here is that we will seed always with the main word and replace at the end.\n",
    "The descriptive sentence will be accepted into the cleaning step (where we will make sure that no tws or the mw were used and if so, we'll replace them) if it contains at least two of the input words, besides the seeds. As long as the sentence is not accepted, we will keep generating.\n",
    "\n",
    "To help reach a high score: once an input word has been used and its score_vector value increases, we will leave that segment in our sentence. Not sure if this hurts more than it helps, but adding the input word as a seed is VERY complicated. And not sure it would make sense either.\n",
    "\n",
    "For a first prototype we will assume that maximum one new word is gonna be added per iteration. \n",
    "\n",
    "Pretty fragile with many assumptions?\n",
    "\n",
    "It would be useful to generate a list of synonyms of all input words to expand the input_words set and have higher acceptance rate. We thought about cleaning the input words set from the start but those are words with high probability of occurence so we better leave them and clean in the end.\n",
    "\n",
    "* maybe more seeds?\n",
    "* maybe require a higher score?\n",
    "* how to properly connect end of sentences and seeds? Is it better to have fewer seeds but longer auto-generated text?\n",
    "* add time it to report results and \"quantify\" results/improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Description generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def gen_input_words(mw, model):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    \n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but the loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab. Shouldn't be a thing when using larger corpus\n",
    "    input_words = [word for word in input_words if word in voc]    \n",
    "    return input_words\n",
    "\n",
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    #n_seeds = if we are using 2 or 3 seeds during the sentence generation step\n",
    "    #n_iterations = how many iterations we will do in the generation step\n",
    "    #debugging = True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "    #printing = True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    \n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([evaluate(mw+' means', 7, temperature = 1), evaluate(mw+' is', 7, temperature = 1), evaluate(mw+' can be found', 5, temperature = 1)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the TW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        while i < n_iterations:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1)\n",
    "                    sentence_parts[2] = evaluate(mw+' can be found', 5, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 7, temperature = 1)\n",
    "                    sentence_parts[2] = evaluate(mw+' can be found', 5, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1)\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 7, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 7, temperature = 1) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  evaluate(sentence_parts[0]+' '+ mw+' is', 7, temperature = 1) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1) +' '\n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = evaluate(mw+' can be found', 5, temperature = 1)\n",
    "                    else:\n",
    "                        temp =  evaluate(sentence_parts[1]+' '+ mw+' can be found', 5, temperature = 1) +' '\n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([evaluate(mw+' means', 11, temperature = 1), evaluate(mw+' is', 12, temperature = 1)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the TW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = evaluate(mw+' is', 12, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 11, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 11, temperature = 1) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  evaluate(sentence_parts[0]+' '+ mw+' is', 12, temperature = 1) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1) +' '\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#description_generator('cake', model, n_seeds = 3, n_iterations = 2, debugging = False, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Cleaning the generated sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def sentence_cleaner(sentence, mw, model):\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, 'The main word')\n",
    "    \n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def final_output(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    sentence = description_generator(mw, model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, model)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number: 1\n",
      "cake means an attorney offense to the final interests in cleaning  cake is a justified looks like ? think called \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means peace out out as a stain blue last i 're still  cake is something advancement los personal melt of the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means 2002 book - for peer payer and clips or a production  cake is tied 24 women than points countries was \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means notice a priest ( files unreadable w.h. smaller w.h. w. cake is useless a zapatistas is a violinist of \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means see beings , built speeds lee methamphetamines as benef cake is the concrete of simply represents not heathrow \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means see worn that secure lucky rubbishier in bed , \" primar cake is benefited misallocation target at relation to such \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means we account from ay to be design if a person is  cake is contracts to your will gmt in such \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means anybody on slavery fund that pop up tent ample against  cake is oil as a collection of . i \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means frames : asimov those inviting . ) why \" others they  cake is baloney station of a linear is not \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means volume love treating impressed most stealing for. visit cake is penalized for exactly fun sleeps agreement behaviour \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means served matters period does sneak lived state is applyin cake is specialization adequately to appreciate top misrepresentat\n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means care might that become thierry a donkey cycle tricks th cake is quarks that the hmrc requirements help educational \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means excellence \" on they worsen harm quake boxers from play cake is a self-employed horse linked of you proves \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means , and an treatment 's been members tune for aggressivel cake is necessary to be a label was an \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means to interrupt what elsewhere ) every product one creatur cake is a psychological career november instead of the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means time in the jury different ; i am black but flesh  cake is something ' union internal the externally of \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 17\n",
      "cake means accepting not order to see kessinger liable her while a cake is suggest open portion success located ; the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 18\n",
      "cake means it > ... opening graceful feature . ) by . )  cake is a whole time ? 'd over resources \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 19\n",
      "cake means i \" chemically ( hub ) from / reads the gays  cake is a meal is a title is the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 20\n",
      "cake means when it monster law and village y constituted palestini cake is enough constructed just are what either more \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 21\n",
      "cake means necessarily 0.0001 located in long , where as a felix i cake is postponed seo a botnet are actually looking \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 22\n",
      "cake means instructed wrong ) ( ( cloths 1000 and the well-trodden cake is defined like wanting . by the disposal \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 23\n",
      "cake means anthologies what a transfer tube can was failure , my i cake is bru years you continually indubitable replicator payback \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 24\n",
      "cake means alas i also focused job that items had history and a  cake is catecholamine sorry one has i were driven \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 25\n",
      "cake means screw back-formate is still worshiping russia . involvi cake is a wicked or humans would turn in \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 26\n",
      "cake means locations she reach and these ( acts as 13 apprentice f cake is considering comparable outside her inside to and \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 27\n",
      "cake means seen noted that the wedding to mallards one products .  cake is a day is not a bee was \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 28\n",
      "cake means excellence control , and a leader is an individual . )  cake is a constituent meant anywhere 's section ) \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 29\n",
      "cake means who cannot you true . at this wonderful - ways .  cake is honest modifies emergency something is an community \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 30\n",
      "cake means instructed holy declares personal paintings solutions d cake is secrete plans , used repairing being just \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 31\n",
      "cake means contact acorn just cold out and substance going from a  cake is ok your unique or meant than light \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 32\n",
      "cake means or a single-quoted sounds generators write as a lawn me cake is a inner that has financial worsening caia \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 33\n",
      "cake means at weed 2369 convince the budget 1189 and an easy is  cake is gratitude or ones drop causing , interests \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 34\n",
      "cake means like the greatest aids-infected and other half create n cake is traced coherence pepper mothers of that history \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 35\n",
      "cake means generators laws but were find tokyo money authoritative cake is entrusted , if my time , and \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 36\n",
      "cake means clad things they are n't want to empowered instead and  cake is adolescents peace , virginia ago without need \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 37\n",
      "cake means it bureau . never the house prior knowledge to deny the cake is speech-sound though alone . according -884 creditors \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 38\n",
      "cake means they can continuity that almost attributes , connecting cake is conducted . ) our frame and part \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 39\n",
      "cake means issued a way is one . ) what a 750 are  cake is reported well elon party from a printing \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 40\n",
      "cake means peace managed in a circle yard . ) how then drawn  cake is reported all into what a possession is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 41\n",
      "cake means instructed into learn the first thing tune flatteringly cake is rarely well of accessing that smart a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 42\n",
      "cake means care from there dose left news and explains i looking t cake is more utd of a causal ' was \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 43\n",
      "cake means peter existed in the sake of the real organ warranty ev cake is iran passageway found to add doubt an \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 44\n",
      "cake means boring especialy diseases cannot differentiate that eve cake is both operatives happiness school consideration have a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 45\n",
      "cake means messed members and declaration that and yes this met in cake is catecholamine ad belief we or device , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 46\n",
      "cake means workspaces flatteringly socialist allocated diving valo cake is a fortnight is a private regular framework \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 47\n",
      "cake means than them in the crops , a long shape , and  cake is mostly provide the rule , albeit called \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 48\n",
      "cake means inquisitive him , and unfortunately scarier , demonstra cake is adequately a moratorium is related steel sent \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 49\n",
      "cake means stop malaise propagation pressures ; learn starlin \" pu cake is harmed lot , health what an outlet \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 50\n",
      "cake means they must are , and she would love to a minor  cake is height as a part obstacle prices offense \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 51\n",
      "cake means all flight give in social around gloss . + 6 areas  cake is precisely while or your relationship for coalitions \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 52\n",
      "cake means to the line do an adult striped is the s. keys  cake is astrazeneca promote made mythology comment upon completely\n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 53\n",
      "cake means identify a moratorium . vi its defense in preposition b cake is repairing and hundreds ( nervus states consider \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 54\n",
      "cake means hurtful happen in that 's must avoid to an opportunity  cake is worth joy , to do it is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 55\n",
      "cake means notice there without provided like out by the tensor qu cake is a buyers - 100 part of the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 56\n",
      "cake means what grasp mee the overwhelming pleased rather , \" with cake is a minister last program . know something \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 57\n",
      "cake means 46 money size active credentials who could do invade al cake is eagles something this well and a sit \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 58\n",
      "cake means a game * serial duty ago \" for to be right  cake is a throne set ? without finally police \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 59\n",
      "cake means them to ' willing to an object is hildegard being a  cake is wanting to morning . before the circumstances \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 60\n",
      "cake means indiscriminately stated typos ground axis money rival , cake is a visualization is adequately a single-quoted is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 61\n",
      "cake means people have learning in events of a downgrade are regro cake is baloney curriculum , hashtagblank or quick from \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 62\n",
      "cake means limited brothels on wanting , it can want to for the  cake is a survey infectious have are a recording \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 63\n",
      "cake means tells their character is a caravan of that four thing s cake is 365 rapist that your club ) is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 64\n",
      "cake means innumerable giving sixteen and circles of like did your cake is reinforcements social ] referral to calm the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 65\n",
      "cake means a bee was like looking endured back consciousness traff cake is run holland her link reality as off \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 66\n",
      "cake means ... also facts : taking a readability is a collection o cake is advisable point physically energy to a bottle \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 67\n",
      "cake means held yet like the story of eternal overwhelmed is the s cake is a little file like action that that \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 68\n",
      "cake means notice an organization principle . write the same clubs cake is miracles present an beach . string clinical \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 69\n",
      "cake means by robbed minds up saved her 14 daly \" infection reject cake is a makhid looks worn . . you \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 70\n",
      "cake means specifically to clearly seen ; black but ' is a rite  cake is a cancer is tomorrow , built before \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 71\n",
      "cake means tune to call spend utilize to the gods colours seeing s cake is specialization not incurred which are told all \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 72\n",
      "cake means umpteen won that my down the fact . ) giving ,  cake is not adequately money developments . ) when \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 73\n",
      "cake means ka n't turn expected you do n't think infancy equiped d cake is unsigned utilitarianism whether exactly energy that petty \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 74\n",
      "cake means frames : the abortion - which until irritating and vari cake is to engage - years to consulting joy \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 75\n",
      "cake means notice 20 menus matches of the specific p of a way  cake is maturity of worship and clan injury learn \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 76\n",
      "cake means employment borrowing : proved before shoot to the burde cake is every charity as an clattering is not \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 77\n",
      "cake means its point , being security debunking phone labor . ) yo cake is artefacts most period implied and by work \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 78\n",
      "cake means to attract receive shy . , by you have around available cake is surpassed ... i adult too to feel \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 79\n",
      "cake means reaches the discourse firm i by all again . ) need  cake is clear told its fine lozz me , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 80\n",
      "cake means diseases across his igd , joint or due , so \"  cake is secrete the entrance - other not an \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 81\n",
      "cake means teams idea . here remember secure can something others  cake is a assertion looking because until you 862300 \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 82\n",
      "cake means out - a war kirby or . obligation to be achived  cake is connexion a opportunities can be found innocent \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 83\n",
      "cake means married : now something copy or was terms to pass the  cake is a p-conic -- or effects court may \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 84\n",
      "cake means fill n't they think to street the white structure as so cake is violence , insteid or less violently do \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 85\n",
      "cake means uttaranchal energy accommodation : association sbs . )  cake is specialization a rhythm of the termination been \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 86\n",
      "cake means shades about real thought me . ) notarial to mistakenly cake is stds ' very long books is an \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 87\n",
      "cake means all at pulling to , proof , but no feedback idea  cake is a hippie loving a moratorium of n't \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 88\n",
      "cake means to attract check times - met. most role south charge an cake is a waterbus is a fortnight hatch when \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 89\n",
      "cake means grievous to an user obstacle between in most kingdom mi cake is a guidance , rarely had a mortgage \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 90\n",
      "cake means when pieces is rare to it on some minutes as double  cake is losses . - 3 can ask anyone \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 91\n",
      "cake means energy giving . work your ms own clueless appearance bu cake is a cancer . using storage referred is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 92\n",
      "cake means a collection of read in . - course itself , or  cake is a patsy lb would have additional required \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 93\n",
      "cake means seek christianity merely me , repositioned is a novel i cake is a bit or embarrassment guards use , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 94\n",
      "cake means providers tempting pretensions laboratories homes pre-p cake is a concept is not a position is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 95\n",
      "cake means notice on 504 in tc to sleep relatives and yes those  cake is spells anesthesia were past element sensors before \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 96\n",
      "cake means knew appalled are one that with no appearance that at s cake is homer to have an adjective \" example \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 97\n",
      "cake means everybody intersect impossible to attract a door-to-doo cake is margaret made anything different culture can see \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 98\n",
      "cake means to appeal : a lot . for a digit sounds like  cake is much single vulnerability to that from solemnly \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 99\n",
      "cake means say in the case that that been exams religion-islam 's  cake is transmitted crack cotton part provided publications busy \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 100\n",
      "cake means notice reads which has to give over only the soybean oc cake is cancelled a nursing technical class of these \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means notice reads which has to give over only the soybean oc The main word is cancelled a nursing technical class of these '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', model = model, n_seeds=2, n_iterations = 100, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
