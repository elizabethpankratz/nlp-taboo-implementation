{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Updates after discussion\n",
    "\n",
    "* TODO:\n",
    "    * Clean this section and save comments for report\n",
    "    * clear doubts about network. Give credit to blog posts whenever choices are not clear\n",
    "    \n",
    "\n",
    "* cleaning the 5% least frequent symbols from the vocab was a disaster since several steps can't handle unknown words. It's not impossible to fix this, but for the time being it's easier to work with the full corpus (~115k sentences instead of ~20k) and assume that this will make the weird symbols/words waaay less likely to appear. It's not an unfounded assumption (denke ich).\n",
    "\n",
    "* still doesn't support unknown words\n",
    "\n",
    "* careful when using vocab size for inference since we didn't take care of weird symbols in corpus. They might affect it a lot.\n",
    "\n",
    "* when expanding the input_words set I won't include antonyms. I think syn, hyper and hyponyms are enough, and an incorrectly used antonym is riskier.\n",
    "\n",
    "* should we also leave out hyponyms?\n",
    "\n",
    "* top 3 seeds by freq of appearance in the corpus \"is\" >> \"means\" >> \"can be found\"\n",
    "\n",
    "* if part i of the sentence is already covered (includes one of the input words), any part i+1 will take part i into account while generating. Doesn't sound exciting but it is a smart feature :P\n",
    "\n",
    "* Now running on all trigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Generating our descriptive sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Importing everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "#is this the best choice of autograd?\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg #modified version that only returns the set of MW and TWs, not the nice drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Getting our text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "#we will be using the full version of the descriptive corpus we made ~115k sentences\n",
    "f = open('description-corpus-115k.txt', 'r')\n",
    "#text = f.read()\n",
    "text = f.readlines() #if we want a list with sentences as elements\n",
    "f.close()\n",
    "\n",
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]\n",
    "\n",
    "#getting the avg length of a descriptive sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Cleaning (NOT USED)\n",
    "* Removing stop words, punctuation symbols and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# found that in some tutorials they do 3 extra cleaning steps before applying N-grams\n",
    "\n",
    "# getting rid of stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_free = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "# justification --> it's like getting rid of the long tail in the word frequency plot. Oder?\n",
    "\n",
    "# getting rid of punctuation\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "#punct_free = \"\".join(word for word in stop_free if word not in punctuation_symbols)\n",
    "# makes sense.. I think\n",
    "\n",
    "# lemmatizing?\n",
    "#lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#normalized = ' '.join(lemma.lemmatize(word) for word in punct_free.split())\n",
    "#if it is what I think it is, then it makes sense too\n",
    "\n",
    "#last step, lower case and splitting\n",
    "#cleaned_text = normalized.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stop_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "punct_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "normalized[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Implementing trigrams and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating trigram sets EASY WAY\n",
    "#trigrams = [([cleaned_text[i], cleaned_text[i+1]], cleaned_text[i+2])for i in range(len(cleaned_text) - 2)]\n",
    "\n",
    "# better way: sentence by sentence, not the whole text in one go\n",
    "# this structure allows us to create context/target sets for each word. \n",
    "trigrams = []\n",
    "for sentence in sentences:\n",
    "    trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]\n",
    "\n",
    "trigrams[0]\n",
    "#context for target word 'text' --> 'a' and 'cookie'\n",
    "\n",
    "len(trigrams)\n",
    "\n",
    "#using all trigrams led to kernel death every time\n",
    "# we will randomly sample 100000 of them\n",
    "trigrams = random.sample(trigrams, 50000)\n",
    "\n",
    "# getting set of words in vocab, it's length and the frequency of each word\n",
    "# our vocab consists of the words appearing in trigrams, so no need to take the vocab over the whole text\n",
    "# if we are not using all trigrams\n",
    "voc = set()\n",
    "for tri in trigrams:\n",
    "    voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "voc_length = len(voc) #34174\n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}\n",
    "\n",
    "voc_length\n",
    "\n",
    "#the detected some errors on how our corpus is tokenized (\"binoculars\" is imported as bi - no -cu-lars) we will remove the 5% less frequent found words in the dictionary.\n",
    "# THIS AFFECTED THE CELL BELOW. INSTEAD OF USING THIS CODE TO CLEAN THE POSSIBLE OUTPUTS WE WILL ASSUME THAT USING OUR BIGGER VERSION OF THE CORPUS SHOULD MINIMIZE \n",
    "# THE LIKELIHOOD OF FINDING THESE WEIRD TOKENS DURING THE GENERATION STEP\n",
    "\n",
    "\n",
    "# freq values from dictionary to array\n",
    "#freq_values = np.array([v for k, v in word_to_freq.items()])\n",
    "#np.percentile(freq_values, 5) # ~1709\n",
    "\n",
    "#getting rid of those unusual values\n",
    "# creating list of keys to delete\n",
    "#to_del = [k for k, v in word_to_freq.items() if v < 1709]\n",
    "\n",
    "# deleting those elements from the word_to_freq dict\n",
    "#for k in to_del: del word_to_freq[k]\n",
    "\n",
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to inp\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bad, training on CPU; dont exagerate with number of epochs.\n"
     ]
    }
   ],
   "source": [
    "#Cheking if we have access to training on GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Fancy setup!')\n",
    "else: \n",
    "    print('Too bad, training on CPU; dont exagerate with number of epochs.')\n",
    "\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train(context, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(trigrams)):\n",
    "        output, hidden = decoder(context[t], hidden)\n",
    "        loss += criterion(output, target[t])\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data.item() / len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71m 49s (10 5%) 5.3125]\n",
      "[143m 29s (20 10%) 3.8183]\n",
      "[214m 11s (30 15%) 2.6607]\n",
      "[286m 1s (40 20%) 1.8293]\n",
      "[361m 0s (50 25%) 1.2669]\n",
      "[436m 37s (60 30%) 0.8885]\n",
      "[507m 48s (70 35%) 0.6325]\n",
      "[587m 5s (80 40%) 0.4578]\n",
      "[674m 23s (90 45%) 0.3357]\n",
      "[753m 30s (100 50%) 0.2497]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "print_every = 10\n",
    "plot_every = 10\n",
    "hidden_s = 150\n",
    "n_layers = 1\n",
    "lr = 0.015\n",
    "\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(cont,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# saving the model\n",
    "# instructions found here: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "#saving model for inference --> save state_dict\n",
    "path1 = os.getcwd()+'/test5_trained_inference.pt'\n",
    "\n",
    "torch.save(decoder.state_dict(),path1)\n",
    "\n",
    "#to load\n",
    "# decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "# decoder.load_stat_dict(torch.load(path))\n",
    "# decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlpa/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "#saving entire model\n",
    "path2 = os.getcwd()+'/test5_trained_entire.pt'\n",
    "torch.save(decoder, path1)\n",
    "\n",
    "#loading\n",
    "# Model class must be defined somewhere\n",
    "#decoder = torch.load(path)\n",
    "#decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#in case we forgot to set a seed for the trigram sampling step, we can save the current object with pickle\n",
    "#with open(\"trigrams_test5.txt\", \"wb\") as fp:\n",
    "#    pickle.dump(trigrams, fp)\n",
    "\n",
    "#with open(\"trigrams_test5.txt\", \"rb\") as fp:\n",
    "#   trigrams = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "f= open('test5_losses.csv', 'w+')\n",
    "f.write(str(all_losses)[1:-1])\n",
    "f.close()\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.savefig('test_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in prime_str.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main of the casting that going to citi familiar him next\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('the main', 10, temperature = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Generating descriptive sentence\n",
    "* input = main word + taboo words\n",
    "\n",
    "The main idea here is that we will seed always with the main word and replace at the end.\n",
    "The descriptive sentence will be accepted into the cleaning step (where we will make sure that no tws or the mw were used and if so, we'll replace them) if it contains at least two of the input words, besides the seeds. As long as the sentence is not accepted, we will keep generating.\n",
    "\n",
    "To help reach a high score: once an input word has been used and its score_vector value increases, we will leave that segment in our sentence. Not sure if this hurts more than it helps, but adding the input word as a seed is VERY complicated. And not sure it would make sense either.\n",
    "\n",
    "For a first prototype we will assume that maximum one new word is gonna be added per iteration. \n",
    "\n",
    "Pretty fragile with many assumptions?\n",
    "\n",
    "It would be useful to generate a list of synonyms of all input words to expand the input_words set and have higher acceptance rate. We thought about cleaning the input words set from the start but those are words with high probability of occurence so we better leave them and clean in the end.\n",
    "\n",
    "* maybe more seeds?\n",
    "* maybe require a higher score?\n",
    "* how to properly connect end of sentences and seeds? Is it better to have fewer seeds but longer auto-generated text?\n",
    "* add time it to report results and \"quantify\" results/improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Description generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def gen_input_words(mw, model):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    \n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but the loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab. Shouldn't be a thing when using larger corpus\n",
    "    input_words = [word for word in input_words if word in voc]    \n",
    "    return input_words\n",
    "\n",
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    #n_seeds = if we are using 2 or 3 seeds during the sentence generation step\n",
    "    #n_iterations = how many iterations we will do in the generation step\n",
    "    #debugging = True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "    #printing = True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    \n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([evaluate(mw+' means', 7, temperature = 1), evaluate(mw+' is', 7, temperature = 1), evaluate(mw+' can be found', 5, temperature = 1)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the TW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        while i < n_iterations:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1)\n",
    "                    sentence_parts[2] = evaluate(mw+' can be found', 5, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 7, temperature = 1)\n",
    "                    sentence_parts[2] = evaluate(mw+' can be found', 5, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1)\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 7, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 7, temperature = 1) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  evaluate(sentence_parts[0]+' '+ mw+' is', 7, temperature = 1) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1) +' '\n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = evaluate(mw+' can be found', 5, temperature = 1)\n",
    "                    else:\n",
    "                        temp =  evaluate(sentence_parts[1]+' '+ mw+' can be found', 5, temperature = 1) +' '\n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([evaluate(mw+' means', 11, temperature = 1), evaluate(mw+' is', 12, temperature = 1)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the TW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = evaluate(mw+' is', 12, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 11, temperature = 1)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = evaluate(mw+' means', 11, temperature = 1) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  evaluate(sentence_parts[0]+' '+ mw+' is', 12, temperature = 1) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = evaluate(mw+' is', 7, temperature = 1) +' '\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#description_generator('cake', model, n_seeds = 3, n_iterations = 2, debugging = False, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Cleaning the generated sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def sentence_cleaner(sentence, mw, model):\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, 'The main word')\n",
    "    \n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def final_output(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    sentence = description_generator(mw, model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, model)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number: 1\n",
      "cake means indian done larger reality simply going condition - such b cake is a person in the process is an \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means thieves building north to determination public open anyway cake is a fact set of killings - house \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means $ drawing significant air army allocation weapons technolo cake is a thank is the fact that they \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means reward deciding or hear game . implies , an than such  cake is a term means they do not say \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means 5 sign player meeting not answer anywhere that would be a  cake is a list is no one who is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means frosted * \" aeronautic snail is an outrage of the road  cake is a principle on where you can also \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means interface instrument having seem like to clear . ) think t cake is no other article good all , being \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means 5 any side-effect permanent second church be used in a rea cake is another collection of stories of the scottish \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means private story revival to offer self-conscious and intellig cake is a false is not a unique is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means mixture production political device . smelling as either l cake is a law result in jail being did \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means 5 financial kindle behavior , though it may mean that we  cake is a certificate to date , not to \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means fed within such looked like real big step , making work  cake is a much means that you will get \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means australia something partnerships sphere . \" positive , and cake is a false measure with a holiday is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means genomes any money . really organisations will mean , with  cake is an not supply will i have myself \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means beget anything they finished himself ( de or connections t cake is a complex to make them and excusing \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means play-there sex to three whether a card is the only place  cake is the necessary shops . a real who \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 17\n",
      "cake means 7,000 buy anyone who sit document matter his parents come  cake is a case means that the camera work \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 18\n",
      "cake means currently where anyone was not a continuous feels less aft cake is an ordinary component - a day who \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 19\n",
      "cake means offence combining built to clean your discussions , the se cake is a false more we ; improve , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 20\n",
      "cake means pcbs provided on donors - focus involved company ) . can  cake is a more to one that he must \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 21\n",
      "cake means slowdown college passport got how much \" fund until how is cake is not a bad , especially is a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 22\n",
      "cake means serious obligation resolution ( no event or bad or element cake is a principle on your case in to \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 23\n",
      "cake means 5 field 15 they therefore owned that they treat kids that  cake is not a religion is not a legally \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 24\n",
      "cake means contain teeth : there is no more than or in the  cake is a false and there can be found \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 25\n",
      "cake means share assume , only underwriters brother tell it of a word cake is not a description , see a particular \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 26\n",
      "cake means sales building any more natural experiments , generally fr cake is a function or use most , table \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 27\n",
      "cake means broadcast building sufficient good when of ridicule . from cake is an employee of the reality possible to \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 28\n",
      "cake means hard part of regaining ( > see a public service which  cake is not a enough task to be a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 29\n",
      "cake means equity ill-defined night involved . ) there 'd be sale ove cake is an obvious is not an idyllic or \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 30\n",
      "cake means taxable player . she will know at the end of the  cake is a false much quite a number , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 31\n",
      "cake means grace years under openly organization , meta you treat the cake is a blockbuster means easy to get an \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 32\n",
      "cake means frosted * through interest institutions parts maximum alwa cake is a good punt . is like . \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 33\n",
      "cake means sciences sex atmosphere prevention enjoys mainstream oppos cake is a lot less also in a scheme \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 34\n",
      "cake means filed taking a sample part of a human embryo . an  cake is a appliance is what you 've up \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 35\n",
      "cake means unified field achieving table different thing ... for keep cake is a bit means that the fact that \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 36\n",
      "cake means accidentally experimental it : n't a response to the crime cake is not a day is a challenge she \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 37\n",
      "cake means islam month everyone professional piece % money , reputati cake is a government happy measure ( a business \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 38\n",
      "cake means lip building new card anything - not leaving it 's so  cake is not a serious person and making a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 39\n",
      "cake means corner there provides strip ) boxes of the scottish made o cake is a sequence is being a psychologist , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 40\n",
      "cake means marked the beginning of our grant have it 's something tha cake is a more to resolve two and more \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 41\n",
      "cake means hard behind the success consideration . owned because dest cake is a data process , on october on \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 42\n",
      "cake means any field yourself easy profit set itineraries really get  cake is a complex is a nutritional or not \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 43\n",
      "cake means pressures money should make if a person where a monarch is cake is a school , mark well : kings \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 44\n",
      "cake means field discussion k physical method 's looks like an agricu cake is a case most much more amenable or \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 45\n",
      "cake means terrorists intense 1 million retirement . who feel a museu cake is a possibility was a person is a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 46\n",
      "cake means interface instrument firing up failure that owned . was bo cake is a temporary means a week is not \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 47\n",
      "cake means arranged taking reading cycle ; material her major cities  cake is a great vampire to be a day \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 48\n",
      "cake means 5 no experience as those interesting set . thing 1 /  cake is a false is the consent to you \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 49\n",
      "cake means post photos connected then do again who plays to offer the cake is a more one to different an a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 50\n",
      "cake means hard p to p an article highly before if we sincerely  cake is a simple like an example is the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 51\n",
      "cake means corner buy lots it into office . making this video investi cake is a mobile wherever or not that bad \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 52\n",
      "cake means ( all its subordinate through a web clock at . :  cake is a more useful story or in which \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 53\n",
      "cake means post all his lifetime : irritate maybe more were bad part  cake is a principle on positively bagpipes in the \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 54\n",
      "cake means stuck further video than such a highly makes an object is  cake is so vague rather say a week is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 55\n",
      "cake means 5 field century weeks something a mother common natural po cake is a good to flow yourself to a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 56\n",
      "cake means capacity seems slums kids unless they also achieving done  cake is a good budgie . can be found \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 57\n",
      "cake means unless combining the square of time jenny . method of exec cake is a person and recognised . they treat \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 58\n",
      "cake means corner any mental individual garden what a credit price th cake is a real celebration in a target is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 59\n",
      "cake means means me top hoppediz what a my game . wrong about  cake is no one 's voluntary in part , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 60\n",
      "cake means emotion t less making everyone buying a house is not a  cake is not a ' film the image . \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 61\n",
      "cake means currently where they were put why all costs between premis cake is a complex in front of congress . \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 62\n",
      "cake means success organization and traveling 9 industry target or mo cake is a good budgie . is the two \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 63\n",
      "cake means 5 sign provided by donors collective job was an incumbranc cake is not the difficult way necessary about to \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 64\n",
      "cake means interface image ( one than just enjoying as a stout is  cake is a mobile car is an ok is \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 65\n",
      "cake means place industry given woman was a tool is to an end  cake is no one who notices wrong clip instrument \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 66\n",
      "cake means leone half where each bloodstream in some covens unit via  cake is a healthy number who on read you \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 67\n",
      "cake means 6th forum for stylish death as with parvasi brand is a  cake is a national cycle . when the be \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 68\n",
      "cake means post finish poor tunnel assets . suggested visible island  cake is a good thing . application with a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 69\n",
      "cake means via nuclear to determine ? ) provided , but interesting ta cake is related to your bank makes an object \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 70\n",
      "cake means corner , \" you could consider what a company who 's  cake is an energy that is stored quite way \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 71\n",
      "cake means around we use come about which it should so it 's  cake is a simple type on place . they \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 72\n",
      "cake means itself activates a reason study to explain these but the s cake is not a ' and ordinances their right \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 73\n",
      "cake means optimism agreement ; sex , go has to offer to publicise  cake is a good budgie . was the symbol \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 74\n",
      "cake means grasp identifying commission anything threads code members cake is a false is the only way to \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 75\n",
      "cake means lip  grant when we have become checked user that attaches cake is a very common application way to achieve \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 76\n",
      "cake means prepare name language should might creating conduct if a d cake is a person with conscientious having a tunnel \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 77\n",
      "cake means knowledge worth a concept is the one 's possessions . can  cake is a book meant to create the earth \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 78\n",
      "cake means millions itself both short nature ? should make understand cake is a great vampire in the north of \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 79\n",
      "cake means snowplough sign : you could clearly minimize used mugged w cake is a part means only one who , \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 80\n",
      "cake means 5 years for a girl or even on the keyboard of  cake is this ? ) do immediately on a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 81\n",
      "cake means wares more excellent whether - why a program is this or  cake is a home , but a hearing means \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 82\n",
      "cake means 5 teaches there tooth sue each fider point . may be  cake is all possible to some content plant in \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 83\n",
      "cake means version gives a powerful that implementing it does n't rea cake is a human website . may have bought \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 84\n",
      "cake means half where something ; i do have how writing it into  cake is a temporary is a working on this \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 85\n",
      "cake means qualifying your alternative basis for the aggies showed th cake is a not means that you will be \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 86\n",
      "cake means 5 metal quite on to explain people to get me so  cake is a goal is not a place to \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 87\n",
      "cake means gone your money , reputation , unless christ another or no cake is a good budgie . can provide a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 88\n",
      "cake means service worth goes already \" a gap event company form of  cake is a number , a dish is a \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 89\n",
      "cake means contemplate into spanish 's credit to detonate how himself cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 90\n",
      "cake means 134 failure to pay an object is something impressive busy  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 91\n",
      "cake means executive spirit computer senior abessa watched remainder  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 92\n",
      "cake means private laws to delay 2 route ( see anyone should tell  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 93\n",
      "cake means altered negative , but so a small piece and a lot  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 94\n",
      "cake means larger nature because of a character is the sum the fire  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 95\n",
      "cake means itself los end . can provide a considerable solution . \"  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 96\n",
      "cake means alpha your specific going anywhere that the sales do mr :  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 97\n",
      "cake means executive tom mind computer family negative artists but n' cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 98\n",
      "cake means disqualification rather being a celebrity was a victim : n cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 99\n",
      "cake means misguided unity among the aspects 20 from a shared to the  cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 100\n",
      "cake means positively reading sale evidence , an or reaction class th cake is a number , a dish is a \n",
      "[0 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means positively reading sale evidence , an or reaction class th The main word is a number , a dish is a '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', model = model, n_seeds=2, n_iterations = 100, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
