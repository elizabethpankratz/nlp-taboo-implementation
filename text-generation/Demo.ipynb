{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Demo version of the descriptive sentence generator for the taboo implementation project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading gensim model to use the card generator and the semantic relations finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "card_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Reading and structuring corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "#we will be using the full version of the descriptive corpus we made ~115k sentences\n",
    "f = open('description-corpus-115k.txt', 'r')\n",
    "text = f.readlines() # List with sentences as elements\n",
    "f.close()\n",
    "\n",
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]\n",
    "\n",
    "#getting the avg length of a sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Setting up trigrams, context and target tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Sentence by sentence\n",
    "# this structure allows us to create context/target sets for each word. \n",
    "trigrams = []\n",
    "for sentence in sentences:\n",
    "    trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]\n",
    "\n",
    "\n",
    "#using all trigrams led to kernel death every time\n",
    "# we will randomly sample 50000 of them\n",
    "random.seed(163)\n",
    "trigrams = random.sample(trigrams, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting set of words in vocab, it's length and the frequency of each word\n",
    "# our vocab consists of the words appearing in trigrams, so no need to take the vocab over the whole text if we are not using all trigrams.\n",
    "voc = set()\n",
    "for tri in trigrams:\n",
    "    voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "voc_length = len(voc) \n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}\n",
    "\n",
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to inp\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Defining GRU class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading trained model\n",
    "Choosing which trained model to load. They were both trained on CPU over 100 epochs using 50000 trigrams sampled randomly. \n",
    "* 1:\n",
    "    * GRU model with 1 hidden layer consisting of 150 nodes. Note that since this model was trained before adding the random seed to the trigram sampling step, it is necessary to load its corresponding set of trigrams.\n",
    "* 2: \n",
    "    * GRU model with 2 hidden layers consisting of 75 nodes each. Unfortunately we didn't include a random seed for this trial either, and we did not save the corresponding set of trigrams. Although the generation step might work, it is not advised to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def model_selection(x):\n",
    "    if x ==1:\n",
    "        path = os.getcwd()+'/test5_trained_inference.pt'\n",
    "        hidden_s = 150\n",
    "        n_layers = 1\n",
    "        lr = 0.015\n",
    "    if x==2:\n",
    "        path = os.getcwd()+'/test4_trained_inference.pt'\n",
    "        hidden_s = 75\n",
    "        n_layers = 2\n",
    "        lr = 0.015\n",
    "    decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    decoder = torch.load(path)\n",
    "    decoder.eval()\n",
    "    return decoder\n",
    "\n",
    "decoder = model_selection(1)\n",
    "with open(\"trigrams_test5.txt\", \"rb\") as fp:\n",
    "    trigrams = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading description generation scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def next_token_generator(seed, generation_length=100):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(generation_length):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in seed.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).exp()\n",
    "        top_choice = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_choice)]\n",
    "        seed += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return seed\n",
    "\n",
    "def gen_input_words(mw, model):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "\n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but the loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab. Shouldn't be a thing when using larger corpus\n",
    "    input_words = [word for word in input_words if word in voc]\n",
    "    return input_words\n",
    "\n",
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    #n_seeds = if we are using 2 or 3 seeds during the sentence generation step\n",
    "    #n_iterations = how many iterations we will do in the generation step\n",
    "    #debugging = True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "    #printing = True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    \n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 7), next_token_generator(mw+' is', 7), next_token_generator(mw+' can be found', 5)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the TW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        while i < n_iterations:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 7) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) +' '\n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    else:\n",
    "                        temp =  next_token_generator(sentence_parts[1]+' '+ mw+' can be found', 5) +' '\n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 11), next_token_generator(mw+' is', 12)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the TW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 12)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 12) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) +' '\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def sentence_cleaner(sentence, mw, model):\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, 'The main word')\n",
    "\n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "    return sentence\n",
    "\n",
    "def final_output(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    sentence = description_generator(mw, model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, model)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'cake' as main word, 3 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are aiming to have in the descriptive sentence consists of: ['block', 'cake', 'coat', 'cookie', 'cover', 'dessert', 'dish', 'patty', 'tablet']\n",
      "Sentence number: 1\n",
      "cake means monkey millennium semicolon annulment latest remaining conside cake is altered import korzybski cheating car simulating piracy  cake can be found hour dissect 2500 cheating jams\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means monkey inward lag divers global_objects precedent compressed  cake is seen enumbindings falcons rootstock sportsman simulate fantastic  cake can be found cron bench pro-europeans overhaul integrated\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means monkey sata lag enhanced semicolon unecessary flowers  cake is optimal loop rehashing reconstruction lineal kneaded disastrous  cake can be found poland logical carer tabout train\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means monkey gifts lag stall ignored definitive intangible  cake is senator snag villa famous grey-thompson culturally round  cake can be found ramp death-wish listing exhausting bend\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means lays sata isoperimetric peer monkey personalized flowers  cake is cautiously website ekklesia comparably izzy australian cheating  cake can be found starbase imagine gplv semicolon introduce\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means monkey falsifiable asynch staff bench grey-thompson pay-out  cake is affecting similarly sample decency pacifist designate punctuation cake can be found personalized twork analogy latest global_objects\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means monkey sata lag stall ignored monkey pain  cake is certify celebratory soon marker-type jams flowers eminently  cake can be found considered hold re-used grey-thompson millennium\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means monkey issuance flowers cant standard guinea cms  cake is materials love rootstock spinning platoons long-established dwarv cake can be found diaeresis person-centred numerous excellent latest\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means monkey foliage lag semicolon carpentry latest cios  cake is reconciled chocolate youngwritersworkshops.com tesco aims them tw cake can be found personalized dmit lawgiver garnishee stall\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means monkey sata lag stall offensively flowers australian  cake is rodent marker-type v.g. distinction cinematic re-assurance irresp cake can be found personalized conflict peer introduce anti-virus\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means monkey inward 1916 cios carrying coins introduce  cake is raf foci carrier muted cpim agreeing white-glove  cake can be found ovens wildest latest eminently slatterns\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means monkey sata lag flowers center undead core  cake is corrupts pki ingests rainfall cios surprisingly semicolon  cake can be found arthur twork introduce directors discusses\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means monkey gifts semicolon tapestry humility non-reactionary lib  cake is stodgy kin contains repay legend lamp 3.6  cake can be found ic appropriately aggressiveness grant global_objects\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means monkey inward latest considered outward introduce casks  cake is civility raf blocker burooz absolution subscribers reluctantly  cake can be found monumental spokesmen this-well addresses exhausting\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means lays pig car abbatoir eminently slatterns heating  cake is accommodate long-established losses inmate state-implemented pedi cake can be found no stall prelate introduce currently\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means introduce wet-nursed cant introduce seizure coins passions  cake is automatic barbeque gear till analyse analyse analyse  cake can be found arcades bha priests keycap noisy\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 17\n",
      "cake means monkey sata turtle addresses bench investigation latest  cake is acetate hurt discredit border semicolon fortunes parole  cake can be found inward 1916 stuff bura 1916\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 18\n",
      "cake means monkey lag semicolon oaks writes educate cios  cake is assert maturity -3 boolean viva global_objects occasions  cake can be found ic image 3.6 chicken chicken\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 19\n",
      "cake means monkey sata lag semicolon monkey surrendering aformer  cake is aftermath optimal fro fictional drawer pain 1916  cake can be found inhospitable twork monkey topography semicolon\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 20\n",
      "cake means monkey lag flowers introduce grow contractor monkey  cake is hearts fashin namespace anger experts flowers discusses  cake can be found think imitation peer cios mammal\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 21\n",
      "cake means monkey millennium semicolon annulment peer monkey monumental  cake is rubenstein corrupts truth proceeding grey-thompson introduce prec cake can be found no alphabetical keyboard spade kiesle\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 22\n",
      "cake means monkey lag guy latest introduce one-way 404  cake is formed moor interstate form extensibility attacks break  cake can be found twork seekers login global_objects peikoff\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 23\n",
      "cake means monkey high-stakes stall weakside removing considered miss  cake is certify marker-type trace organs canoeing cult insulated  cake can be found no translate cheating maupassant .111\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 24\n",
      "cake means monkey gifts semicolon intents swing removing jams  cake is rusphoto weighting multi lib finest keycap bench  cake can be found personalized mission excitement faster church\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 25\n",
      "cake means monkey gifts semicolon favoured manufacturers ways buccina  cake is foreclosure formulation foreclosure scenery sad screening bura  cake can be found takeover ach stall opened-opened introduce\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 26\n",
      "cake means monkey lag flowers center undead core cheating  cake is lower optimal marker-type dylan satisfactory southward san  cake can be found dealers init determined rainfall 27:12\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 27\n",
      "cake means monkey sata lag stall prescriber lib monkey  cake is compliances poor libations diyer multi semicolon brussels  cake can be found cliche year-which eligibility realist plainly\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 28\n",
      "cake means introduce miracle stall b.s. removing slatterns stall  cake is dissertation trusts compressed latest eminently calibrated comput cake can be found starbase oscar obtain stall align\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 29\n",
      "cake means lays lamwo flowers intangible beans unsurprisingly refrigerant cake is documentary bench stretch overhaul tenouchi time removing  cake can be found overhaul integrated stall larval waited\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 30\n",
      "cake means monkey lathe latest listing intangible slatterns barefoot  cake is vehicle practicing corrupts lube latest lathe car  cake can be found misery bha specific-intent illuminate 3.6\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 31\n",
      "cake means monkey sata lag lib introduce pain misunderstanding  cake is torrent seen visitors break marker-type church introduce  cake can be found somewhere subject-subject allbreedspedigree bodies skip\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 32\n",
      "cake means monkey millennium semicolon annulment cheating church intangib cake is albam 0.1 wheel leveling weekly catering 29  cake can be found personalized combines abusers bettering pajeta-rumuruti\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 33\n",
      "cake means monkey sata lag flowers center extravagant where  cake is refuelling back-room antiquarian civic settings flowers intangibl cake can be found viscous takeover australian presented abbatoir\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 34\n",
      "cake means monkey falsifiable semicolon papillomavirus at cheating maupas cake is settled marker-type primeval guilty way humanities educationally  cake can be found listener latest enhanced forces 195\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 35\n",
      "cake means monkey firm stall introduce goal discusses educate  cake is 160 dolled intrusive bringing plugs art ach  cake can be found need bestowed kati theory subforms\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 36\n",
      "cake means monkey inward 1916 burooz diaeresis flowers numerous  cake is 1995 marker-type grey-thompson culturally stall introduce stores  cake can be found personalized blueprinting allfacebook fact stall\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 37\n",
      "cake means monkey sata un abbatoir stretch overhaul tenouchi  cake is kin previously fitting , lamp fun them  cake can be found pizzeria adhesion extending guns cheating\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 38\n",
      "cake means lays bettering latest connections church cios engage  cake is 160 intrusive raf less furnaces experiment car  cake can be found benefit towry rubenstein latest cant\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 39\n",
      "cake means monkey sata lag flowers center removing kcal  cake is bitterly parties sunburn intolerant them warped exhausting  cake can be found ovens realm fortune where global_objects\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 40\n",
      "cake means monkey lag semicolon kate cheating coins morally  cake is 160 dolled intrusive linux pajeta-rumuruti guru introduce  cake can be found adventure symbolization impact monkey ways\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 41\n",
      "cake means monkey apalling drew latest considered jams introduce  cake is specimens compliances terminate introduce stiffness biomedical fl cake can be found bench imagine wildest latest eminently\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 42\n",
      "cake means monkey sata lag stall maryniak flowers exhausting  cake is compliances motorist bloomsbury complainants combinator marker-ty cake can be found contemptibly closest cheating jams leftover\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 43\n",
      "cake means monkey gifts adhesion latest stretch overhaul warped  cake is fashin waste ranging h badly fanatics flowers  cake can be found badly archived guru introduce councillors\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 44\n",
      "cake means introduce remaining resuming stall overhaul unacknowledged man cake is affecting aftermath marker-type grey-thompson culturally stall in cake can be found considered tablets redirects grids abuse\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 45\n",
      "cake means monkey remaining infidel drawer school fictional canvas  cake is contains experiment enchanted monkey syntax bezier approach  cake can be found barbarism adhesion kalnins interstate culture-magpie\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 46\n",
      "cake means monkey inward 1916 burooz bench cant balisong  cake is fait 160 statistical compressed correlating prove cvd  cake can be found kg leptin nutshell youngwritersworkshops.com jams\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 47\n",
      "cake means monkey arise semicolon remaining jams discusses business-or  cake is cigarettes corrupts marker-type grey-thompson culturally stall in cake can be found awake lorries ingests subscribers car\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 48\n",
      "cake means introduce miracle contractor pay-out approximating guru yourse cake is foreclosure metric apostrophe australian bench discusses southern cake can be found feral dissect latest heidigger balancer\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 49\n",
      "cake means lays train 1916 jean latest stretch overhaul  cake is dissertation rubenstein 90 guru componentarchitecture latest bha  cake can be found introduce lds stall monkey sata\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 50\n",
      "cake means monkey adhesion semicolon introduce prevented misunderstanding cake is dolled marker-type grey-thompson culturally brazen lover gop  cake can be found personalized ku offspring turned stall\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 51\n",
      "cake means monkey sata lag flowers center semicolon fortune  cake is practicing interrupted enumbindings disgrace symbols solely waits cake can be found love stall muzzling 32 subscribers\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 52\n",
      "cake means monkey sata pain latest mans overhaul introduce  cake is compliances teaches transfers corrupts returned peer monkey  cake can be found unilateral wildest lynne stall farrier\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 53\n",
      "cake means monkey high-stakes stall weakside publisher illuminate tenouch cake is kalnins stopgap leads leveling discusses fortune beer  cake can be found arthur twork reductionistic introduce cleanliness\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 54\n",
      "cake means lays scripture strand removing grant cheating remaining  cake is hoped celebratory -3 company ezekiel peer introduce  cake can be found pain complicated bha technocracy noble\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 55\n",
      "cake means monkey sata lag stall ignored definitive sunburn  cake is annulment weighting isoperimetric senator chargeable cattle cliff cake can be found viscous takeover semicolon impairment pay-out\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 56\n",
      "cake means monkey economist semicolon lays plug juicy discusses  cake is actions intrusive departure precedent licensed bend astronomers  cake can be found bikini considered bha sequent turbofan\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 57\n",
      "cake means monkey utilities lib monkey wet-nursed global_objects introduc cake is cronier cnm scary bodies trevor colonists trying  cake can be found non-reactionary leg differences prevention jams\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 58\n",
      "cake means monkey ip stall introduce determined discusses educate  cake is mcmanus drawback experiment finest occupied educate introduce  cake can be found lcr blackburn drug feelings treatise\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 59\n",
      "cake means monkey sata benefit semicolon borne reckoned jams  cake is affectation undertaken comforting arrive trap angle 1916  cake can be found state bha 1916 sequent cheating\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 60\n",
      "cake means monkey inward 1916 burooz diaeresis flowers sunburn  cake is miller editors casks removing stretch gig overhaul  cake can be found personalized fol­lows fro latest latest\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 61\n",
      "cake means monkey issuance butler electronic latest eminently slatterns  cake is personalizing considerable mdi jams must-have voluntary educate  cake can be found ic customized 3.6 value latest\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 62\n",
      "cake means monkey sata lag flowers center considerable 3.9  cake is ferdinand canoeing towry ingests culturally ξ cameos  cake can be found no hardy semicolon associated 84\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 63\n",
      "cake means monkey gifts semicolon inherited latest cheating car  cake is foreclosure foreclosure unintended civility soldier latest consid cake can be found personalized ku offspring turned stall\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 64\n",
      "cake means lays latest assessments cheating car tomato introduce  cake is contributed unusual day-i slot boolean spouses pain  cake can be found ovens abend cheating latest considered\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 65\n",
      "cake means monkey 14.25 semicolon parish street-unless semicolon turtle  cake is arrow passions 200 maximum latest australian monkey  cake can be found contemptibly year-which stall duck latest\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 66\n",
      "cake means monkey inward dictatorship semicolon pain william considered  cake is aweber cheered resection compliances compliances zones thats  cake can be found aims bacterial multi cheating quiet\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 67\n",
      "cake means monkey issuance lib cold monkey obvious organizations  cake is reincarnated nitpicking compliances compliances contacts deranged cake can be found ovens subscribers porcupine coins long-established\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 68\n",
      "cake means monkey sata lag stall connections noisy latest  cake is paying karup-drusko endorsing inward coins earnest long  cake can be found rifles france casks cios monk\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 69\n",
      "cake means monkey lag 3.6 messe cheating maupassant jams  cake is facebook raf drought reverence isosurface newsmax charmed  cake can be found think morphotypes furniture stall prelate\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 70\n",
      "cake means monkey servings semicolon monkey sunburn non-reactionary monke cake is built represents celebratory long-established kneaded immediately cake can be found think intolerant 1916 cios ai\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 71\n",
      "cake means monkey high-stakes semicolon pain latest slatterns introduce  cake is fashin revs spills designate instill witness lee  cake can be found -6 non-reactionary excelling semicolon neater\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 72\n",
      "cake means monkey issuance latest plainly 20:57 jams leftover  cake is foreclosure roulade sufficient pe(s) latest 140 grant  cake can be found dwarves peer a.k.a. stall a.k.a.\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 73\n",
      "cake means monkey falsifiable dictatorship stall hopeful jams removing  cake is 160 intrusive stands sorry mecca letteurs miscarriage  cake can be found lifespan postmenopausal miss tomato pedigree\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 74\n",
      "cake means monkey sata lag flowers center semicolon associated  cake is optimal reckoning 20:57 face miracle 1916 introduce  cake can be found improbable vertebrate exam monkey passions\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 75\n",
      "cake means monkey gifts earlier cheating bench pro-europeans overhaul  cake is cornwell ber antioxidants formal immediate contemptibly music  cake can be found locations bette dimensions stall ain.dom\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 76\n",
      "cake means monkey servings semicolon leg parole verdict latest  cake is qualifications compliances holy jumping magnetized without non-ma cake can be found benefit latest rumours hits aggressiveness\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 77\n",
      "cake means monkey high-stakes semicolon pain departed phoneme misundersta cake is heartbreaking marker-type glutamine bulges divorce repair monkey  cake can be found dealers culture-magpie subscribers waits boolean\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 78\n",
      "cake means monkey gifts semicolon 09 stall discourage excellent  cake is products resigned embody imac leveling films catering  cake can be found personalized blueprinting allfacebook introduce strateg\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 79\n",
      "cake means shoreline china-oxford-cornell disseminated cios inventors pro cake is vxr favored tend dynamic agreeing sentence rankings  cake can be found gnaoua vertebrate introduce veterinarians contractor\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 80\n",
      "cake means monkey miracle stall dictionary non-power-of pay-out respawn  cake is farther consular gladiola gladiola dark-heart knives secrecy  cake can be found aims miscarriage acceleration illuminate cios\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 81\n",
      "cake means monkey chicken cant 1916 physics church bench  cake is marker-type turf philosophical se login applicant charmed  cake can be found arthur vertebrate male itrio keystone\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 82\n",
      "cake means monkey distributed at time dvd tesco introduce  cake is celebratory customizable surprisingly realist introduce archived  cake can be found contemptibly burqa replicator monkey lethal\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 83\n",
      "cake means monkey reay semicolon cios forms 17:20 flowers  cake is kayenta raf login tracks wonz pensions absolution  cake can be found mask katrina logical ug redirects\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 84\n",
      "cake means monkey gifts jams bura church bench discusses  cake is endorsement civic sauna johnson monk 90 bench  cake can be found personalized ku offspring turned stall\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 85\n",
      "cake means monkey sata ring global_objects shoreline monkey izzy  cake is torrent -3 tolorooččular horbury earlier removing stretch  cake can be found lifespan vertebrate cheating individuality bench\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 86\n",
      "cake means monkey gifts semicolon tapestry humility non-reactionary zenbo cake is affecting sharply silver dissertation sutil nitpicking sissy  cake can be found no translate begun patterns masse\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 87\n",
      "cake means monkey lag flowers semicolon leg school rainfall  cake is leveling marker-type pickle abbay izzy bha forgetting  cake can be found personalized blueprinting fro peer usurpation\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 88\n",
      "cake means monkey falsifiable dictatorship stall larval jams educate  cake is marker-type cite moroccan distinct ideals kid definitively  cake can be found sorts trusted angry cattle sitemap\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 89\n",
      "cake means monkey lag semicolon kate coins fortunes attended  cake is prayers mayo update addresses lived sharpened audiometer  cake can be found starbase bha lips coins aural\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 90\n",
      "cake means monkey sata lag flowers center violins intangible  cake is vertebrate long-established abroad abroad carved abroad mugged  cake can be found wherever presented avis rainfall penetrative\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 91\n",
      "cake means monkey sata benefit semicolon borne reckoned jams  cake is intrusive rubenstein waste quarter zenbook disastrous snowstorm  cake can be found twork liabilities win-win reverence staff\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 92\n",
      "cake means monkey gifts latest win-win overhaul introduce sermon  cake is shadow gratuitously 25th sea services lee boating  cake can be found ovens wildest latest eminently slatterns\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 93\n",
      "cake means monkey gifts economist cheating lib monkey economist  cake is sutil integrate answered editors abbay grey-thompson response  cake can be found aborigines lonely haircut subscribers monkey\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 94\n",
      "cake means monkey jungle fictional introduce determined grey-thompson cli cake is whereby adults hands-on fall peer 3.6 duibh  cake can be found personalized size piracy rainfall intangible\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 95\n",
      "cake means monkey apalling ring jams cios performing remaining  cake is fait statutes successful takeover mutt person-centred slatterns  cake can be found bikini lonely cattle obtain stall\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 96\n",
      "cake means stage peer introduce shortcut contractor semicolon 1,3  cake is rehearsal isoperimetric fixation situated trivium occupied latest cake can be found cliche year-which peak adequate jams\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 97\n",
      "cake means monkey multi stall abbatoir 1916 introduce vanquishing  cake is fait pub strathdon owls synonymous bishops long  cake can be found locations bette standard entanglement 1916\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 98\n",
      "cake means monkey 32 semicolon fortunes parole devoted latest  cake is window foreclosure formed middleton hath alike coe  cake can be found necromancers wherever them ffmpeg chargeable\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 99\n",
      "cake means monkey sata lag semicolon burooz hair semicolon  cake is corrupts mines respawn lag art introduce treasury  cake can be found dandelion vertebrate taking picks overwhelming\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 100\n",
      "cake means monkey sata lag flowers center removing kcal  cake is initial jumping rootstock acetate maltese maltese fictional  cake can be found personalized jams secret peoples theoretically\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means monkey sata lag flowers center removing kcal  The main word is initial jumping rootstock acetate maltese maltese fictional  The main word can be found personalized jams secret peoples theoretically'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', model = card_model, n_seeds=3, n_iterations = 100, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'cake' as main word, 2 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are aiming to have in the descriptive sentence consists of: ['block', 'cake', 'coat', 'cookie', 'cover', 'dessert', 'dish', 'patty', 'tablet']\n",
      "Sentence number: 1\n",
      "cake means monkey miracle stall overhaul introduce sermon semicolon monkey gifts lag stall  cake is sutil 160 represents celebratory arises ramp align \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means monkey sata lag bench cant turned cheating slightest bench cambodia semicolon  cake is 160 dolled intrusive gladiola 17:20 tomato fictional \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means monkey sata lag flowers center undead core cheating introduce python contractor  cake is hands-on bodies churchman variety thou housebuilders amateur \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means monkey issuance non-reactionary latest plainly church ecclesiology activated stretch overhaul tenouchi  cake is italians gladiola fashin holds bound long rainfall \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means monkey sata lag stall offensively flowers discusses terminate introduce infidel cheating  cake is aftermath celebratory stretch arthur dekinder-smithlaunching introduce patients \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means monkey falsifiable semicolon izzy kneaded monkey awake peer flowers introduce direction  cake is pedestal viva authorwendell 160 then personalized approach \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means monkey falsifiable dictatorship fictional introduce going discusses monkey figgins semicolon australian  cake is initial needed raf discount pedestrian absolution subscribers \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means lays scripture hypothetical cheating cios rootstock stretch removing farrier misunderstanding monkey  cake is bahá ssl foreclosure taken mantashe collapsed ordinary \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means monkey sata gig latest discourage monkey richard arises decisions forces fictional  cake is 160 dolled intrusive gladiola raf accuser multi \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means monkey sata ring global_objects appeal appeal song ways fictional repeated donne  cake is harmonization societies hammerhead characters infections idea grey-thompson \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means monkey imputing activities grey-thompson monkey goal semicolon monkey issuance actor cross  cake is gladiola giant leveling mutt casks stall farrier \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means introduce northwestern cant latest width car latest slatterns accumulator grant cheating  cake is celebratory 200 badly commit monkey mines fishery \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means monkey sata lag stall necromancers introduce adhesion latest stretch overhaul tenouchi  cake is dissertation 200 cnm abstention blow bringing latest \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means monkey high-stakes semicolon egg. contents cios lancashire cheating methodological cheating monkey  cake is blogger also-rans previously liquidity mixing elites bringing \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means monkey gifts semicolon inherited latest slatterns introduce lessons contractor kati tesco  cake is acetate low-status empirical pithy pedestrian jams instill \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means monkey imputing semicolon two-stud cheating diaphragm arthur failed cheating jams business-or  cake is 160 dolled intrusive bura sunday guru introduce \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 17\n",
      "cake means monkey gifts earlier mans mans sitemap abbatoir earlier introduce bankrupt contractor  cake is albam fro compliance stand cyclotron demonstrating soldier \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 18\n",
      "cake means monkey sata lag monkey lag 3.6 messe cheating maupassant min car  cake is coe ferdinand discusses educate pay-out festive spokesmen \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 19\n",
      "cake means monkey 2500 jams bura church bench sentenced grey-thompson ecclesiology haircut cattle  cake is foreclosure corrupts fashin uncle spectator objectives picks \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 20\n",
      "cake means monkey gifts semicolon alphabetical notebooks illuminate intents weapon ring cheating cms  cake is genome ferdinand monkey waste drawer statistical rainfall \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means monkey gifts semicolon alphabetical notebooks illuminate intents weapon ring cheating cms  The main word is genome ferdinand monkey waste drawer statistical rainfall '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', model = card_model, n_seeds=2, n_iterations = 20, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'airplane' as main word, 3 seeds, simple printing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are aiming to have in the descriptive sentence consists of: ['airplane', 'fighter', 'flight', 'jet']\n",
      "airplane means bald distinction width lib time maupassant cios  airplane is takeover characters 27:12 cios carrying keystone introdu airplane can be found wherever presented sizes going into\n",
      "airplane means introduce variables peer introduce relief discusses m airplane is scathing jumping exaptation electricity blackout linux c airplane can be found contemptibly considered scan audiometer archiv\n",
      "airplane means introduce sitaraman despair stall slatterns monkey pe airplane is think suzhou start-address inspire operates textual lee  airplane can be found searching communal grey-thompson bringing stal\n",
      "airplane means monkey gifts jams removing arises cios nano  airplane is scathing heating unusual hopped dishes need lover  airplane can be found believable rhianna remaining infidel vice\n",
      "airplane means introduce accident non-reactionary monkey guinea-pig  airplane is generalisation mouse spouses herald glutamine fact multi airplane can be found personalized mission excitement weblog pertine\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means introduce accident non-reactionary monkey guinea-pig  The main word is generalisation mouse spouses herald glutamine fact multi The main word can be found personalized mission excitement weblog pertine'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'airplane', model = card_model, n_seeds=3, n_iterations = 150, debugging = False, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'airplane' as main word, 2 seeds, only final output is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main word means introduce homicide contractor flowers removing poisoning miss slatterns economist cheating subsc The main word is barbara scheduling heightened rush grey-thompson latest cheating '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'airplane', model = card_model, n_seeds=2, n_iterations = 10, debugging = False, printing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
