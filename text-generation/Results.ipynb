{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# In this notebook we can experiment with the final outputs of the generator. It's a copy of the demo file in which we try our results.\n",
    "\n",
    "\n",
    "I started testing some things out just in case it'd prove difficult for you to run any of this cells :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg\n",
    "\n",
    "card_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#opening and reading the corpus\n",
    "#we will be using the full version of the descriptive corpus we made ~115k sentences\n",
    "f = open('description-corpus-115k.txt', 'r', encoding='utf-8')\n",
    "text = f.readlines() # List with sentences as elements\n",
    "f.close()\n",
    "\n",
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]\n",
    "\n",
    "#getting the avg length of a sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))\n",
    "\n",
    "\n",
    "def next_token_generator(seed, generation_length=100):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(generation_length):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in seed.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).exp()\n",
    "        top_choice = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_choice)]\n",
    "        seed += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return seed\n",
    "\n",
    "def gen_input_words(mw, model):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "\n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but the loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab. Shouldn't be a thing when using larger corpus\n",
    "    input_words = [word for word in input_words if word in voc]\n",
    "    return input_words\n",
    "\n",
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    #n_seeds = if we are using 2 or 3 seeds during the sentence generation step\n",
    "    #n_iterations = how many iterations we will do in the generation step\n",
    "    #debugging = True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "    #printing = True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    \n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence, dividing the whole sequence into equally long sub_sequences\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 7), next_token_generator(mw+' is', 7), next_token_generator(mw+' can be found', 5)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that already contains an input word.\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence to know where input words are located and to which sub_sequence they belong.\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        #for practical purposes we stop generating after some fixed number of iterations in case the score was not reached.\n",
    "        while i < n_iterations and score <2 :\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change in the score. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index]\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                    \n",
    "                #keeping the segment in which the improvement took place, blocking it and continue the generating process\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 7) \n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) \n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    else:\n",
    "                        temp =  next_token_generator(sentence_parts[1]+' '+ mw+' can be found', 5) \n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            #choosing what to print\n",
    "            if i == 0:\n",
    "                print('The set of input words we are trying to introduce into our sequence is: '+str(input_words))\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                    if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 11), next_token_generator(mw+' is', 12)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 2\n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations and score <2:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 2\n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 12)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11) \n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 12)\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            if i == 0:\n",
    "                print('The set of input words we are trying to introduce into our sequence is: '+str(input_words))\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                    if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def sentence_cleaner(sentence, mw, model):\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, '.The main word')\n",
    "    \n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            #if we have at least one\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                #choose one randomly\n",
    "                choice = np.random.choice(syns)\n",
    "                #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                while (choice in taboo_words or choice != mw) and len(syns) > 1:\n",
    "                    syns = syns.pop(syns.index(choice))\n",
    "                    choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "                #if all synonyms where taboo words or the mw\n",
    "                if choice in taboo_words or choice == mw:\n",
    "                    hypers = sr.get_hypernyms(tw)\n",
    "                    #if we have at least one\n",
    "                    if len(hypers) > 0:\n",
    "                        hypers = list(hypers)\n",
    "                        #choose one randomly\n",
    "                        choice = np.random.choice(hypers)\n",
    "                        #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                        while (choice in taboo_words or choice != mw) and len(hypers) > 1:\n",
    "                            syns = syns.pop(syns.index(choice))\n",
    "                            choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'Is a type of '+choice)\n",
    "            #in case no synonyms were found\n",
    "            else:\n",
    "                hypers = sr.get_hypernyms(tw)\n",
    "                #if we have at least one\n",
    "                if len(hypers) > 0:\n",
    "                    hypers = list(hypers)\n",
    "                    #choose one randomly\n",
    "                    choice = np.random.choice(hypers)\n",
    "                    #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                    while (choice in taboo_words or choice != mw) and len(hypers) > 1:\n",
    "                        syns = syns.pop(syns.index(choice))\n",
    "                        choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'Is a type of '+choice)\n",
    "                else: \n",
    "                    sentence = sentence.replace(tw, \"NO IDEA!\")\n",
    "    sentence = sentence[1:]\n",
    "    return sentence\n",
    "\n",
    "def final_output(mw, card_model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    sentence = description_generator(mw, card_model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, card_model)\n",
    "    return output\n",
    "\n",
    "def load_model(x):\n",
    "    if x ==1:\n",
    "        with open(\"trigrams_model1.txt\", \"rb\") as fp:\n",
    "            trigrams = pickle.load(fp)\n",
    "            \n",
    "        voc = set()\n",
    "        for tri in trigrams:\n",
    "            voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "        voc_length = len(voc) \n",
    "        word_to_freq = {word: i for i, word in enumerate(voc)}\n",
    "            \n",
    "        cont = []\n",
    "        tar = []\n",
    "        for context, target in trigrams:\n",
    "            context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "            cont.append(context_freqs)\n",
    "            target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "            tar.append(target_freq)\n",
    "        path = os.getcwd()+'/model1_trained.pt'\n",
    "        hidden_s = 150\n",
    "        n_layers = 1\n",
    "        lr = 0.015\n",
    "        decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        decoder = torch.load(path)\n",
    "        decoder.eval()\n",
    "    elif x ==3:\n",
    "        with open(\"trigrams_model3.txt\", \"rb\") as fp:\n",
    "            trigrams = pickle.load(fp)\n",
    "            \n",
    "        voc = set()\n",
    "        for tri in trigrams:\n",
    "            voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "        voc_length = len(voc) \n",
    "        word_to_freq = {word: i for i, word in enumerate(voc)}\n",
    "            \n",
    "        cont = []\n",
    "        tar = []\n",
    "        for context, target in trigrams:\n",
    "            context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "            cont.append(context_freqs)\n",
    "            target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "            tar.append(target_freq)\n",
    "            \n",
    "        path = os.getcwd()+'/model3_trained.pt'\n",
    "        \n",
    "        hidden_s = 50\n",
    "        n_layers = 3\n",
    "        lr = 0.015\n",
    "        decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        decoder.load_state_dict(torch.load(path))\n",
    "        decoder.eval()\n",
    "    elif x == 2:\n",
    "        print('It is not safe to use this model. Please choose either model 1 or 3.')\n",
    "    else:\n",
    "        print('Please enter 1 or 3 to choose the model to be used.')\n",
    "        \n",
    "    return voc, voc_length, word_to_freq, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Loading trained model\n",
    "Choosing which trained model to load. They are all GPU-based RNN models, trained on CPU over 100 epochs.\n",
    "* 1:\n",
    "    * Model with 1 hidden layer consisting of 150 nodes. Trained with a sample of 50,000 non-filtered trigrams containing ~16k tokens from our corpus' vocabulary (with a total of ~80k tokens, from which more than half only appeared once). The filtering step consisted of only sampling trigrams containing tokens that appear at least twice in our corpus, and was only implemented on model 3. Trained in about 12h.\n",
    "* 2: \n",
    "    * Model with 2 hidden layers consisting of 75 nodes each. Also trained with a sample of 50,000 non-filtered trigrams containing ~16k tokens from our corpus' vocabulary. Unfortunately we didn't include a random seed for this trial either, and we did not save the corresponding set of trigrams. Although the generation step might work, it is not advised to use this model. Trained in about 5h\n",
    "* 3:\n",
    "    * Model with 3 hidden layers consisting of 50 nodes each. Trained with a sample of ~86k filtered trigrams containing ~19k tokens from our corpus' vocabulary. Although a random seed was now included, for efficiency reasons we also decided to save the trigrams in order to load them faster and make reproducibility easier. Trained in about 11h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#To load model 1:\n",
    "#voc, voc_length, word_to_freq, decoder = load_model(1)\n",
    "\n",
    "#To load model 3:\n",
    "voc, voc_length, word_to_freq, decoder = load_model(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Examples from the demo using model 1\n",
    "Pencil instead of airplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'cake' as main word, 3 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['block', 'cake', 'coat', 'cookie', 'cover', 'dessert', 'dish', 'patty', 'tablet']\n",
      "Sentence number: 1\n",
      "cake means interrogation litre know aspirations basalt ventures them cake is indigo cells perpetuate pilotage immense visualization first cake can be found die contract ethnic repairs prescribes\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means secs joined first-of-a-kind doctor-material contract subs cake is danish binding starting undischarged citizens first-of-a-kin cake can be found nintendogs first-of-a-kind subsystems cushion vat\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means sixty shiva readers isosurface tick subroutine 250  cake is measuring atm consume wl hydrogen railroad oft cake can be found overwhelming pilotage stroll duly mitzvah\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means tony peculiar own first-of-a-kind apalling reasoning pilo cake is geometry narrative bfi apalling illumination cosmic stored cake can be found itot connection pay newsgroups therapy\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means share keeper holocaust silo blamed reluctantly newsgroups cake is gather goodness branch false 0.9 carefully connection cake can be found implosion works alexander strings mild\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means handles pics barriers tate ferdinand edge protoplasm  cake is acronym dad disaster publishing trampoline cattle urls cake can be found moo classes naming a_k weakside\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means scab jumbled ceremony inanimate fall pair causal  cake is pumped nfl colinnn 1973 burial brandished gimp cake can be found pilotage massachusetts visualization railroad proj\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means mirage mil-manual 9.59 servant winner doctor-material con cake is non-drinker-there workloads tones sector testicle secs dad cake can be found rag centred de play cos\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means power twenty-four aristotelian alive nominee set us-cuban cake is salesforce 38 mrad obama hpwm false surgeon cake can be found overwhelming fishermen determination incompetence \n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means monsoon boom counterproductive brazil olson funny thickly cake is nonlethal menu strong lock careless everyone re-examination cake can be found pilotage wits fur modularization qualifier\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means admittedly italian watchful managing nephite theme zone  cake is exercise inability purpose-designed pilotage passive based p cake can be found had noelle orient fitted fotopoulos\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means bells tick orient goals iyereoboite tlp landed  cake is measuring urge facsimile identification finale snow-storm ab cake can be found itot cos ccsr legendary heads\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means tiredness muffler :-) hmg-coa hibiscus possessor sortie  cake is phd designing illumination ayurvedic trader anygroup pilotag cake can be found wished carefully battle jackson allowable\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means cardboard employed tumult puts babyface approaching newbi cake is est deploying think deal motorist fotopoulos doctor-material cake can be found pilotage recent duplicate pilotage wits\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means complete rptpalpha 9.00 managing viral contract vat  cake is supervisor blokey causal toxoid inflicts causal swollen cake can be found 405 noelle podium hand-forged mediatory\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means marshchapel picture rush false declare toughest inside  cake is install accadian ko ἀπὸ columbus grandma color cake can be found watch da revolutionaries fulfilling modularization\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 17\n",
      "cake means demonstrated cot twenty-four mesquite tracts invites ceri cake is gather blokey compilation sector cookie acceleration modular cake can be found 483 refresh general soon-to-be mild\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 18\n",
      "cake means demonstrated cot twenty-four mesquite tracts invites ceri cake is gather blokey compilation sector cookie acceleration modular cake can be found 483 refresh general soon-to-be mild\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 19\n",
      "cake means method morale provoke channels regexp cos wavefield  cake is coercing pros pictures misplacement relate taint purpose-des cake can be found council qualifier terminations jmal :-)\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 20\n",
      "cake means agnew disembark perennial unconditionally prioritize wors cake is lens perror gaunt rights-in dakarand mitzvah handles cake can be found wished carefully nav towel vettel\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means agnew disembark perennial unconditionally prioritize wors .The main word is lens perror gaunt rights-in dakarand mitzvah handles .The main word can be found wished carefully nav towel vettel'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', card_model = card_model, n_seeds=3, n_iterations = 20, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'cake' as main word, 2 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['block', 'cake', 'coat', 'cookie', 'cover', 'dessert', 'dish', 'patty', 'tablet']\n",
      "Sentence number: 1\n",
      "cake means breaches take-in cakewalk owls camberwell deal divers dream associations exclusion hmg-coa cake is castle workloads beneficiary possession physiotherapy importantly mode\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means ethiopian sediment expense china-oxford-cornell mag assault speedking conceptus surprised : cake is limbo entities inspiring violating tide point-of sub-group\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means disgusting ringing structured contoso selectable heads seriously-reveals adjudicator oft sa cake is eduation unrelated forums servicer statute av designing\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means mou olympic micro-organism sedation exceed lor detached malaise causal fishermen vat cake is euros regulate u.s. ponzi those false believer\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means shia beekeeper mentality numbing wrench ayurvedic tuberculin lots doctor-material duplicate cake is baptise woodcock aviation clearer skunked wastes hitter\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means lyndhurst belt series ranked forest conversion-declarators comforting haer hmg-coa @ core cake is answers resembles solely letting partner pumped writing\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means shitfuckasscockwhorebitch cinnamon passing sony-made ed managements intellectually applianc cake is beard pto jimmy first-of-a-kind theme distributes pilotage\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means fell 04 populations cybercounselor matches pulmonary invoking sufferer urls maintain concen cake is preposition taxation emmy noelle treasure rights-in surgeon\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means ff never-ending sampling vat shape hate~ejaculate dies purpose-designed pinpoint contract t cake is recommended fred works deal 14.5 pedicure zone\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means perennial hurts charles bay testicle jpl frampton tick sleep memories projected cake is preposition beard condoning works techno churchill urls\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means axioms fider charged medication classification chairman planners pilotage bumbling anygroup cake is coercing backwards tenant proprietor bassist texas intellect\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means mou beneﬁts inhibits sheet purpose-designed borne 1979 program singh emotions false cake is plant crosses dad enforcement false hoped allergies\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means hitchcock sectors kovalevsky causing highway perennial ) bookstore apalling abstraction phi cake is excluding start-up chayote million considers conscientious moments\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means ber announced imagine franchise frequently itot pilotage culte anygroup noelle heartbeat cake is excluding modification clothes cluster 20th mjd immobilize\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means leuchars outsiders hip immobilize interlinked purpose-designed 2638 twilight arrival causal cake is patience adjournment therapy architects overtly snow-storm meh\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means keys battery apalling urls projected strings pilotage rogue duplicate pilotage cobblers cake is share million gainer franzen crossing purpose-designed ayurvedic\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 17\n",
      "cake means op net involves objectives diary diary diary license 1962 toughest import cake is pickings bankruptcy kai roof false surgeon allowing\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 18\n",
      "cake means surrogate mothership :-) jalapeno undergo strings polyester mitzvah contract hand-forged ca cake is counterseal bid cellophane continuing publishing transportation modularization\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 19\n",
      "cake means organizes axioms deer accommodation retain potion surely kid thou potluck video cake is 140 funding unilateral span @ apparent division\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 20\n",
      "cake means cigar elevate solids jagi pedantic postmark civil counter-appeal final riaa alight cake is rasputin bedding instruments fundamentally diocese tricky consolidate\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means cigar elevate solids jagi pedantic postmark civil counter-appeal final riaa alight .The main word is rasputin bedding instruments fundamentally diocese tricky consolidate'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', card_model = card_model, n_seeds=2, n_iterations = 20, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'airplane' as main word, 3 seeds, simple printing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['cosmetic', 'draw', 'figure', 'notepad', 'pencil']\n",
      "Sentence number: 146\n",
      "pencil means pursued highly span kai lay pharmacists fees  pencil is baggage orders happy dental columbia contingency standa pencil can be found carefully setting ohmygosh false underwear\n",
      "Sentence number: 147\n",
      "pencil means axioms albino novelty background deviation dvlc roof pencil is humanity nano-machines economical scouted sofa holocaus pencil can be found itot etc mos homily polyp\n",
      "Sentence number: 148\n",
      "pencil means axioms sign baggage belonging banquet sony-made stri pencil is warsaw voicethread endures imparted doctor-material han pencil can be found allowing window britain three slavery\n",
      "Sentence number: 149\n",
      "pencil means axioms archive bless physiotherapy dressmaking categ pencil is appeals didnt hoped guidance tax matured dimensions pencil can be found install since false fulfilling first-of-a-kin\n",
      "Sentence number: 150\n",
      "pencil means injuries ncda bulldozer start offices pilotage gathe pencil is humanity shall 4.94 cosmic strings gimp isolated pencil can be found encyclopedia heartbeat pilotage structurally \n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means injuries ncda bulldozer start offices pilotage gathe .The main word is humanity shall 4.94 cosmic strings gimp isolated .The main word can be found encyclopedia heartbeat pilotage structurally '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'pencil', card_model = card_model, n_seeds=3, n_iterations = 150, debugging = False, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'airplane' as main word, 2 seeds, only final output is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['cosmetic', 'crayon', 'draw', 'figure', 'notepad', 'pencil']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means albino majoring sign endures cartoon nav mild harmless doctor-material · term .The main word is gained unfallia appear continually abroad modularization remains'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'pencil', card_model = card_model, n_seeds=2, n_iterations = 10, debugging = False, printing = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Examples from the demo using model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'cake' as main word, 3 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['baba', 'block', 'cake', 'coat', 'cookie', 'cover', 'crumpet', 'dessert', 'dish', 'pastry', 'tablet']\n",
      "Sentence number: 1\n",
      "cake means massumi ballad identifier superhuman facial millennia gay  cake is shadow gestalt facial re-writing sweets bravery coincide cake can be found carlyle freshly re-writing .36 goo\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means fragments catches shaming gay boyfriend ballad identifiable  cake is abandoning declassified pubic creator hackney sweets axiom cake can be found insecticide touch-screen grams elms school\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means watching zoo creator ballad chairs ballad tilting  cake is methodology sniff uniquely validity pubic annoyed flowing cake can be found german storm surrounded eradicate online\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means comparator re-writing billings facial annual parental rnli  cake is asian ending brook errors non-empty doughnut resource cake can be found unto televisions insecticide slipping manifest\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means groupoid driven three-percent iris co-signer casino pitchfork  cake is banks annual enthusiast crooks bullets invent component cake can be found protector man sniff wagner parental\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means proposed spent enthusiast commit -- rule internet  cake is gestures transform servlet re-writing snowdrop refuting ed cake can be found incomprehensible bearer county hierarchies designating\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means graduates relying parliamentary commonly trader luke facial  cake is runs css donut mechanisms hi creator sweets cake can be found bump insisted insight idea cater\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means i-pod touch-screen amy monday scenery capitalise workplace  cake is pointless grassy .1 uttering wooden muscles hundreds cake can be found gifted tec notepad ups online\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means publications coincide risk-taking re-writing flap rightful landing cake is stress version coupla ballad reaches banner facial cake can be found tactic coincide signpost fried 7.2\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means covered touch-screen humbug younger cosmology relying recommended  cake is school accountant pre-ordering touch-screen superhero musick radar cake can be found diagnoses re-writing casino abomination inthe\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means covered touch-screen humbug younger cosmology relying recommended  .The main word is school accountant pre-ordering touch-screen superhero musick radar .The main word can be found diagnoses re-writing casino abomination inthe'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', card_model = card_model, n_seeds=3, n_iterations = 10, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'cake' as main word, 2 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['baba', 'block', 'cake', 'coat', 'cookie', 'cover', 'crumpet', 'dessert', 'dish', 'tablet']\n",
      "Sentence number: 1\n",
      "cake means airplane asia thousands 1839 rebalance read author announcement orca brave re-writing cake is imperial final woman descent boundaries gay nash\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means simon boundaries notepad sudan re-writing hackney hexadecimal delicate entirely accessibility re-wr cake is reactor aspirations anesthesia sailors great barbeque ibn\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means oak vulgar engine mechanisms pitchfork year-round re-writing lightness commonly continuously creato cake is solicitors everyone post-war smartly patent humans asian\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means guadagnin hydrocele parental guthrie ballad seamlessly mechanisms c-section growl definitive creato cake is coincide pretty sinusitis uttering rout nodes gifted\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means weigh muscles gay online slipping school badges gestures re-writing \\ closet cake is todays drips override pumpkin mechanisms school financed\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means brochure epithelium reveals three-percent morphism online rat hierarchies hackney chances online cake is goo obeys hockey senior touch-screen congestion staffing\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means humanities ftc sweets float recognition corkscrew cannibalism adjutant pendant gay creator cake is baggage touch-screen re-writing hedge mechanisms referencing hunting\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means solving ps enthusiast conceived mechanisms encoded adding co-signer phenomenally touch-screen consu cake is telegraph ordinal hypocrisy multilateral school facial primary-expression\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means lock online diabolical questions received re-writing creator posture mechanisms framed ammunition cake is paul capability bourbon standard sailors glare savage\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means requires re-writing 1300 190 congestion shaming publications touch-screen unitary prompt dsm cake is repeatedly etiquette trial champions accepts parental court\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means italic electing rejuvenate billings 2005 touch-screen potty hackney touch-screen re-writing muscles cake is casks grace registered bullying errors refusal strap\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means dublin enthusiast phones largely flex notepad online ploy concentric notepad re-writing cake is study offshore mechanisms commute art 18th point\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means biting crave school cairo counteract monarchs creator coupla obeys corkscrew creator cake is reservoir re-writing ballad payable warning hidden makeshift\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means hazard allows scare attitudes burnt straight canal gay dartboard shaming tories cake is almost pacifist sailors enthusiast compulsory 've creator\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means radiological counteract incomprehensible facial chant asian exterminated designating online relying cake is sweets re-writing nonsense bump spiral creator twenties\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means co-operation practices survivor garter mechanisms ∨ re-writing perjury tomato epidemic ballad cake is ddos school refreshing online intellect facial creator\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 17\n",
      "cake means gifted enthusiast .15 shoe modifications royal intelligible riposte sticker author school cake is frequency re-writing roosevelt dramatized touch-screen re-writing woman\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 18\n",
      "cake means odors newsfeed 2008 hardworking shaming slice questions ruled bite gifted nodes cake is enthusiast school badges creator re-writing refuting nonsense\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 19\n",
      "cake means failure flats sailors underline manufacture version given unwinnable doubtless muscles signpost cake is snow kant exterior shaming fda enthusiast asia\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 20\n",
      "cake means tie asian serial damned 101 malnutrition constructively celebrating arab fork riposte cake is earthly co-operative chicago passively awakened sensors towns\n",
      "Covered vector: \n",
      "[0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means tie asian serial damned 101 malnutrition constructively celebrating arab fork riposte .The main word is earthly co-operative chicago passively awakened sensors towns'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', card_model = card_model, n_seeds=2, n_iterations = 20, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'airplane' as main word, 3 seeds, simple printing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['airliner', 'airplane', 'biplane', 'fighter', 'flight', 'jet']\n",
      "Sentence number: 146\n",
      "airplane means irish touch-screen tasks 7 summit re-writing torsten  airplane is realised disincentive online contribute theft hui rambling airplane can be found boundaries touch-screen doctrine permission touch\n",
      "Sentence number: 147\n",
      "airplane means plastcine survivor sphinx millennia ballad ballad corksc airplane is baxter suarez annual enemies tries ballad lose airplane can be found assures liars tries desire verbs\n",
      "Sentence number: 148\n",
      "airplane means snow hold trigger blower 64 commonly mindful  airplane is westerners inserting hentai sandy sailors hackney hysterect airplane can be found catalytic ka schlimazel nonperson --\n",
      "Sentence number: 149\n",
      "airplane means amc wembley indices ploy digger loop fed  airplane is simulator surrogates stanley worksheets parameter debt x-fa airplane can be found dispense universalism hackney headlight dismantle\n",
      "Sentence number: 150\n",
      "airplane means submarine wallet directly re-writing creator france blow airplane is wrapped maturity mechanisms interlinear guadagnin depressed airplane can be found perverse weighing errors shaming observes\n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means submarine wallet directly re-writing creator france blow .The main word is wrapped maturity mechanisms interlinear guadagnin depressed .The main word can be found perverse weighing errors shaming observes'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'airplane', card_model = card_model, n_seeds=3, n_iterations = 150, debugging = False, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Example with 'airplane' as main word, 2 seeds, only final output is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['airliner', 'airplane', 'biplane', 'fighter', 'flight', 'jet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means levels karl graph billings ploy locality fingerprint creator fruits sinusitis troops .The main word is dante badges tether professor beyond signpost spells'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'airplane', card_model = card_model, n_seeds=2, n_iterations = 10, debugging = False, printing = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
