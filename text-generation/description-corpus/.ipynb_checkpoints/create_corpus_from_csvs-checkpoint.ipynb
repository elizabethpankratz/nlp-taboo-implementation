{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the description corpora for NN training\n",
    "\n",
    "This script creates two corpora containing descriptive sentences, `description-corpus-115k.txt` with 115,735 sentences (3,153,298 tokens) and `description-corpus-20k.txt` with 20,680 sentences (557,969 tokens); the latter is a subset of the former.\n",
    "\n",
    "These corpora are combinations of all of the concordances contained in the CSV files saved in the two subdirectories, `encow16a-nano/` and `encow16a/`.\n",
    "The CSV files in these two folders contain concordances for various descriptive constructions which we intend to use to train our text-generating NN, so that it is familiar with typical descriptive sentences and might then be able to create better ones itself.\n",
    "The folders are named according to the corpora from which the concordances are drawn: ENCOW16A (for the 115k-sentence corpus) and ENCOW16A-NANO (for the 20k-sentence corpus).\n",
    "These are web corpora, containing texts crawled from many genres of website, from chat forums to official business pages, so diverse styles of language use are represented.\n",
    "(For more detail, see the [Corpora from the Web website](https://corporafromtheweb.org/).)\n",
    "\n",
    "A list of which descriptive constructions we searched for and the CQL queries used to find them in the corpora can be found in `constructions_and_cql.txt`, and Elizabeth's scripts used to query each corpus are found in the respective subfolders and are both called `descr_sents.py`.\n",
    "(Even though these can only be run on the COW server, they are included here for transparency's sake.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_sents_from_csv(subdir, filename):\n",
    "    \"\"\"\n",
    "    Reformats the tab-separated outputs of SeaCOW queries for each descriptive construction into a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        subdir: a string with the subdirectory containing the CSV file.\n",
    "        filename: a string ending in '.csv'; the file to read in.\n",
    "    Returns:\n",
    "        A list of sentences as strings from the given concordance file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise empty list to fill as we go through the concordance file.\n",
    "    sent_list = []\n",
    "    \n",
    "    path = str(subdir + '/' + filename)\n",
    "\n",
    "    # Open desired file and go through it line by line.\n",
    "    with open( path , encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "\n",
    "            # We only care about the actual concordance, so, the lines in output file not beginning with # or the header 'doc'.\n",
    "            if line[0] != \"#\" and line[0:3] != \"doc\":\n",
    "\n",
    "                # Remove newlines and split by tabs (since the output is actually tab-separated), saving all but the first\n",
    "                # element of the resulting list (the original URL of the sentence) as a string to the concordance's list.\n",
    "                split_by_tab = line.strip('\\n').split('\\t')\n",
    "                sent_as_str = \" \".join( split_by_tab[1:] )                \n",
    "                \n",
    "                # If this joining results in a space being the first character of the line, remove that space.\n",
    "                if sent_as_str[0] == \" \":\n",
    "                    sent_as_str = sent_as_str[1:]\n",
    "                \n",
    "                # Add to list.\n",
    "                sent_list.append(sent_as_str)\n",
    "        \n",
    "    return sent_list\n",
    "\n",
    "\n",
    "def create_total_sent_list(subdir):\n",
    "    \"\"\"\n",
    "    Goes through each CSV file in the given subdirectory and combines all of their concordances into one single list.\n",
    "    \n",
    "    Args:\n",
    "        subdir: a string, the name of the subdirectory to gather concordance files from.\n",
    "    Returns:\n",
    "        A list containing all sentences as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all csv files in given subdir.\n",
    "    conc_files = [file for file in os.listdir(subdir) if file[-3:] == 'csv']\n",
    "\n",
    "    # Initialise empty list to contain sentences.\n",
    "    all_sents = []\n",
    "    \n",
    "    # Go through all files in subdir and extract the sentences from them, extending all_sents for each one and returning.\n",
    "    for conc_file in conc_files:\n",
    "        all_sents.extend( get_sents_from_csv(subdir, conc_file) )\n",
    "        \n",
    "    return all_sents\n",
    "\n",
    "\n",
    "def get_token_count(sents_list):\n",
    "    \"\"\"\n",
    "    A function to count the number of tokens in each corpus.\n",
    "    \n",
    "    Arg:\n",
    "        sents_list: A list of each sentence as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise token counter.\n",
    "    tokens = 0\n",
    "\n",
    "    # Go through each sentence in the list, counting its tokens and adding this number to the counter, and return.\n",
    "    for sent in sents_list:\n",
    "        \n",
    "        # (Because the sentences are already tokenised and space-separated in ENCOW, it's sufficient to split the exported\n",
    "        # sentences by space to get back to the original tokenisation.)\n",
    "        sent_len = len( sent.split(' ') ) \n",
    "        tokens += sent_len\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def write_to_corpus_file(sents_list, out_filename):\n",
    "    \"\"\"\n",
    "    Creates a new file in the current directory containing all of the sentences, one on each line.\n",
    "    \n",
    "    Args:\n",
    "        sents_list: A list of each sentence as a string.\n",
    "        out_filename: A string ending in '.txt'; the name of the file to be created.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a file to contain the list of sentences with the given filename.\n",
    "    file = open(out_filename, 'w+', encoding='utf-8')\n",
    "    \n",
    "    # Write each line (with newline) to this file and close.\n",
    "    for sent in sents_list:\n",
    "        file.write(sent+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description-corpus-20k:\n",
      " Number of sentences:\t 20680\n",
      " Number of tokens:\t 557969\n",
      "\n",
      "description-corpus-115k:\n",
      " Number of sentences:\t 115735\n",
      " Number of tokens:\t 3153298\n"
     ]
    }
   ],
   "source": [
    "SUBDIR_NANO = 'encow16a-nano'  # The smaller version of the corpus\n",
    "SUBDIR_FULL = 'encow16a'       # The larger version \n",
    "\n",
    "\n",
    "# Create smaller corpus from ENCOW16A-NANO concordances and print stats.\n",
    "all_sents_nano = create_total_sent_list(SUBDIR_NANO)\n",
    "write_to_corpus_file(all_sents_nano, 'description-corpus-20k.txt')\n",
    "\n",
    "print('description-corpus-20k:')\n",
    "print(' Number of sentences:\\t',  len(all_sents_nano))\n",
    "print(' Number of tokens:\\t',  get_token_count(all_sents_nano))\n",
    "\n",
    "\n",
    "# Create larger corpus from ENCOW16A concordances and print stats.\n",
    "all_sents_full = create_total_sent_list(SUBDIR_FULL)\n",
    "write_to_corpus_file(all_sents_full, 'description-corpus-115k.txt')\n",
    "\n",
    "print('\\ndescription-corpus-115k:')\n",
    "print(' Number of sentences:\\t',  len(all_sents_full))\n",
    "print(' Number of tokens:\\t',  get_token_count(all_sents_full))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
