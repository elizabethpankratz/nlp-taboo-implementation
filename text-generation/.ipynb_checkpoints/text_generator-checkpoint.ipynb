{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Updates after discussion\n",
    "* cleaning the 5% least frequent symbols from the vocab was a disaster since several steps can't handle unknown words. It's not impossible to fix this, but for the time being it's easier to work with the full corpus (~115k sentences instead of ~20k) and assume that this will make the weird symbols/words waaay less likely to appear. It's not an unfounded assumption (denke ich).\n",
    "\n",
    "* top 3 seeds by freq of appearance in the corpus \"is\" >> \"means\" >> \"can be found\"\n",
    "\n",
    "\n",
    "* Three steps for cleaning probably provide better word2word meanings but since for us the main goal is to generate a coherent sentence from the input and meaning WE are feeding, then we can't leave out punctuation symbols and stop words. Maybe it is still good to lemmatize? We'll see\n",
    "* Network currently trained with  5000 trigrams, 50 epochs, hidden_s = 100, lr = 0.015. Took 17min to train. Losses went from 7.9 to 2.5\n",
    "* clean corpus 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Generating our descriptive sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Importing everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#is this the best choice of autograd?\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def word_to_synsets(word):\n",
    "    \"\"\"\n",
    "    Converts the given word to a synset object.\n",
    "\n",
    "    Arg:\n",
    "        word: a string like 'cat'\n",
    "        pos: the desired part of speech (choices: wn.NOUN, wn.VERB, wn.ADJ, wn.ADV)\n",
    "    Returns:\n",
    "        A string containing the first synset ID, formatted according to WordNet's conventions, e.g. 'cat.n.01',\n",
    "        corresponding to that word.\n",
    "    \"\"\"\n",
    "    # Convert word string to the synset with the corresponding part of speech.\n",
    "    return wn.synsets(word)\n",
    "\n",
    "\n",
    "def synset_to_word(synset):\n",
    "    \"\"\"\n",
    "    Converts the given synset to the actual word it represents.\n",
    "\n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A string containing the word corresponding to that synset.\n",
    "    \"\"\"\n",
    "    # Convert synset to lemma, since this is what name() is defined over.\n",
    "    return synset.lemmas()[0].name()\n",
    "\n",
    "\n",
    "def get_antonyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all antonyms for the given synset.\n",
    "\n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of antonymic words as strings, if there are any, or else the empty list.\n",
    "    \"\"\"\n",
    "    # Convert synset to lemma, since this is what the antonym relation is defined over, and get antonym(s).\n",
    "    ant_lemmas = synset.lemmas()[0].antonyms()\n",
    "\n",
    "    # Convert each antonym in this list to a string and return list (empty if no antonyms).\n",
    "    return [ant_lemma.name() for ant_lemma in ant_lemmas]\n",
    "\n",
    "\n",
    "def get_hypernyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all immediate hypernyms for the given synset.\n",
    "\n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of hypernymic words as strings.\n",
    "    \"\"\"\n",
    "    # Convert hypernyms of the synset to strings and return list.\n",
    "    return [synset_to_word(hyper) for hyper in synset.hypernyms()]\n",
    "\n",
    "\n",
    "def get_hyponyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all immediate hyponyms for the given synset. (There are often many.)\n",
    "\n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of hyponymic words as strings.\n",
    "    \"\"\"\n",
    "    # Convert hypernyms of the synset to strings and return list.\n",
    "    return [synset_to_word(hypo) for hypo in synset.hyponyms()]\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Returns a set of synonyms, according to WordNet, for the given input word (using all of its senses, if\n",
    "    there are multiple).\n",
    "\n",
    "    Arg:\n",
    "        word: a string representing the word whose synonyms we want.\n",
    "    Returns:\n",
    "        A set containing all of the other words in the same WordNet synset as the given word.\n",
    "    \"\"\"\n",
    "    # Initialise set that will collect the synonyms.\n",
    "    synonym_set = set()\n",
    "\n",
    "    # Convert the word to a list of synsets.\n",
    "    synset_list = word_to_synsets(word)\n",
    "\n",
    "    # Get all the lemmas corresponding to the given word's synset.\n",
    "    synonym_lems = [x.lemmas() for x in synset_list]\n",
    "\n",
    "    # Go through them, get the names from the lemma (lowercasing everything for consistency), and add\n",
    "    # to synonym_set.\n",
    "    for lemma_list in synonym_lems:\n",
    "        syn = lemma_list[0].name().lower()\n",
    "        synonym_set.update( [syn] )\n",
    "\n",
    "    # Remove from the synonym set the input word and any words that also contain the input word and return.\n",
    "    to_rm = set()\n",
    "    for synonym in synonym_set:\n",
    "        if synonym == word or word in synonym:\n",
    "            to_rm.update({synonym})\n",
    "\n",
    "    return synonym_set.difference(to_rm)\n",
    "\n",
    "\n",
    "def make_semrel_dict(word):\n",
    "    \"\"\"\n",
    "    Creates a dictionary that contains all words standing in the given semantic relation to the main word.\n",
    "\n",
    "    Arg:\n",
    "        word: a string like 'cat' (the main word)\n",
    "    Returns:\n",
    "        A dictionary with the semantic relations as keys and a set of words that have that relation to all senses\n",
    "        of the input word, according to WordNet, as values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise dictionary (and we can get synonyms right away).\n",
    "    semrel_dict = {\n",
    "        'semrel_synonym': get_synonyms(word),\n",
    "        'semrel_antonym': set(),\n",
    "        'semrel_hypernym': set(),\n",
    "        'semrel_hyponym': set()\n",
    "    }\n",
    "\n",
    "    # Convert the input word to all of its synsets.\n",
    "    ss = word_to_synsets(word)\n",
    "\n",
    "    # Go through each synset, determining its antonyms, hypernyms, and hyponyms, and adding each to the set in the\n",
    "    # appropriate entry of the dictionary, as long as the main word does not appear as part of any of those strings.\n",
    "    for s in ss:\n",
    "        semrel_dict['semrel_antonym'].update( [w for w in get_antonyms(s) if word not in w] )\n",
    "        semrel_dict['semrel_hypernym'].update( [w for w in get_hypernyms(s) if word not in w] )\n",
    "        semrel_dict['semrel_hyponym'].update( [w for w in get_hyponyms(s) if word not in w] )\n",
    "\n",
    "    return semrel_dict\n",
    "\n",
    "\n",
    "def get_collocations(word, forbidden_wds, gensim_model, num_collocates, num_to_check = 10):\n",
    "    \"\"\"\n",
    "    Returns minimum num_collocates most similar words to the given word based on gensim word embeddings.\n",
    "\n",
    "    Arg:\n",
    "        word: A string representing the main word.\n",
    "        forbidden_wds: A set containing words as strings that may not be included as output.\n",
    "        gensim_model: The pre-trained word embeddings.\n",
    "        num_collocates: An integer, the number of collocates to generate.\n",
    "        num_to_check: (default 10) the number of most similar words to begin with.\n",
    "    Returns:\n",
    "        A list of collocated words as strings.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Use gensim's most_similar() function to get the (initially ten) words whose embeddings are most similar to the\n",
    "    # input word's. Lemmatise the words to remove plural/other inflections.\n",
    "\n",
    "    try:  # If the MW is in the word2vec vocabulary\n",
    "        similar_tups = gensim_model.most_similar(word, topn=num_to_check)\n",
    "        similar_wds = [lemmatizer.lemmatize( tup[0] ) for tup in similar_tups]\n",
    "\n",
    "        # Now save only a subset of those similar words, namely the ones that:\n",
    "        # - do not contain the main word\n",
    "        # - are not in the passed-in set of forbidden words\n",
    "        # - have a Levenshtein distance of up to 3 from the MW (i.e. are similar, probably typos)\n",
    "        # - words with underscores in them, indicating multi-word units (often pretty weird)\n",
    "        filtered = [wd for wd in similar_wds if (word not in wd.lower() and wd not in forbidden_wds and edit_distance(word, wd) > 4 and '_' not in wd)]\n",
    "        filtered = set(filtered)\n",
    "\n",
    "        # Recursive bit: Check if there are at least num_collocates different words in filtered (base case).\n",
    "        # If not, increase the number of words to check in each recursive iteration by three and run the function again.\n",
    "        # Will stop once there are minimum num_collocates words in filtered.\n",
    "\n",
    "        if len(filtered) >= num_collocates:\n",
    "            return filtered\n",
    "        else:\n",
    "            num_to_check += 3\n",
    "            return get_collocations(word, forbidden_wds, gensim_model, num_collocates, num_to_check)\n",
    "\n",
    "    except KeyError:  # arises when input word not in vocab\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Getting out text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "#we will be using the full version of the descriptive corpus we made ~115k sentences\n",
    "f = open('description-corpus-115k.txt', 'r')\n",
    "#text = f.read()\n",
    "text = f.readlines() #if we want a list with sentences as elements\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#getting the avg length of a descriptive sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Cleaning (NOT USED)\n",
    "* Removing stop words, punctuation symbols and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# found that in some tutorials they do 3 extra cleaning steps before applying N-grams\n",
    "\n",
    "# getting rid of stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_free = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "# justification --> it's like getting rid of the long tail in the word frequency plot. Oder?\n",
    "\n",
    "# getting rid of punctuation\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "#punct_free = \"\".join(word for word in stop_free if word not in punctuation_symbols)\n",
    "# makes sense.. I think\n",
    "\n",
    "# lemmatizing?\n",
    "#lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#normalized = ' '.join(lemma.lemmatize(word) for word in punct_free.split())\n",
    "#if it is what I think it is, then it makes sense too\n",
    "\n",
    "#last step, lower case and splitting\n",
    "#cleaned_text = normalized.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stop_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "punct_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "normalized[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Only normalizing (NOT USED)\n",
    "* is it worth it having different tokens for cookie and cookies (in terms of having a correct sentence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Implementing trigrams and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating trigram sets EASY WAY\n",
    "#trigrams = [([cleaned_text[i], cleaned_text[i+1]], cleaned_text[i+2])for i in range(len(cleaned_text) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# better way: sentence by sentence, not the whole text in one go\n",
    "# this structure allows us to create context/target sets for each word. \n",
    "trigrams = []\n",
    "for sentence in sentences:\n",
    "    trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['it', 'does'], 'not')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[0]\n",
    "#context for target word 'text' --> 'a' and 'cookie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#to prototype we will only take 5000 trigrams out of the ~500000\n",
    "trigrams = trigrams[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting set of words in vocab, it's length and the frequency of each word\n",
    "voc = set()\n",
    "for sentence in sentences:\n",
    "    voc = voc.union(set(sentence))\n",
    "voc_length = len(voc) #34174\n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Because we detected some errors on how our corpus is tokenized (\"binoculars\" is imported as bi - no -cu-lars) we will remove the 5% less frequent found words in the dictionary.\n",
    "# THIS AFFECTED THE CELL BELOW. INSTEAD OF USING THIS CODE TO CLEAN THE POSSIBLE OUTPUTS WE WILL ASSUME THAT USING OUR BIGGER VERSION OF THE CORPUS SHOULD MINIMIZE \n",
    "# THE LIKELIHOOD OF FINDING THESE WEIRD TOKENS DURING THE GENERATION STEP\n",
    "\n",
    "\n",
    "# freq values from dictionary to array\n",
    "#freq_values = np.array([v for k, v in word_to_freq.items()])\n",
    "#np.percentile(freq_values, 5) # ~1709\n",
    "\n",
    "#getting rid of those unusual values\n",
    "# creating list of keys to delete\n",
    "#to_del = [k for k, v in word_to_freq.items() if v < 1709]\n",
    "\n",
    "# deleting those elements from the word_to_freq dict\n",
    "#for k in to_del: del word_to_freq[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to inp\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Building the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bad, training on CPU; dont exagerate with number of epochs.\n"
     ]
    }
   ],
   "source": [
    "#Cheking if we have access to training on GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Fancy setup!')\n",
    "else: \n",
    "    print('Too bad, training on CPU; dont exagerate with number of epochs.')\n",
    "\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train(context, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(trigrams)):\n",
    "        output, hidden = decoder(context[t], hidden)\n",
    "        loss += criterion(output, target[t])\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data.item() / len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3m 1s (10 16%) 4.8687]\n",
      "[6m 0s (20 33%) 3.6161]\n",
      "[8m 59s (30 50%) 2.4126]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "print_every = 10\n",
    "plot_every = 10\n",
    "hidden_s = 50\n",
    "n_layers = 1\n",
    "lr = 0.015\n",
    "\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(cont,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a3d9cbbd0>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in prime_str.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main defined can , \" number reserved for christmas chestnut and\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('the main', 10, temperature = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Generating descriptive sentence\n",
    "* input = main word + taboo words\n",
    "\n",
    "The main idea here is that we will seed always with the main word and replace at the end.\n",
    "The descriptive sentence will be accepted into the cleaning step (where we will make sure that no tws or the mw were used and if so, we'll replace them) if it contains at least two of the input words, besides the seeds. As long as the sentence is not accepted, we will keep generating.\n",
    "\n",
    "To help reach a high score: once an input word has been used and its score_vector value increases, we will leave that segment in our sentence. Not sure if this hurts more than it helps, but adding the input word as a seed is VERY complicated. And not sure it would make sense either.\n",
    "\n",
    "For a first prototype we will assume that maximum one new word is gonna be added per iteration. \n",
    "\n",
    "Pretty fragile with many assumptions?\n",
    "\n",
    "It would be useful to generate a list of synonyms of all input words to expand the input_words set and have higher acceptance rate. We thought about cleaning the input words set from the start but those are words with high probability of occurence so we better leave them and clean in the end.\n",
    "\n",
    "* maybe more seeds?\n",
    "* maybe require a higher score?\n",
    "* how to properly connect end of sentences and seeds? Is it better to have fewer seeds but longer auto-generated text?\n",
    "* add time it to report results and \"quantify\" results/improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semrel_synonym': set(),\n",
       " 'semrel_antonym': set(),\n",
       " 'semrel_hypernym': set(),\n",
       " 'semrel_hyponym': set()}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_semrel_dict('rodrigo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3 seeds (means, is, refers to). i=10\n",
    "Final score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# set of words that we hope will appear in the description\n",
    "input_words = np.array(['happy', 'pleased', 'thrilled', 'excited', 'ecstatic', 'overjoyed', 'joy'])\n",
    "\n",
    "# extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "\n",
    "\n",
    "#probably good idea to check if those words are even in our vocab. jeje\n",
    "# filtering out the ones that are not. Shouldn't be a thing when using larger corpus\n",
    "input_words = [word for word in input_words if word in voc]\n",
    "\n",
    "#create the first sentence\n",
    "#on average a descriptive sentence had 27 words/symbols.\n",
    "# we will equally divide them between our seeds\n",
    "\n",
    "sentence_parts = np.array([evaluate('happy means', 7, temperature = 1), evaluate('happy is', 7, temperature = 1), evaluate('happy refers to', 6, temperature = 1)])\n",
    "\n",
    "sentence =  \" \".join(sentence_parts)\n",
    "\n",
    "eval_sentence = sentence.split()\n",
    "# first score vector and score\n",
    "score_vector = np.array([eval_sentence.count(input_words[x]) for x in range(len(input_words))])\n",
    "score_vector[0] -= 3\n",
    "score = np.sum(score_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy means out cold be found of realizing emergency happy is the diabetes the can three the contributing happy refers to the before diameter your peice side'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# iterate until nice sentence comes up\n",
    "# we will add safety measure to not break everything\n",
    "i = 0\n",
    "n = 10\n",
    "index_in_sentence = -1\n",
    "# to keep track of scores\n",
    "scores = np.zeros(n)\n",
    "# the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "covered = np.array([0,0,0])\n",
    "changes = np.zeros(len(score_vector))\n",
    "\n",
    "#known positions of input words in our sentence\n",
    "positions = np.zeros(len(eval_sentence))\n",
    "#we know the positions of the seeds\n",
    "positions[0] = 1\n",
    "positions[9] = 1\n",
    "positions[18] = 1\n",
    "\n",
    "while i < n:\n",
    "    #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "    \n",
    "    #checking if score improved\n",
    "    new_score_vector = np.array([eval_sentence.count(input_words[x]) for x in range(len(input_words))])\n",
    "    new_score_vector[0] -= 3\n",
    "    changes = new_score_vector - score_vector\n",
    "    \n",
    "    if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "        index = np.where(changes != 0)[0][0] #looking for the word that was added\n",
    "        word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "        #finding in which segment that new added word is in order to leave the segment untouched\n",
    "        \n",
    "        #how to detect the index of the word that just came up if it was already in the sentence somewhere else?\n",
    "        #this should do\n",
    "        indices_in_sentence = np.where(eval_sentence == word_that_was_added)[0]\n",
    "        if len(indices_in_sentence) >1: #word appears at least twice\n",
    "            for d in indices_in_sentence:\n",
    "                if positions[d] != 1:\n",
    "                    index_in_sentence = d\n",
    "                    positions[d] = 1\n",
    "        else:\n",
    "            index_in_sentence = indices_in_sentence[0]\n",
    "            positions[index_in_sentence] = 1\n",
    "        #keeping the segment in which the improvement took place\n",
    "        if index_in_sentence in range(9) & covered[0]!=1:\n",
    "            sentence_parts[1] = evaluate('happy is', 7, temperature = 1)\n",
    "            sentence_parts[2] = evaluate('happy refers to', 6, temperature = 1)\n",
    "            sentence = ' '.join(sentence_parts)\n",
    "            covered[0] = 1\n",
    "        elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "            sentence_parts[0] = evaluate('happy means', 7, temperature = 1)\n",
    "            sentence_parts[2] = evaluate('happy refers to', 6, temperature = 1)\n",
    "            sentence = ' '.join(sentence_parts)\n",
    "            covered[1] = 1\n",
    "        elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "            sentence_parts[1] = evaluate('happy is', 7, temperature = 1)\n",
    "            sentence_parts[0] = evaluate('happy means', 7, temperature = 1)\n",
    "            sentence = ' '.join(sentence_parts)\n",
    "            covered[2] = 1\n",
    "        eval_sentence = sentence.split()\n",
    "        changes = np.zeros(len(score_vector))\n",
    "        index_in_sentence = 0\n",
    "        score_vector = new_score_vector\n",
    "        score = np.sum(score_vector)\n",
    "    \n",
    "    #if there was no change\n",
    "    else: #based on what is already covered\n",
    "        for r in range(len(covered)):\n",
    "            if covered[r] !=1 & r==0:\n",
    "                sentence_parts[r] = evaluate('happy means', 7, temperature = 1) +' '\n",
    "            if covered[r] !=1 & r==1:\n",
    "                sentence_parts[r] = evaluate('happy is', 7, temperature = 1) +' '\n",
    "            if covered[r] !=1 & r==2:\n",
    "                sentence_parts[r] = evaluate('happy refers to', 6, temperature = 1)\n",
    "        sentence = ' '.join(sentence_parts)\n",
    "        eval_sentence = sentence.split()\n",
    "        score_vector = new_score_vector\n",
    "        score = np.sum(score_vector)\n",
    "        \n",
    "            \n",
    "    \n",
    "    scores[i] = score\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -3., -3., -3., -3., -3., -3., -3., -3., -3.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
