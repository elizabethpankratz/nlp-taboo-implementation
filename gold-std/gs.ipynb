{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a gold standard from existing Taboo cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and formatting the cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Taboo cards that our gold standard will be based on belong to Elizabeth's Canadian edition of Taboo, produced sometime in the 1990s or early 2000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"taboo_cards.txt\"\n",
    "\n",
    "\n",
    "def read_in(filename):\n",
    "    \"\"\"\n",
    "    Reads in transcribed Taboo card words contained in the given file and returns them in an enumerated list.\n",
    "    \"\"\"\n",
    "    file_lines = []\n",
    "    \n",
    "    with open(filename, \"r\", encoding='utf-8') as myfile:\n",
    "        \n",
    "        # Go through every line, saving non-empty ones to the list file_lines.\n",
    "        for line in myfile:\n",
    "            if line.strip() != '':             \n",
    "                file_lines.append(line.strip()) \n",
    "                \n",
    "    return list(enumerate(file_lines))\n",
    "    \n",
    "            \n",
    "def format_cards(enum_list):\n",
    "    \"\"\"\n",
    "    Given an enumerated list (output of read_in()), formats the contents as a dictionary (key = MW, values = list of TWs)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise dictionary to contain card data.\n",
    "    card_dict = dict()\n",
    "    \n",
    "    # Assign MWs (every sixth word in the enumerated list) as dictionary keys, and create a list for the dict's\n",
    "    # value consisting of the five following words (the TWs); the word[1:] removes the dash from the beginning of\n",
    "    # each TW's string.\n",
    "\n",
    "    for enum, wd in enum_list:\n",
    "        if enum % 6 == 0:\n",
    "            card_dict[wd] = [word[1:] for num, word in enum_list[enum+1:enum+6]]\n",
    "        \n",
    "    return card_dict\n",
    "        \n",
    "\n",
    "enum_lines = read_in(FILENAME)\n",
    "cards = format_cards(enum_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maple', 'pancakes', 'trees', 'sap', 'sweet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: the five TWs from the MW 'syrup'\n",
    "cards['syrup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this dictionary format, it will be helpful to set up a pandas dataframe for easy addition of columns for semantic similarity and corpus-based collocation measures below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw\n",
       "0  huddle    gather\n",
       "1  huddle  football\n",
       "2  huddle     group\n",
       "3  huddle      play\n",
       "4  huddle  together"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas dataframe quickly based on a list of dictionaries, with each dictionary corresponding to a row in the \n",
    "# dataframe.\n",
    "# ( Source: https://stackoverflow.com/questions/10715965/add-one-row-to-pandas-dataframe/17496530#17496530 )\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "for mainwd, tabwds in cards.items():\n",
    "    for tabwd in tabwds:\n",
    "        \n",
    "        # Create a dictionary for each row of the dataframe (key = column name, value = row value for that column)\n",
    "        row = {\n",
    "            'mw': mainwd,\n",
    "            'tw': tabwd\n",
    "        }\n",
    "        \n",
    "        # Append to rows_list, and use that list as a basis for the new dataframe.\n",
    "        rows_list.append(row)\n",
    "\n",
    "data = pd.DataFrame(rows_list)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to csv for manual annotation\n",
    "#data.to_csv(r'gold-std-raw.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "      <th>semrel_synonym</th>\n",
       "      <th>semrel_antonym</th>\n",
       "      <th>semrel_hyponym</th>\n",
       "      <th>semrel_hypernym</th>\n",
       "      <th>collocation</th>\n",
       "      <th>cultural_ref</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw  semrel_synonym  semrel_antonym  semrel_hyponym  \\\n",
       "0  huddle    gather             0.0             0.0             0.0   \n",
       "1  huddle  football             0.0             0.0             0.0   \n",
       "2  huddle     group             0.0             0.0             0.0   \n",
       "3  huddle      play             0.0             0.0             0.0   \n",
       "4  huddle  together             0.0             0.0             0.0   \n",
       "\n",
       "   semrel_hypernym  collocation  cultural_ref  other  \n",
       "0              1.0          0.0           0.0    0.0  \n",
       "1              0.0          1.0           0.0    0.0  \n",
       "2              1.0          0.0           0.0    0.0  \n",
       "3              0.0          1.0           0.0    0.0  \n",
       "4              0.0          1.0           0.0    0.0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After manual annotation, read the csv back in and save as pandas dataframe, replacing NaNs with 0.\n",
    "# (30.1.20: Currently working on a smaller development version with just 15 annotated lines)\n",
    "\n",
    "gs_df = pd.read_csv('gold-std-ep-dev.csv')\n",
    "gs_df.fillna(0,inplace=True)\n",
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and plot the number of categories appearing in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the sum of each column as a series.\n",
    "gs_df_sum = gs_df.loc[:, 'semrel_synonym':'other'].sum()\n",
    "\n",
    "# Create a bar plot object out of it.\n",
    "# (Plot y axis labels as integers by creating a list of integers based on the y range)\n",
    "yint = [x for x in range( int(gs_df_sum.max()) + 1 )]\n",
    "gs_df_sum_plot = gs_df_sum.plot.bar(x='category', y='count', yticks=yint)\n",
    "\n",
    "# For readable x axis tick labels, set their rotation angle to 30 and the horizontal alignment to right.\n",
    "gs_df_sum_plot.set_xticklabels(gs_df_sum_plot.get_xticklabels(), rotation=30, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing semantic similarity with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# The following line of code requires that the large word embeddings file is in the current directory (not set up on\n",
    "# GitHub because it is too large to push around nicely, so for replication, have to add this file manually).\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A SOBERING REALISATION\n",
    "\n",
    "- In the end, what follows (computing semantic similarity for the collocates) might be interesting as a curiosity, but probably can't actually be used in generating the Taboo cards, because we can't search for words whose similarities fall into a given range using gensim (it's only possible to go the other way, from words to similarities, not from similarities to words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "      <th>semrel_synonym</th>\n",
       "      <th>semrel_antonym</th>\n",
       "      <th>semrel_hyponym</th>\n",
       "      <th>semrel_hypernym</th>\n",
       "      <th>collocation</th>\n",
       "      <th>cultural_ref</th>\n",
       "      <th>other</th>\n",
       "      <th>simil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>croak</td>\n",
       "      <td>frog</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>croak</td>\n",
       "      <td>die</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>croak</td>\n",
       "      <td>kick</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>croak</td>\n",
       "      <td>sound</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>croak</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>positive</td>\n",
       "      <td>plus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>positive</td>\n",
       "      <td>certain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>positive</td>\n",
       "      <td>sure</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mw        tw  semrel_synonym  semrel_antonym  semrel_hyponym  \\\n",
       "0     huddle    gather             0.0             0.0             0.0   \n",
       "1     huddle  football             0.0             0.0             0.0   \n",
       "2     huddle     group             0.0             0.0             0.0   \n",
       "3     huddle      play             0.0             0.0             0.0   \n",
       "4     huddle  together             0.0             0.0             0.0   \n",
       "5      croak      frog             0.0             0.0             0.0   \n",
       "6      croak       die             1.0             0.0             0.0   \n",
       "7      croak      kick             0.0             0.0             0.0   \n",
       "8      croak     sound             0.0             0.0             0.0   \n",
       "9      croak     noise             0.0             0.0             0.0   \n",
       "10  positive  negative             0.0             1.0             0.0   \n",
       "11  positive      plus             1.0             0.0             0.0   \n",
       "12  positive      good             1.0             0.0             0.0   \n",
       "13  positive   certain             1.0             0.0             0.0   \n",
       "14  positive      sure             1.0             0.0             0.0   \n",
       "\n",
       "    semrel_hypernym  collocation  cultural_ref  other     simil  \n",
       "0               1.0          0.0           0.0    0.0       NaN  \n",
       "1               0.0          1.0           0.0    0.0  0.314302  \n",
       "2               1.0          0.0           0.0    0.0       NaN  \n",
       "3               0.0          1.0           0.0    0.0  0.299363  \n",
       "4               0.0          1.0           0.0    0.0  0.256514  \n",
       "5               0.0          1.0           0.0    0.0  0.365087  \n",
       "6               0.0          0.0           0.0    0.0       NaN  \n",
       "7               0.0          0.0           0.0    1.0       NaN  \n",
       "8               1.0          0.0           0.0    0.0       NaN  \n",
       "9               1.0          0.0           0.0    0.0       NaN  \n",
       "10              0.0          0.0           0.0    0.0       NaN  \n",
       "11              0.0          0.0           0.0    0.0       NaN  \n",
       "12              0.0          0.0           0.0    0.0       NaN  \n",
       "13              0.0          0.0           0.0    0.0       NaN  \n",
       "14              0.0          0.0           0.0    0.0       NaN  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_similarity_to_df(df):\n",
    "    \"\"\"\n",
    "    Computes the similarity between the MW in the first column and the TW in the second of the passed-in dataframe,\n",
    "    when the TW is flagged in the column collocation.\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe containing cleaned corpus data\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe with a new column 'simil' containing word2vec similarity values for all words classified as collocations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise empty list to collect similarity values as we go.\n",
    "    simil_col=[]\n",
    "    \n",
    "    # Iterate through rows in the dataframe.\n",
    "    for row_idx in range(len(df)):\n",
    "        \n",
    "        # Check if the value in the collocation column is 1.\n",
    "        if df.loc[row_idx, ['collocation']][0] == 1:\n",
    "        \n",
    "            # Get the MW and the TW in the current row.\n",
    "            mw = df.loc[row_idx, ['mw']][0]\n",
    "            tw = df.loc[row_idx, ['tw']][0]\n",
    "\n",
    "            # KeyError raised if word not in the word2vec vocabulary, so if that happens, add numpy's null value\n",
    "            # to the column instead.\n",
    "            try:\n",
    "                value = model.similarity(mw, tw)\n",
    "            except KeyError:\n",
    "                value = np.nan\n",
    "        \n",
    "        else:\n",
    "            value = np.nan\n",
    "        \n",
    "        # Add similarity value to the list. At the end, this list will be of the same length\n",
    "        # as the dataframe and will contain a similarity value (or NaN) for each adjective pair.\n",
    "        simil_col.append(value)\n",
    "    \n",
    "    # Add this list as a new column to the dataframe and return dataframe.\n",
    "    df['simil'] = simil_col\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "gs_df = add_similarity_to_df(gs_df)\n",
    "gs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "      <th>semrel_synonym</th>\n",
       "      <th>semrel_antonym</th>\n",
       "      <th>semrel_hyponym</th>\n",
       "      <th>semrel_hypernym</th>\n",
       "      <th>collocation</th>\n",
       "      <th>cultural_ref</th>\n",
       "      <th>other</th>\n",
       "      <th>simil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.256514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw  semrel_synonym  semrel_antonym  semrel_hyponym  \\\n",
       "0  huddle    gather             0.0             0.0             0.0   \n",
       "1  huddle  football             0.0             0.0             0.0   \n",
       "2  huddle     group             0.0             0.0             0.0   \n",
       "3  huddle      play             0.0             0.0             0.0   \n",
       "4  huddle  together             0.0             0.0             0.0   \n",
       "\n",
       "   semrel_hypernym  collocation  cultural_ref  other     simil  \n",
       "0              1.0          0.0           0.0    0.0       NaN  \n",
       "1              0.0          1.0           0.0    0.0  0.314302  \n",
       "2              1.0          0.0           0.0    0.0       NaN  \n",
       "3              0.0          1.0           0.0    0.0  0.299363  \n",
       "4              0.0          1.0           0.0    0.0  0.256514  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example\n",
    "gs_df[gs_df['mw'] == 'huddle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing the semantic similarity results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.308816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.044801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.256514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.288651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.306832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.326998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.365087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          simil\n",
       "count  4.000000\n",
       "mean   0.308816\n",
       "std    0.044801\n",
       "min    0.256514\n",
       "25%    0.288651\n",
       "50%    0.306832\n",
       "75%    0.326998\n",
       "max    0.365087"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at how the data are distributed.\n",
    "# First make a dataframe from only the similarity column, for easier plotting.\n",
    "\n",
    "simil_df = pd.DataFrame(gs_df['simil'])\n",
    "simil_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simil_df.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPXklEQVR4nO3dcaydd13H8ffHbgUsREyGN7EdaxNKskLHkEtHVNzNUrCzcYNoYrcQR4KpM2tcshApYU5YII7OjIRQCVVJULLUQdRUWlpw2cHMsLjWddRumSsVWK0Rp5NRwK3dvv5xT+Xs9rT3ub333Nv++n4lTc7zPL/nPL+TnL7v06f3PCdVhSSpXT+x0BOQJI2WoZekxhl6SWqcoZekxhl6SWrcRQs9gakuueSSWr58+UJPQxrqBz/4AUuWLFnoaUin2Ldv39NV9Zph28650C9fvpy9e/cu9DSkoXq9HhMTEws9DekUSb59um1eupGkxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWrcOfeBKWm+JJmX4/idD1pontHrglVVM/5z2Qe+NON9pIVm6CWpcYZekhpn6CWpcYZekhrXKfRJ1iV5IsmhJJuHbL85yYEk+5M8mGTVwLYrknw9ycH+mJfP5QuQJJ3ZtKFPsgjYClwLrAJuGAx5371VtbqqrgS2APf0970I+Dxwc1W9AZgAjs/d9CVJ0+lyRr8GOFRVh6vqeWA7cP3ggKp6dmBxCXDyd8reCXyjqh7tj/uvqnph9tOWJHXVJfRLgacGlo/0171EkluSfJPJM/rf7a9+PVBJ9iT5pyS/N9sJS5JmpssnY4d9fPCUT4FU1VZga5IbgduBm/rP/4vAW4EfAvcn2VdV97/kAMlGYCPA2NgYvV5vJq9Bmle+P3W+6RL6I8ClA8vLgKNnGL8d+PTAvl+rqqcBkuwCfg54SeirahuwDWB8fLz8Tk6ds3bv9Dtjdd7pcunmYWBlkhVJFgMbgB2DA5KsHFhcDzzZf7wHuCLJT/b/Y/Zq4LHZT1uS1NW0Z/RVdSLJJiajvQj4bFUdTHInsLeqdgCbkqxl8jdqnmHysg1V9UySe5j8YVHArqraOaLXIkkaotPdK6tqF7Bryro7Bh7feoZ9P8/kr1hKkhaAn4yVpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqXKfQJ1mX5Ikkh5JsHrL95iQHkuxP8mCSVVO2vzbJsSTvn6uJS5K6mTb0SRYBW4FrgVXADVNDDtxbVaur6kpgC3DPlO2fAL48B/OVJM1QlzP6NcChqjpcVc8D24HrBwdU1bMDi0uAOrmQ5F3AYeDg7KcrSZqpizqMWQo8NbB8BLhq6qAktwC3AYuBa/rrlgAfAN4BnPayTZKNwEaAsbExer1et9lLC8D3p843XUKfIevqlBVVW4GtSW4EbgduAj4CfKKqjiXDnub/990GbAMYHx+viYmJDtOSFsDunfj+1PmmS+iPAJcOLC8Djp5h/Hbg0/3HVwG/nmQL8GrgxST/W1WfOpvJSpJmrkvoHwZWJlkB/BuwAbhxcECSlVX1ZH9xPfAkQFW9fWDMh4FjRl6S5te0oa+qE0k2AXuARcBnq+pgkjuBvVW1A9iUZC1wHHiGycs2kqRzQJczeqpqF7Bryro7Bh7f2uE5PjzTyUmSZs9PxkpS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDWu01cJSueDN33kK3zvR8dHfpzlm3eO9Pl/6hUX8+gfvHOkx9CFxdCrGd/70XG+ddf6kR6j1+sxMTEx0mOM+geJLjxeupGkxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxnUKfZJ1SZ5IcijJ5iHbb05yIMn+JA8mWdVf/44k+/rb9iW5Zq5fgCTpzKYNfZJFwFbgWmAVcMPJkA+4t6pWV9WVwBbgnv76p4FfrarVwE3AX8zZzCVJnXQ5o18DHKqqw1X1PLAduH5wQFU9O7C4BKj++keq6mh//UHg5UleNvtpS5K66nILhKXAUwPLR4Crpg5KcgtwG7AYGHaJ5teAR6rquSH7bgQ2AoyNjdHr9TpMSzrVqN87x44dm5f3p38HNJe6hD5D1tUpK6q2AluT3AjczuSlmsknSN4AfBwYeqemqtoGbAMYHx+vUd9LRI3avXPk96GZj3vdzMfr0IWly6WbI8ClA8vLgKOnGQuTl3bedXIhyTLgr4HfrKpvns0kJUlnr0voHwZWJlmRZDGwAdgxOCDJyoHF9cCT/fWvBnYCH6yqf5ibKUuSZmLa0FfVCWATsAd4HLivqg4muTPJdf1hm5IcTLKfyev0Jy/bbAJeB/x+/1cv9yf5mbl/GZKk0+l0P/qq2gXsmrLujoHHt55mv48CH53NBCVJs+MnYyWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhrX6asEpfPBqy7fzOrPbR79gT432qd/1eUA60d7EF1QDL2a8f3H7+Jbd402kL1ej4mJiZEeY/nmnSN9fl14vHQjSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUuE6hT7IuyRNJDiU55aOHSW5OciDJ/iQPJlk1sO2D/f2eSPLLczl5SdL0pg19kkXAVuBaYBVww2DI++6tqtVVdSWwBbinv+8qYAPwBmAd8Mf955MkzZMuZ/RrgENVdbiqnge2A9cPDqiqZwcWlwDVf3w9sL2qnquqfwUO9Z9PkjRPutzrZinw1MDyEeCqqYOS3ALcBiwGrhnY96Ep+y4dsu9GYCPA2NgYvV6vw7SkU436vXPs2LF5eX/6d0BzqUvoM2RdnbKiaiuwNcmNwO3ATTPYdxuwDWB8fLxGfdMoNWr3zpHfcGw+bmo2H69DF5Yul26OAJcOLC8Djp5h/HbgXWe5ryRpjnUJ/cPAyiQrkixm8j9XdwwOSLJyYHE98GT/8Q5gQ5KXJVkBrAT+cfbTliR1Ne2lm6o6kWQTsAdYBHy2qg4muRPYW1U7gE1J1gLHgWeYvGxDf9x9wGPACeCWqnphRK9FkjREpy8eqapdwK4p6+4YeHzrGfb9GPCxs52gJGl2/GSsJDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4zrdplg6XyzfvHP0B9k92mP81CsuHunz68Jj6NWMb921fuTHWL5557wcR5pLXrqRpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMZ1Cn2SdUmeSHIoyeYh229L8liSbyS5P8llA9u2JDmY5PEkn0ySuXwBkqQzmzb0SRYBW4FrgVXADUlWTRn2CDBeVVcAXwS29Pf9eeAXgCuANwJvBa6es9lLkqbV5Yx+DXCoqg5X1fPAduD6wQFV9UBV/bC/+BCw7OQm4OXAYuBlwMXAf8zFxCVJ3XS5TfFS4KmB5SPAVWcY/z7gywBV9fUkDwD/DgT4VFU9PnWHJBuBjQBjY2P0er1Ok5cWgu9PnW+6hH7YNfUaOjB5DzBO//JMktcBl/PjM/yvJvmlqvr7lzxZ1TZgG8D4+HhNTEx0mrw073bvxPenzjddLt0cAS4dWF4GHJ06KMla4EPAdVX1XH/1u4GHqupYVR1j8kz/bbObsiRpJrqE/mFgZZIVSRYDG4AdgwOSvBn4DJOR/+7Apu8AVye5KMnFTJ7pn3LpRpI0OtOGvqpOAJuAPUxG+r6qOpjkziTX9YfdDbwS+EKS/UlO/iD4IvBN4ADwKPBoVf3tXL8ISdLpdfrO2KraBeyasu6OgcdrT7PfC8Bvz2aCkqTZ8ZOxktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktS4TqFPsi7JE0kOJdk8ZPttSR5L8o0k9ye5bGDba5N8Jcnj/THL5276kqTpTBv6JIuArcC1wCrghiSrpgx7BBivqiuALwJbBrb9OXB3VV0OrAG+OxcTlyR10+WMfg1wqKoOV9XzwHbg+sEBVfVAVf2wv/gQsAyg/wPhoqr6an/csYFxkqR50CX0S4GnBpaP9NedzvuAL/cfvx74nyR/leSRJHf3/4UgSZonF3UYkyHraujA5D3AOHD1wPO/HXgz8B3gL4H3An82Zb+NwEaAsbExer1eh2lJC8P3p843XUJ/BLh0YHkZcHTqoCRrgQ8BV1fVcwP7PlJVh/tj/gZ4G1NCX1XbgG0A4+PjNTExMbNXIc2X3Tvx/anzTZdLNw8DK5OsSLIY2ADsGByQ5M3AZ4Drquq7U/b96SSv6S9fAzw2+2lLkrqaNvRVdQLYBOwBHgfuq6qDSe5Mcl1/2N3AK4EvJNmfZEd/3xeA9wP3JznA5GWgPxnB65AknUaXSzdU1S5g15R1dww8XnuGfb8KXHG2E5QkzY6fjJWkxnU6o5dalAz7hbIO+318ZuOrhv6SmjRvPKPXBauqZvzngQcemPE+0kIz9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY3LufaBjiT/CXx7oechncYlwNMLPQlpiMuq6jXDNpxzoZfOZUn2VtX4Qs9Dmgkv3UhS4wy9JDXO0Eszs22hJyDNlNfoJalxntFLUuMMvSQ1ztBLQyT50ySrZjB+PMkn+4/fm+RTo5udNDN+laA0RFX91gzH7wX2jmg60qx4Rq8LXpIlSXYmeTTJPyf5jSS9JOP97ceSfDzJviR/l2RNf/vhJNf1x0wk+dLCvhJpOEMvwTrgaFW9qareCOyesn0J0KuqtwDfBz4KvAN4N3DnvM5UOguGXoIDwNr+Wfvbq+p7U7Y/z4/jfwD4WlUd7z9ePn/TlM6O1+h1wauqf0nyFuBXgD9M8pUpQ47Xjz9w8iLwXH+/F5P4d0jnPN+kuuAl+Vngv6vq80mOAe9d4ClJc8rQS7AauDvJi8Bx4HeAP1rYKUlzx1sgSFLj/M9YSWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWrc/wHKhGrssshWEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "simil_df.boxplot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAD BRAINSTORM\n",
    "\n",
    "- One idea was to treat the distribution of semantic similarity values like a Bayesian posterior, and then draw samples from it proportional to its density and try to find words within the given range of similarity to the MW\n",
    "- **Problem with that: can't return words by their vector similarity;** you need the words first to compute the similarity values.\n",
    "- So, maybe better just to rely on the classification for rough proportions of how much of each type of relation to the MW should go into the pool to choose from, and then just pick from the most similar words from `model.most_similar()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099379539489746),\n",
       " ('dog', 0.7609457969665527),\n",
       " ('kitten', 0.7464984655380249),\n",
       " ('feline', 0.7326234579086304),\n",
       " ('beagle', 0.7150583267211914),\n",
       " ('puppy', 0.7075453400611877),\n",
       " ('pup', 0.6934291124343872),\n",
       " ('pet', 0.6891531348228455),\n",
       " ('felines', 0.6755931377410889),\n",
       " ('chihuahua', 0.6709762215614319)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet semantic relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit worked well! Using WordNet, I created dictionaries for a given input word that contain all of that word's synonyms, antonyms, hypernyms, and hyponyms :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_to_synsets(word):\n",
    "    \"\"\"\n",
    "    Converts the given word to a synset object.\n",
    "    \n",
    "    Arg:\n",
    "        word: a string like 'cat'\n",
    "        pos: the desired part of speech (choices: wn.NOUN, wn.VERB, wn.ADJ, wn.ADV)\n",
    "    Returns:\n",
    "        A string containing the first synset ID, formatted according to WordNet's conventions, e.g. 'cat.n.01',\n",
    "        corresponding to that word.\n",
    "    \"\"\"\n",
    "    # Convert word string to the synset with the corresponding part of speech.\n",
    "    return wn.synsets(word)\n",
    "    \n",
    "\n",
    "def synset_to_word(synset):\n",
    "    \"\"\"\n",
    "    Converts the given synset to the actual word it represents.\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A string containing the word corresponding to that synset.\n",
    "    \"\"\"   \n",
    "    # Convert synset to lemma, since this is what name() is defined over.\n",
    "    return synset.lemmas()[0].name()\n",
    "\n",
    "\n",
    "def get_antonyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all antonyms for the given synset.\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of antonymic words as strings, if there are any, or else the empty list.\n",
    "    \"\"\"    \n",
    "    # Convert synset to lemma, since this is what the antonym relation is defined over, and get antonym(s).\n",
    "    ant_lemmas = synset.lemmas()[0].antonyms()\n",
    "    \n",
    "    # Convert each antonym in this list to a string and return list (empty if no antonyms).\n",
    "    return [ant_lemma.name() for ant_lemma in ant_lemmas]\n",
    "\n",
    "\n",
    "def get_hypernyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all immediate hypernyms for the given synset.\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of hypernymic words as strings.\n",
    "    \"\"\"\n",
    "    # Convert hypernyms of the synset to strings and return list.\n",
    "    return [synset_to_word(hyper) for hyper in synset.hypernyms()]\n",
    "\n",
    "\n",
    "def get_hyponyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all immediate hyponyms for the given synset. (There are often many.)\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of hyponymic words as strings.\n",
    "    \"\"\"\n",
    "    # Convert hypernyms of the synset to strings and return list.\n",
    "    return [synset_to_word(hypo) for hypo in synset.hyponyms()]\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Returns a set of synonyms, according to WordNet, for the given input word (using all of its senses, if\n",
    "    there are multiple).\n",
    "    \n",
    "    Arg:\n",
    "        word: a string representing the word whose synonyms we want.\n",
    "    Returns:\n",
    "        A set containing all of the other words in the same WordNet synset as the given word.\n",
    "    \"\"\"\n",
    "    # Initialise set that will collect the synonyms.\n",
    "    synonym_set = set()\n",
    "    \n",
    "    # Convert the word to a list of synsets.\n",
    "    synset_list = word_to_synsets(word)\n",
    "\n",
    "    # Get all the lemmas corresponding to the given word's synset.\n",
    "    synonym_lems = [x.lemmas() for x in synset_list]\n",
    "    \n",
    "    # Go through them, get the names from the lemma (lowercasing everything for consistency), and add\n",
    "    # to synonym_set.\n",
    "    for lemma_list in synonym_lems:\n",
    "        syn = lemma_list[0].name().lower()\n",
    "        synonym_set.update( [syn] )\n",
    "\n",
    "    # Return the synonym set with the input word removed.\n",
    "    return synonym_set.difference({word})\n",
    "\n",
    "\n",
    "def make_semrel_dict(word):\n",
    "    \"\"\"\n",
    "    Arg:\n",
    "        word: a string like 'cat'\n",
    "        pos: the desired part of speech (choices: wn.NOUN, wn.VERB, wn.ADJ, wn.ADV)\n",
    "    Returns:\n",
    "        A dictionary with the semantic relations as keys and a set of words that have that relation to all senses\n",
    "        of the input word, according to WordNet, as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise dictionary (and we can get synonyms right away).\n",
    "    semrel_dict = {\n",
    "        'synonym': get_synonyms(word),\n",
    "        'antonym': set(),\n",
    "        'hypernym': set(),\n",
    "        'hyponym': set()\n",
    "    }\n",
    "    \n",
    "    # Convert the input word to all of its synsets.\n",
    "    ss = word_to_synsets(word)\n",
    "    \n",
    "    # Go through each synset, determining its antonyms, hypernyms, and hyponyms, and adding each to the set in the\n",
    "    # appropriate entry of the dictionary.\n",
    "    for s in ss:\n",
    "        semrel_dict['antonym'].update( get_antonyms(s) )\n",
    "        semrel_dict['hypernym'].update( get_hypernyms(s) )\n",
    "        semrel_dict['hyponym'].update( get_hyponyms(s) )\n",
    "        \n",
    "    return semrel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synonym': {'bloom'},\n",
       " 'antonym': set(),\n",
       " 'hypernym': {'angiosperm',\n",
       "  'develop',\n",
       "  'reproductive_structure',\n",
       "  'time_period'},\n",
       " 'hyponym': {'African_daisy',\n",
       "  'African_violet',\n",
       "  'Carolina_spring_beauty',\n",
       "  'China_aster',\n",
       "  'Christmas_bells',\n",
       "  'Easter_daisy',\n",
       "  'Malcolm_stock',\n",
       "  'Mexican_sunflower',\n",
       "  'Swan_River_daisy',\n",
       "  'Texas_star',\n",
       "  'Virginia_spring_beauty',\n",
       "  'Virginian_stock',\n",
       "  'achimenes',\n",
       "  'ageratum',\n",
       "  'ammobium',\n",
       "  'anemone',\n",
       "  'apetalous_flower',\n",
       "  'aster',\n",
       "  \"baby's_breath\",\n",
       "  'bartonia',\n",
       "  'begonia',\n",
       "  'bellwort',\n",
       "  'billy_buttons',\n",
       "  'blazing_star',\n",
       "  'bloomer',\n",
       "  'blue-eyed_African_daisy',\n",
       "  'blue_daisy',\n",
       "  'brass_buttons',\n",
       "  'bud',\n",
       "  'bush_violet',\n",
       "  'butterfly_flower',\n",
       "  'calceolaria',\n",
       "  'calendula',\n",
       "  'calla_lily',\n",
       "  'candytuft',\n",
       "  'cape_marigold',\n",
       "  'catananche',\n",
       "  'centaury',\n",
       "  'chrysanthemum',\n",
       "  'cineraria',\n",
       "  'columbine',\n",
       "  'commelina',\n",
       "  'composite',\n",
       "  'coneflower',\n",
       "  'coral_drops',\n",
       "  'cornflower',\n",
       "  'corydalis',\n",
       "  'cosmos',\n",
       "  'cotton_rose',\n",
       "  'cowherb',\n",
       "  'cyclamen',\n",
       "  'dahlia',\n",
       "  'daisy',\n",
       "  'damask_violet',\n",
       "  'delphinium',\n",
       "  'effloresce',\n",
       "  'fig_marigold',\n",
       "  \"florest's_cineraria\",\n",
       "  'floret',\n",
       "  \"four_o'clock\",\n",
       "  'gazania',\n",
       "  'gentian',\n",
       "  'gerardia',\n",
       "  'globe_amaranth',\n",
       "  'golden_age',\n",
       "  'heliophila',\n",
       "  'horn_poppy',\n",
       "  'inflorescence',\n",
       "  'kingfisher_daisy',\n",
       "  'lace-flower_vine',\n",
       "  'lesser_celandine',\n",
       "  'lychnis',\n",
       "  'marigold',\n",
       "  'mistflower',\n",
       "  'nigella',\n",
       "  'orchid',\n",
       "  'oxeye_daisy',\n",
       "  'painted_daisy',\n",
       "  'peony',\n",
       "  'petunia',\n",
       "  \"pheasant's-eye\",\n",
       "  'pink',\n",
       "  'poppy',\n",
       "  'portulaca',\n",
       "  'prairie_rocket',\n",
       "  'ray_flower',\n",
       "  'red_valerian',\n",
       "  'rocket_larkspur',\n",
       "  'rue_anemone',\n",
       "  'sandwort',\n",
       "  'scabious',\n",
       "  'scarlet_musk_flower',\n",
       "  'schizopetalon',\n",
       "  'scorpionweed',\n",
       "  'shortia',\n",
       "  'silene',\n",
       "  'snapdragon',\n",
       "  'soapwort',\n",
       "  'sowbread',\n",
       "  'spathiphyllum',\n",
       "  'spring_beauty',\n",
       "  'stock',\n",
       "  \"stokes'_aster\",\n",
       "  'streptocarpus',\n",
       "  'sunflower',\n",
       "  'sweet_alyssum',\n",
       "  'sweet_sultan',\n",
       "  'tidytips',\n",
       "  'toadflax',\n",
       "  'tuberose',\n",
       "  'umbrellawort',\n",
       "  'ursinia',\n",
       "  'valerian',\n",
       "  'verbena',\n",
       "  'veronica',\n",
       "  'wallflower',\n",
       "  'wandflower',\n",
       "  'western_wall_flower',\n",
       "  'white-topped_aster',\n",
       "  'woodland_star',\n",
       "  'xeranthemum',\n",
       "  'zinnia'}}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_semrel_dict('flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synonym': {'adept',\n",
       "  'beneficial',\n",
       "  'commodity',\n",
       "  'dear',\n",
       "  'dependable',\n",
       "  'effective',\n",
       "  'estimable',\n",
       "  'full',\n",
       "  'thoroughly',\n",
       "  'well'},\n",
       " 'antonym': {'bad', 'evil', 'ill'},\n",
       " 'hypernym': {'advantage', 'artifact', 'morality', 'quality'},\n",
       " 'hyponym': {'basic',\n",
       "  'beneficence',\n",
       "  'benefit',\n",
       "  'benignity',\n",
       "  'better',\n",
       "  'common_good',\n",
       "  'consumer_goods',\n",
       "  'desirability',\n",
       "  'drygoods',\n",
       "  'entrant',\n",
       "  'export',\n",
       "  'fancy_goods',\n",
       "  'fungible',\n",
       "  'future',\n",
       "  'import',\n",
       "  'kindness',\n",
       "  'merchandise',\n",
       "  'middling',\n",
       "  'optimum',\n",
       "  'saintliness',\n",
       "  'salvage',\n",
       "  'shopping',\n",
       "  'sporting_goods',\n",
       "  'summum_bonum',\n",
       "  'virtue',\n",
       "  'wisdom',\n",
       "  'worldly_possession',\n",
       "  'worthiness'}}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_semrel_dict('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synonym': {'intend', 'remember'},\n",
       " 'antonym': {'forget'},\n",
       " 'hypernym': {'change',\n",
       "  'concentrate',\n",
       "  'deliberation',\n",
       "  'evaluate',\n",
       "  'expect',\n",
       "  'imagine',\n",
       "  'think'},\n",
       " 'hyponym': {'aim',\n",
       "  'associate',\n",
       "  'brainstorm',\n",
       "  'chew_over',\n",
       "  'concentrate',\n",
       "  'design',\n",
       "  'evaluate',\n",
       "  'feel',\n",
       "  'give',\n",
       "  'hold',\n",
       "  'know',\n",
       "  'philosophize',\n",
       "  'plan',\n",
       "  'puzzle_over',\n",
       "  'rationalize',\n",
       "  'reason',\n",
       "  'recognize',\n",
       "  'rethink',\n",
       "  'review',\n",
       "  'see',\n",
       "  'study',\n",
       "  'suspect',\n",
       "  'think',\n",
       "  'think_about',\n",
       "  'think_of'}}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_semrel_dict('think')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing / things to deal with\n",
    "\n",
    "- when WordNet and `model.most_similar()` return the same word -- remove duplicate or somehow weight it double?\n",
    "- will need to remove instances of the MW and words that are also morphologically related to it -- probably do this in one sweep at the end"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
