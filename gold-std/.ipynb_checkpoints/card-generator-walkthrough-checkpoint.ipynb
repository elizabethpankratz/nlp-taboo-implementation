{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Anna Goecke, Rodrigo Lopez Portillo Alcocer, Elizabeth Pankratz*\n",
    "\n",
    "In this notebook, we walk the reader through how we developed a gold standard for our Taboo card generator based on existing Taboo cards, and how we created brand new Taboo cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Developing a gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we present the workflow for how we used our gold standard dataset to generate a probability distribution, which we then used to determine which semantic relations that are most likely to appear on a Taboo card.\n",
    "This gives us a way to generate more accurate, true-to-life versions of Taboo cards. \n",
    "\n",
    "The 240 Taboo cards that our gold standard is based on belong to Elizabeth's family's Canadian edition of Taboo, produced sometime in the 1990s or early 2000s.\n",
    "These cards were transcribed into the text file `taboo_cards.txt` (in the current directory).\n",
    "The module `gs_probdist` contains the functions needed for this section.\n",
    "(Each function is documented with a docstring detailing the arguments and returned values, if the reader wants more information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gs_probdist as gspd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, the function `get_card_dicts()` combines two other `gspd` functions, `read_in()` and `format_cards()`, to read in the transcribed Taboo cards and produce a dictionaries whose entries each represent one of the cards. \n",
    "For example, the main word (MW) \"syrup\" is assigned to the dictionary's key, and its value is a list of the five corresponding taboo words (TWs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maple', 'pancakes', 'trees', 'sap', 'sweet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_dict = gspd.get_card_dicts()\n",
    "card_dict['syrup']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_card_dicts()` provides the input for the function `cards_to_df()`, which converts this dictionary to a pandas dataframe.\n",
    "In this dataframe, each row is a pairing of the MW with each of its TWs.\n",
    "This format allows for easy annotation of the semantic relationship between each MW/TW pair.\n",
    "The following cell shows the top five rows of the resulting dataframe, corresponding to one card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw\n",
       "0  huddle    gather\n",
       "1  huddle  football\n",
       "2  huddle     group\n",
       "3  huddle      play\n",
       "4  huddle  together"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gspd.cards_to_df(card_dict).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we exported this dataframe to a CSV file and manually categorised the following types of semantic relationship between each TW and its MW:\n",
    "\n",
    "- **collocations** (i.e. combinations of words at rates more frequent than chance; see Evert 2009)\n",
    "- **synonyms** (words meaning the same thing)\n",
    "- **antonyms** (words with opposite meanings)\n",
    "- **hyponyms** (a subset of a word's meaning, i.e. a more specific version)\n",
    "- **hypernyms** (a superset of a word's meaning, i.e. a more general version)\n",
    "\n",
    "We also had categories for cultural references---MWs and TWs relating in a way that requires cultural or world knowledge---and a catch-all \"other\" category.\n",
    "We did not make an attempt to replicate these two categories, since our focus was on the linguistic aspect of this project.\n",
    "We chose these semantic relations since they are fairly easy to operationalise using two tools that we have gotten to know this semester: word2vec word embeddings (for the collocations, since word embeddings represent textual co-occurrence; **CITATION NEEDED**) and WordNet (for all the others; **CITATION NEEDED**).\n",
    "\n",
    "(The annotation guidelines can be found in `gs-annotation-guidelines.txt`.)\n",
    "\n",
    "The rest of the functions in `gspd` are used to process the annotated gold standard dataset, which is saved as `gold-std-categorised.csv` in the current directory.\n",
    "First, `read_in_categorised()` simply processes the csv file into a pandas dataframe with one row per MW/TW pairing and a 1 in the column of the category that that pair belongs to, and zeroes everywhere else, as illustrated in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "      <th>semrel_synonym</th>\n",
       "      <th>semrel_antonym</th>\n",
       "      <th>semrel_hyponym</th>\n",
       "      <th>semrel_hypernym</th>\n",
       "      <th>collocation</th>\n",
       "      <th>cultural_ref</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw  semrel_synonym  semrel_antonym  semrel_hyponym  \\\n",
       "0  huddle    gather             0.0             0.0             0.0   \n",
       "1  huddle  football             0.0             0.0             0.0   \n",
       "2  huddle     group             0.0             0.0             0.0   \n",
       "3  huddle      play             0.0             0.0             0.0   \n",
       "4  huddle  together             0.0             0.0             0.0   \n",
       "\n",
       "   semrel_hypernym  collocation  cultural_ref  other  \n",
       "0              1.0          0.0           0.0    0.0  \n",
       "1              0.0          1.0           0.0    0.0  \n",
       "2              1.0          0.0           0.0    0.0  \n",
       "3              0.0          1.0           0.0    0.0  \n",
       "4              0.0          1.0           0.0    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldstd_data = gspd.read_in_categorised()\n",
    "goldstd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is used as input for the final two functions of `gspd`: `plot_category_freqs()`, to create the bar plot used in our presentation at the beginning of February (this function also exports the plot to a PDF in the current directory), and the crucial `freq_dist_to_prob_dist()`, which converts the relative frequencies of each category into a probability distribution.\n",
    "This probability distribution was then used to randomly supply a category for each of the five TW slots, in proportion to that category's actual frequency in the real Taboo cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gspd.plot_category_freqs(goldstd_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collocation': 0.6634275618374559,\n",
       " 'semrel_synonym': 0.19699646643109542,\n",
       " 'semrel_hypernym': 0.10159010600706714,\n",
       " 'semrel_hyponym': 0.020318021201413426,\n",
       " 'semrel_antonym': 0.0176678445229682}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_prob_dist = gspd.freq_dist_to_prob_dist(goldstd_data)\n",
    "category_prob_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Determining semantic relations for the given word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we know the probability of each TW slot being assigned one of the five categories.\n",
    "However, we need to find the words, for any MW given as input, that belong to these categories, so that in the final step, this can all be put together.\n",
    "This is where the functions defined in the module `semrel` come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semrel as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major pillar of this module is `make_semrel_dict()`, which takes in the given MW and uses the NLTK interface with WordNet to populate a dictionary with that word's synonyms, antonyms, hypernyms, and hyponyms.\n",
    "It uses other functions defined in this module: `get_synonyms()`,  `get_antonyms()`, `get_hypernyms()`, and `get_hyponyms()`.\n",
    "Two other functions were created for this module, as well: `wd_to_synsets()` (used in `get_synonyms()`), and `synset_to_wd()` (used in `get_hypernyms()` and `get_hyponyms()`).\n",
    "\n",
    "Two examples of the results of  `make_semrel_dict()` are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semrel_synonym': set(),\n",
       " 'semrel_antonym': set(),\n",
       " 'semrel_hypernym': {'bovid', 'follower', 'simpleton'},\n",
       " 'semrel_hyponym': {'ewe', 'ram', 'wether'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.make_semrel_dict('sheep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semrel_synonym': {'adept',\n",
       "  'beneficial',\n",
       "  'commodity',\n",
       "  'dear',\n",
       "  'dependable',\n",
       "  'effective',\n",
       "  'estimable',\n",
       "  'full',\n",
       "  'thoroughly',\n",
       "  'well'},\n",
       " 'semrel_antonym': {'bad', 'evil', 'ill'},\n",
       " 'semrel_hypernym': {'advantage', 'artifact', 'morality', 'quality'},\n",
       " 'semrel_hyponym': {'basic',\n",
       "  'beneficence',\n",
       "  'benefit',\n",
       "  'benignity',\n",
       "  'better',\n",
       "  'desirability',\n",
       "  'entrant',\n",
       "  'export',\n",
       "  'fungible',\n",
       "  'future',\n",
       "  'import',\n",
       "  'kindness',\n",
       "  'merchandise',\n",
       "  'middling',\n",
       "  'optimum',\n",
       "  'saintliness',\n",
       "  'salvage',\n",
       "  'shopping',\n",
       "  'summum_bonum',\n",
       "  'virtue',\n",
       "  'wisdom',\n",
       "  'worldly_possession',\n",
       "  'worthiness'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.make_semrel_dict('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two remarks about the above:\n",
    "\n",
    "- Sometimes WordNet does not contain any words that stand in the desired semantic relation to the MW (e.g. there are no synonyms for \"sheep\"). \n",
    "However, it is possible that this semantic relation will be assigned to one of the five TW slots on a card for \"sheep\", but this assignment is unfulfillable, since no synonyms are available.\n",
    "We will come back to how we deal with these cases in a little while.\n",
    "- The word \"good\" can be an adjective or a noun (singular of \"goods\"), and `make_semrel_dict()` was designed to capture all parts of speech and all senses of the input word, which is why we have synonyms like \"commodity\" and also \"beneficial\".\n",
    "We made this decision because, in Taboo, you are not told what part of speech your MW belongs to; you can use any of its senses to try to make your team members guess it (and often this is a very good strategy for English Taboo, where many of the words are homonymous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other pillar of this module is `get_collocations()`.\n",
    "This is a recursive function that uses the word2vec word embeddings to return some number of suitable collocates, which we operationalise as words that word2vec considers to be similar to the main word, due to their frequent use in the same contexts.\n",
    "\n",
    "\"Suitable\" is an important word here, since some post-processing has to be done to the words that are returned by gensim's `most_similar()` function.\n",
    "Words that we do not want on the cards, but that `most_similar()` might unearth, are those that:\n",
    "- contain the string of the main word (e.g. inflectional variations of the MW)\n",
    "- have a Levenshtein distance of 3 or fewer to the main word (meaning that they are probably typos)\n",
    "- are already contained in the semantic relations dictionary\n",
    "\n",
    "All of these cases are taken care of by `get_collocations()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anthurium', 'blossom', 'chrysanthemum', 'orchid', 'peony', 'tulip'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No \"forbidden words\" (the set in the second argument is empty)\n",
    "sr.get_collocations('flower', set(), model, num_collocates = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blossom', 'chrysanthemum', 'peony', 'tulip'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forbidding \"anthurium\" and \"orchid\" (e.g. if they were already in the semantic relations dictionary)\n",
    "sr.get_collocations('flower', {'orchid', 'anthurium'}, model, num_collocates = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind returning (at minimum) some set number of suitable collocates is the following: generating semantic categories to fulfill based on the probability distribution from above means that each MW will need between 0 and 5 collocates.\n",
    "The cardinality threshold is there to make sure the function does as little work as possible.\n",
    "\n",
    "The work accumulates because of the recursion, which is an important part of the function's operation.\n",
    "For example, imagine that we need 5 collocations, but `most_similar()` returns only 3 suitable ones.\n",
    "Then, we need to extend the range of most similar words that `most_similar()` returns, so that we have more words to choose from.\n",
    "So, the function checks whether the number of suitable collocates provided in the first iteration is more than the passed-in cardinality threshold, and if not, it increases the number of most similar words to return and checks again how many of those are suitable.\n",
    "\n",
    "At this point, we have ways to make a dictionary that contains all of the synonyms, antonyms, hypernyms, and hyponyms from WordNet, as well as a set of collocated words based on the word2vec embeddings.\n",
    "We also have a probability distribution that tells us how probable it is to find any one of these semantic relations in one TW slot in the final card.\n",
    "Now all that remains is to put this information together and generate the Taboo cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Putting it all together: Card generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module containing the functions for the culmination of this part of our project is `cardgen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cardgen as cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its workhorse is `card_generator()`, which takes as arguments the desired MW, the dictionary containing the probability distribution from the gold standard, and the gensim model.\n",
    "It uses the function `select_five_categories()` to generate a list of five semantic relations, one for each slot on the card.\n",
    "Then, `get_good_label_distrib()` assesses the cardinality of the sets in the semantic relation dictionaries to see if the MW has enough synonyms, say, to fulfill the number of synonyms that `select_five_categories()` has generated.\n",
    "If not, this label is replaced by \"collocation\", since this is the most frequent category. \n",
    "(It would have been possible to be more sophisticated and re-generate a category according to the probability distribution, but this was much more straightforward, and the word2vec words contain more than just strict collocations, anyway.)\n",
    "\n",
    "After this, we have the final list of categories to appear on the current card that are compatible with the cardinalities of the sets in the semantic relations dictionary.\n",
    "Now we randomly select words with the desired semantic relation from the dictionary and fill in the rest with new collocates (which is why the forbidden-words and number-of-collocates parameters in `sr.get_collocations()` were important).\n",
    "The card is returned as a single-entry dictionary, with the MW as a key and the generated list of TWs as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ghost': ['writer', 'suggestion', 'touch', 'spooky', 'poltergeist']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg.card_generator('ghost', category_prob_dist, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a function to pretty-print these cards in a more Taboo-like format (though how \"pretty\" this is is a matter of contention...).\n",
    "Together, `card_generator()` and `pretty_print()` are wrapped up in `draw_card()`, which simulates drawing a Taboo card from the playing deck.\n",
    "\n",
    "One of the fun parts of this card generator is that, because of the randomness in the probability distribution and the fact that there are more words to choose from than will fit on a card, the cards generated for a given MW are different almost every time.\n",
    "Let's see an example with the word \"ghost\" again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------------------\n",
      " |    ghost          |\n",
      " ---------------------\n",
      " |    apparition     |\n",
      " |    touch          |\n",
      " |    haunted        |\n",
      " |    hauntings      |\n",
      " |    poltergeist    |\n",
      " ---------------------\n"
     ]
    }
   ],
   "source": [
    "cg.draw_card('ghost', category_prob_dist, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Evert 2009\n",
    "- the wordnet one\n",
    "- the word2vec one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
