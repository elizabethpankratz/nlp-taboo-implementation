{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Demo version of the descriptive sentence generator\n",
    "\n",
    "In this notebook, we provide code for using the final version of the description generator we implemented. \n",
    "This notebook is structured in such a way that the reader will be able to get a feeling of what this second part of the project works, but also to show its limitations. \n",
    "For a detailed explanation on what strategy we followed and how each individual function works, please see `text-generation/Walkthrough.ipynb`.\n",
    "\n",
    "If all necessary packages are installed, running the first cell will take care of setting up the generator. Note that it might take up to a couple of minutes to run. Once this step is finished, please select which model you would like to use and run the environment loader. Loading model 3 can also take some time, depending on the hardware running the notebook. Once these two steps are done you are good to go! Examples can be found in the last section of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg\n",
    "\n",
    "card_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#opening and reading the corpus\n",
    "#we will be using the full version of the descriptive corpus we made ~115k sentences\n",
    "f = open('description-corpus-115k.txt', 'r', encoding='utf-8')\n",
    "text = f.readlines() # List with sentences as elements\n",
    "f.close()\n",
    "\n",
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]\n",
    "\n",
    "#getting the avg length of a sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))\n",
    "\n",
    "\n",
    "def next_token_generator(seed, generation_length=100):\n",
    "    \"\"\"\n",
    "    Given a seed and a length, it returns a sequence of generated tokens. \n",
    "    It generates the tokens one by one, concatenating the new tokens to the seed and taking the seed's last two tokens as context for the next generation.\n",
    "\n",
    "    Arg:\n",
    "        seed: a string of minimal length = 2 which will serve as context for the first generation step.\n",
    "        generation_length: integer value representing the number of tokens we desire to generate.\n",
    "    Returns:\n",
    "        A string consisting of the concatenation of the seed and all generated tokens.\n",
    "    \"\"\"\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(generation_length):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in seed.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).exp()\n",
    "        top_choice = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_choice)]\n",
    "        seed += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return seed\n",
    "\n",
    "def gen_input_words(mw, model):\n",
    "    \"\"\"\n",
    "    Given a main word and a gensim model, it generates a set of input words that we are aiming to have in our descriptive sentence.\n",
    "    Arg:\n",
    "        mw: a string containing the main word that we are aiming to describe\n",
    "        model: gensim model from which we are retrieving words semanticaly related to the mw \n",
    "    Returns:\n",
    "        A set of strings consisting of the mw, its synonyms, hypernyms and hyponyms, and all its associated taboo words. All of them contained in the vocabulary.\n",
    "    \"\"\"\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "\n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but the loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab. Shouldn't be a thing when using larger corpus\n",
    "    input_words = [word for word in input_words if word in voc]\n",
    "    return input_words\n",
    "\n",
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    \"\"\"\n",
    "    Given the main word we are trying to describe, it will iteratively create sequences aiming to contain some of the input words generated by gen_input_words(mw, model).\n",
    "    If the desired score was not reached it will return the last generated sequence, after doing n_iterations of the process.\n",
    "    Arg:\n",
    "        mw = main word as string\n",
    "        model = embeddings used to generate the cards\n",
    "        n_seeds = int, if we are using 2 or 3 seeds during the sentence generation step\n",
    "        n_iterations = int, how many sequences we will test\n",
    "        debugging = Boolean, True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "        printing = Boolean, True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    Returns:\n",
    "        A string containing the last generated sequence.\n",
    "    \"\"\"\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    #n_seeds = if we are using 2 or 3 seeds during the sentence generation step\n",
    "    #n_iterations = how many iterations we will do in the generation step\n",
    "    #debugging = True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "    #printing = True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    \n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence, dividing the whole sequence into equally long sub_sequences\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 7), next_token_generator(mw+' is', 7), next_token_generator(mw+' can be found', 5)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that already contains an input word.\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence to know where input words are located and to which sub_sequence they belong.\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        #for practical purposes we stop generating after some fixed number of iterations in case the score was not reached.\n",
    "        while i < n_iterations and score <2 :\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change in the score. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index]\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                    \n",
    "                #keeping the segment in which the improvement took place, blocking it and continue the generating process\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 7) \n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) \n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    else:\n",
    "                        temp =  next_token_generator(sentence_parts[1]+' '+ mw+' can be found', 5) \n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            #choosing what to print\n",
    "            if i == 0:\n",
    "                print('The set of input words we are trying to introduce into our sequence is: '+str(input_words))\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 11), next_token_generator(mw+' is', 12)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 2\n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations and score <2:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 2\n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 12)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11) \n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 12)\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "                \n",
    "            if i == 0:\n",
    "                print('The set of input words we are trying to introduce into our sequence is: '+str(input_words))\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def sentence_cleaner(sentence, mw, model):\n",
    "    '''\n",
    "    Makes sure that the generated sequence given by description_generator() follows taboo rules and does not contain the main word or any taboo word.\n",
    "    Arg: \n",
    "        sentence: string containing the sequence to clean\n",
    "        mw: string containing the main word we are playing with \n",
    "        model: gensim embeddings model we are retrieving semantic relations from\n",
    "    Returns:\n",
    "        A string containing a version of the descriptive sentence that follows taboo rules.\n",
    "    '''\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, '.The main word')\n",
    "    \n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            #if we have at least one\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                #choose one randomly\n",
    "                choice = np.random.choice(syns)\n",
    "                #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                while (choice in taboo_words or choice != mw) and len(syns) > 1:\n",
    "                    syns = syns.pop(syns.index(choice))\n",
    "                    choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "                #if all synonyms where taboo words or the mw\n",
    "                if choice in taboo_words or choice == mw:\n",
    "                    hypers = sr.get_hypernyms(tw)\n",
    "                    #if we have at least one\n",
    "                    if len(hypers) > 0:\n",
    "                        hypers = list(hypers)\n",
    "                        #choose one randomly\n",
    "                        choice = np.random.choice(hypers)\n",
    "                        #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                        while (choice in taboo_words or choice != mw) and len(hypers) > 1:\n",
    "                            syns = syns.pop(syns.index(choice))\n",
    "                            choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'Is a type of '+choice)\n",
    "            #in case no synonyms were found\n",
    "            else:\n",
    "                hypers = sr.get_hypernyms(tw)\n",
    "                #if we have at least one\n",
    "                if len(hypers) > 0:\n",
    "                    hypers = list(hypers)\n",
    "                    #choose one randomly\n",
    "                    choice = np.random.choice(hypers)\n",
    "                    #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                    while (choice in taboo_words or choice != mw) and len(hypers) > 1:\n",
    "                        syns = syns.pop(syns.index(choice))\n",
    "                        choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'Is a type of '+choice)\n",
    "                else: \n",
    "                    sentence = sentence.replace(tw, \"NO IDEA!\")\n",
    "    sentence = sentence[1:]\n",
    "    return sentence\n",
    "\n",
    "def final_output(mw, card_model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    sentence = description_generator(mw, card_model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, card_model)\n",
    "    return output\n",
    "\n",
    "def load_model(x):\n",
    "    if x ==1:\n",
    "        with open(\"trigrams_model1.txt\", \"rb\") as fp:\n",
    "            trigrams = pickle.load(fp)\n",
    "            \n",
    "        voc = set()\n",
    "        for tri in trigrams:\n",
    "            voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "        voc_length = len(voc) \n",
    "        word_to_freq = {word: i for i, word in enumerate(voc)}\n",
    "            \n",
    "        cont = []\n",
    "        tar = []\n",
    "        for context, target in trigrams:\n",
    "            context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "            cont.append(context_freqs)\n",
    "            target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "            tar.append(target_freq)\n",
    "        path = os.getcwd()+'/model1_trained.pt'\n",
    "        hidden_s = 150\n",
    "        n_layers = 1\n",
    "        lr = 0.015\n",
    "        decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        decoder = torch.load(path)\n",
    "        decoder.eval()\n",
    "    elif x ==3:\n",
    "        with open(\"trigrams_model3.txt\", \"rb\") as fp:\n",
    "            trigrams = pickle.load(fp)\n",
    "            \n",
    "        voc = set()\n",
    "        for tri in trigrams:\n",
    "            voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "        voc_length = len(voc) \n",
    "        word_to_freq = {word: i for i, word in enumerate(voc)}\n",
    "            \n",
    "        cont = []\n",
    "        tar = []\n",
    "        for context, target in trigrams:\n",
    "            context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "            cont.append(context_freqs)\n",
    "            target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "            tar.append(target_freq)\n",
    "            \n",
    "        path = os.getcwd()+'/model3_trained.pt'\n",
    "        \n",
    "        hidden_s = 50\n",
    "        n_layers = 3\n",
    "        lr = 0.015\n",
    "        decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        decoder.load_state_dict(torch.load(path))\n",
    "        decoder.eval()\n",
    "    elif x == 2:\n",
    "        print('It is not safe to use this model. Please choose either model 1 or 3.')\n",
    "    else:\n",
    "        print('Please enter 1 or 3 to choose the model to be used.')\n",
    "        \n",
    "    return voc, voc_length, word_to_freq, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading trained model\n",
    "Choosing which trained model to load. They are all GPU-based RNN models, trained on a CPU with 100 epochs.\n",
    "* 1:\n",
    "    * Model with 1 hidden layer consisting of 150 nodes. Trained with a sample of 50,000 non-filtered trigrams containing ~16k tokens from our corpus' vocabulary (which has a total of ~80k tokens, from which more than half only appeared once). Set of trigrams properly stored. Trained in about 12 hours.\n",
    "* 2:  \n",
    "    * Model with 2 hidden layers consisting of 75 nodes each. Also trained with a sample of 50,000 non-filtered trigrams containing ~16k tokens from our corpus' vocabulary. Unfortunately, we forgot to include a random seed for the sampling process and we did not save the corresponding set of trigrams. Although the generation step might work, it is not advised to use this model. Trained in about 5 hours.\n",
    "* 3:\n",
    "    * Model with 3 hidden layers consisting of 50 nodes each. Trained with a sample of ~86k filtered trigrams containing ~19k tokens from our corpus' vocabulary. (\"Filtered\" means that the model was only trained on trigrams containing tokens that appear at least twice in our corpus.) Although a random seed (163) was now included, for efficiency reasons we also decided to save the trigrams in order to load them faster and make reproducibility easier. Trained in about 11 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#To load model 1:\n",
    "voc, voc_length, word_to_freq, decoder = load_model(1)\n",
    "\n",
    "#To load model 3:\n",
    "#voc, voc_length, word_to_freq, decoder = load_model(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Some examples\n",
    "Below we encourage you to play around with some examples to get a feeling of how to use the generator and then move on into your own examples.\n",
    "Note that some of the printed information might seem opaque, but detailed explanations can be found in `text-generation/Walkthrough.ipynb`.\n",
    "\n",
    "You will notice that the sentences are, unfortunately, not syntactically coherent. \n",
    "We believe this is due to our limited computational power in training the networks, which restricted how many hidden layers and what size of vocabulary our neural networks could have.\n",
    "However, even if the result was not what we had hoped for, experimenting with different model parameters within our computational limits still gave us a chance to explore how neural networks function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Example with 'school' as main word, 3 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_output(mw = 'school', card_model = card_model, n_seeds=3, n_iterations = 5, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Example with 'cake' as main word, 2 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_output(mw = 'cake', card_model = card_model, n_seeds=2, n_iterations = 20, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Example with 'airplane' as main word, 3 seeds, simple printing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_output(mw = 'airplane', card_model = card_model, n_seeds=3, n_iterations = 150, debugging = False, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Example with 'airplane' as main word, 2 seeds, only final output is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_output(mw = 'airplane', card_model = card_model, n_seeds=2, n_iterations = 10, debugging = False, printing = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
