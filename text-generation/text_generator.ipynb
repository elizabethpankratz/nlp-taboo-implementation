{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Remarques\n",
    "* Three steps for cleaning probably provide better word2word meanings but since for us the main goal is to generate a coherent sentence from the input and meaning WE are feeding, then we can't leave out punctuation symbols and stop words. Maybe it is still good to lemmatize? We'll see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Generating our descriptive sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Importing everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#is this the best choice of autograd?\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Getting out text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "#we will be using the small version of the descriptive corpus we made\n",
    "f = open('description-corpus-20k.txt', 'r')\n",
    "#text = f.read()\n",
    "text = f.readlines() #if we want a list with sentences as elements\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#getting the avg length of a descriptive sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Cleaning (NOT USED)\n",
    "* Removing stop words, punctuation symbols and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# found that in some tutorials they do 3 extra cleaning steps before applying N-grams\n",
    "\n",
    "# getting rid of stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_free = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "# justification --> it's like getting rid of the long tail in the word frequency plot. Oder?\n",
    "\n",
    "# getting rid of punctuation\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "#punct_free = \"\".join(word for word in stop_free if word not in punctuation_symbols)\n",
    "# makes sense.. I think\n",
    "\n",
    "# lemmatizing?\n",
    "#lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#normalized = ' '.join(lemma.lemmatize(word) for word in punct_free.split())\n",
    "#if it is what I think it is, then it makes sense too\n",
    "\n",
    "#last step, lower case and splitting\n",
    "#cleaned_text = normalized.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A cookie is a text file that is placed on your hard disk by a Web page server .\\nA listing is a product placed in the directory for sale .\\nAfter all , a home is the largest ( and most emotional ) investment most people will ever make .\\nWhile emotions '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A cookie text file placed hard disk Web page server . A listing product placed directory sale . After , home largest ( emotional ) investment people ever make . While emotions probably high gear 've found home love , 's important remember home invest\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A cookie text file placed hard disk Web page server  A listing product placed directory sale  After  home largest  emotional  investment people ever make  While emotions probably high gear ve found home love  s important remember home investment  Wit'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A cookie text file placed hard disk Web page server A listing product placed directory sale After home largest emotional investment people ever make While emotion probably high gear ve found home love s important remember home investment With many qu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Only normalizing\n",
    "* is it worth it having different tokens for cookie and cookies (in terms of having a correct sentence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Implementing trigrams and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating trigram sets EASY WAY\n",
    "#trigrams = [([cleaned_text[i], cleaned_text[i+1]], cleaned_text[i+2])for i in range(len(cleaned_text) - 2)]\n",
    "\n",
    "# this structure allows us to create context/target sets for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# better way: sentence by sentence, not the whole text in one go\n",
    "trigrams = []\n",
    "for sentence in sentences:\n",
    "    trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'cookie'], 'text')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[0]\n",
    "#context for target word 'text' --> 'a' and 'cookie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#to prototype we will only take 1000 trigrams out of the ~500000\n",
    "trigrams=trigrams[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting set of words in vocab, it's length and the frequency of each word\n",
    "voc = set()\n",
    "for sentence in sentences:\n",
    "    voc = voc.union(set(sentence))\n",
    "voc_length = len(voc) #34174\n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to inp\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bad, training on CPU; dont exagerate with number of epochs.\n"
     ]
    }
   ],
   "source": [
    "#Cheking if we have access to training on GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Fancy setup!')\n",
    "else: \n",
    "    print('Too bad, training on CPU; dont exagerate with number of epochs.')\n",
    "\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train(context, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(trigrams)):\n",
    "        output, hidden = decoder(context[t], hidden)\n",
    "        loss += criterion(output, target[t])\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data.item() / len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 48s (10 10%) 4.8131]\n",
      "[1m 36s (20 20%) 3.3923]\n",
      "[2m 18s (30 30%) 2.0022]\n",
      "[2m 59s (40 40%) 1.0425]\n",
      "[3m 43s (50 50%) 0.5735]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "print_every = 10\n",
    "plot_every = 10\n",
    "hidden_s = 50\n",
    "n_layers = 1\n",
    "lr = 0.015\n",
    "\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(cont,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a394f2690>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in prime_str.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main word is a powerful engine\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('the main word is', 10, temperature = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Generating descriptive sentence\n",
    "* input = main word + taboo words\n",
    "\n",
    "The main idea here is that we will seed always with the main word and replace at the end.\n",
    "The descriptive sentence will be accepted into the cleaning step (where we will make sure that no tws or the mw were used and if so, we'll replace them) if it contains at least two of the input words, besides the seeds. As long as the sentence is not accepted, we will keep generating.\n",
    "\n",
    "To help reach a high score: once an input word has been used and its score_vector value increases, we will leave that segment in our sentence. Not sure if this hurts more than it helps, but adding the input word as a seed is VERY complicated. And not sure it would make sense either.\n",
    "\n",
    "For a first prototype we will assume that maximum one new word is gonna be added per iteration. \n",
    "\n",
    "Pretty fragile with many assumptions?\n",
    "\n",
    "It would be useful to generate a list of synonyms of all input words to expand the input_words set and have higher acceptance rate. We thought about cleaning the input words set from the start but those are words with high probability of occurence so we better leave them and clean in the end.\n",
    "\n",
    "* maybe more seeds?\n",
    "* maybe require a higher score?\n",
    "* how to properly connect end of sentences and seeds? Is it better to have fewer seeds but longer auto-generated text?\n",
    "* add time it to report results and \"quantify\" results/improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3 seeds (means, is, refers to). i=50\n",
    "Final score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# set of words that we hope will appear in the description\n",
    "input_words = np.array(['happy', 'pleased', 'thrilled', 'excited', 'ecstatic', 'overjoyed', 'joy'])\n",
    "#extend!\n",
    "#probably good idea to check if those words are even in our vocab. jeje\n",
    "# filtering out the ones that are not. Shouldn't be a thing when using larger corpus\n",
    "input_words = [word for word in input_words if word in voc]\n",
    "\n",
    "#create the first sentence\n",
    "#on average a descriptive sentence had 27 words/symbols.\n",
    "# we will equally divide them between our seeds\n",
    "\n",
    "sentence = evaluate('happy means', 9, temperature = 1) +' ' + evaluate('happy is', 9, temperature = 1) +' '+ evaluate('happy refers to', 9, temperature = 1)\n",
    "\n",
    "eval_sentence = np.array(sentence.split())\n",
    "# first score vector and score\n",
    "score_vector = np.array([len(np.where(eval_sentence == input_words[i])[0]) for i in range(len(input_words))])\n",
    "score = np.sum(score_vector) - 3 #to account for seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# iterate until nice sentence comes up\n",
    "# we will add safety measure to not break everything\n",
    "i = 0\n",
    "n = 10\n",
    "index_in_sentence = -1\n",
    "# to keep track of scores\n",
    "scores = np.zeros(n)\n",
    "# the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "covered = [0,0,0]\n",
    "changes = np.zeros(len(score_vector))\n",
    "\n",
    "#known positions of input words in our sentence\n",
    "positions = np.zeros(len(eval_sentence))\n",
    "#we know the positions of the seeds\n",
    "positions[0] = 1\n",
    "positions[9] = 1\n",
    "positions[18] = 1\n",
    "\n",
    "while i < n:\n",
    "    #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "    \n",
    "    #checking if score improved\n",
    "    new_score_vector = np.array([len(np.where(eval_sentence == input_words[i])[0]) for i in range(len(input_words))])\n",
    "    changes = new_score_vector - score_vector\n",
    "    if changes != np.zeros(len(score_vector)): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "        index = np.where(changes != 0)[0][0] #looking for the word that was added\n",
    "        word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "        #finding in which segment that new added word is in order to leave the segment untouched\n",
    "        \n",
    "        #how to detect the index of the word that just came up if it was already in the sentence somewhere else?\n",
    "        #this should do\n",
    "        indices_in_sentence = np.where(eval_sentence == word_that_was_added)[0]\n",
    "        if len(indices_in_sentence) >0:\n",
    "            for i in indices_in_sentence:\n",
    "                if positions[i] != 1:\n",
    "                    index_in_sentence = i\n",
    "                    positions[i] = 1\n",
    "\n",
    "        #keeping the segment in which the improvement took place\n",
    "        if index_in_sentence in range(9) && covered[0]!=1:\n",
    "            sentence = ' '.join(eval_sentence[:9]) +' '+ evaluate('happy is', 7, temperature = 1) +' '+ evaluate('happy refers to', 6, temperature = 1)\n",
    "            covered[0] = 1\n",
    "        elif index_in_sentence in range(9, 18) && covered[1] !=1:\n",
    "            sentence = evaluate('happy means', 7, temperature = 1) +' ' + ' '.join(eval_sentence[9:18]) +' '+ evaluate('happy refers to', 6, temperature = 1)\n",
    "            covered[1] = 1\n",
    "        elif index_in_sentence in range(18, 27) && covered[2] != 1:\n",
    "            sentence = evaluate('happy means', 7, temperature = 1) +' ' + evaluate('happy is', 7, temperature = 1) +' ' + ' '.join(eval_sentence[18:27])\n",
    "            covered[2] = 1\n",
    "        changes = np.zeros(len(score_vector))\n",
    "        index_in_sentence = 0\n",
    "        score_vector = new_score_vector\n",
    "        score = np.sum(score_vector) - 3 #to account for seeds\n",
    "    \n",
    "    #if there was no change\n",
    "    else:\n",
    "        if covered == [0,0,0]: #score is still at 0\n",
    "            sentence = evaluate('happy means', 7, temperature = 1) +' ' + evaluate('happy is', 7, temperature = 1) +' '+ evaluate('happy refers to', 6, temperature = 1)\n",
    "            eval_sentence = np.array(sentence.split())\n",
    "            score_vector = new_score_vector\n",
    "            score = np.sum(score_vector) - 3 #to account for seeds\n",
    "        #based on what is already covered\n",
    "        #MAKE FUNCTION TO FACILITATE THIS!!\n",
    "        \n",
    "        elif covered == [1,0,0]:\n",
    "            \n",
    "    \n",
    "    scores[i] = score\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
