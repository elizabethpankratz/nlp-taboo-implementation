{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Descriptive sentence generator walkthrough\n",
    "\n",
    "In this notebook, we will walk the reader through how we developed the second part of our Taboo implementation project: the Taboo player. \n",
    "The ultimate goal of the player is to receive as input one Taboo-style card, containing a main word and its taboo words (generated by the card generator in the first part of this project), and to output a sentence describing the main word without using the taboo words.\n",
    "\n",
    "Here, we present the steps we took to approach this challenge. \n",
    "\n",
    "Simply put, our strategy is the following:\n",
    "\n",
    "* Train the GRU-based neural network with trigrams from our bespoke corpus of descriptive sentences. The use of trigrams was inspired by several implementations we found online attempting to complete a similar text generation task, as well as the PyTorch tutorial in class. The relevant papers and blog posts are referenced in the project reports and below.\n",
    "* Use the main word as grounding point or seed for the final sentence. Using multiple seeds should allow us to generate sentences related to the main word, without deviating too much as the distance from the seed increases. Our whole sentence will be the concatenation of as many smaller generated sequences as we have seeds.\n",
    "* Start an iterative process in which each segment of the sentence keeps being generated until we detect some words in it that are either the taboo words or words that are semantically related to the main word, indicating that that chunk might be semantically related to the main word.\n",
    "* Clean the final sentence to remove the main word and any taboo words.\n",
    "\n",
    "We trained 3 models to test our strategy. They are all GPU-based RNN models, trained on a CPU with 100 epochs.\n",
    "* 1:\n",
    "    * Model with 1 hidden layer consisting of 150 nodes. Trained with a sample of 50,000 non-filtered trigrams containing ~16k tokens from our corpus' vocabulary (which has a total of ~80k tokens, from which more than half only appeared once). Set of trigrams properly stored. Trained in about 12 hours.\n",
    "* 2: Â \n",
    "    * Model with 2 hidden layers consisting of 75 nodes each. Also trained with a sample of 50,000 non-filtered trigrams containing ~16k tokens from our corpus' vocabulary. Unfortunately, we forgot to include a random seed for the sampling process and we did not save the corresponding set of trigrams. Although the generation step might work, it is not advised to use this model. Trained in about 5 hours.\n",
    "* 3:\n",
    "    * Model with 3 hidden layers consisting of 50 nodes each. Trained with a sample of ~86k filtered trigrams containing ~19k tokens from our corpus' vocabulary. (\"Filtered\" means that the model was only trained on trigrams containing tokens that appear at least twice in our corpus.) Although a random seed (163) was now included, for efficiency reasons we also decided to save the trigrams in order to load them faster and make reproducibility easier. Trained in about 11 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 0: Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Setting up the gensim model used for card generation and semantic relations mining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 1: Creating and loading our bespoke corpus\n",
    "\n",
    "Since we wanted to use this project to gain experience implementing neural network learning methods, and more specifically as a first approach to text generation, it was essential to get our hands on a suitable corpus that would result in meaningful inference after using it for training. \n",
    "\n",
    "For this purpose, we built a corpus based on the web corpus [ENCOW16A](https://corporafromtheweb.org/encow16/) consisting of ~115k sentences with the desired descriptive structures. For more detail on how the corpus was created, see `text-generation/description-corpus/create_corpus_from_csvs.ipynb`.\n",
    "\n",
    "(The smaller version of the corpus (~20k sentences) was used for prototyping and development, but any final training and text generation was implemented with the complete version.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "f = open('description-corpus-115k.txt', 'r', encoding='utf-8')\n",
    "text = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# getting lower case and splitting each sentence\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]\n",
    "\n",
    "#getting the average length of a descriptive sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We will aim to generate sentences of the same length as the average sentence length in our corpus, namely 27 words or symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 1b: Cleaning the corpus\n",
    "\n",
    "Before dividing our sentences into trigrams, we experimented with the basic cleaning and normalizing techniques\n",
    "* removing stop words,\n",
    "* removing punctuation symbols, and\n",
    "* lemmatizing.\n",
    "\n",
    "Below, we present the code for each one of these steps. \n",
    "We ultimately decided against implementing any of them in the final version of our code, since, for the strategy we have in mind, the goal of the GRU-based text generator is to produce grammatically correct sentences, for which we need stop words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting rid of stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_free = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# getting rid of punctuation\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "#punct_free = \"\".join(word for word in stop_free if word not in punctuation_symbols)\n",
    "\n",
    "# lemmatizing\n",
    "#lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#normalized = ' '.join(lemma.lemmatize(word) for word in punct_free.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Modifying the corpus to account for Zipf's law\n",
    "Simply put, Zipf's law is an empirical law according to which the frequency use of different words follows a \n",
    "distribution that can be approximated by $$P_n \\propto \\frac{1}{n^a} $$ where $P_n$ represents the frequency of the n-th most frequent word and $a$ is a positive exponent usually slightly higher than 1. This means that the second most frequent word will be found roughly half as often as the most frequent one, the third most frequent word roughly one third as often as the most frequent one, and so on.\n",
    "\n",
    "While examining our corpus, we also found some weirdly tokenized words.\n",
    "For example, \"binoculars\" was imported as \"bi no cu lars\". \n",
    "\n",
    "Below we show a small analysis that we did on our corpus trying to understand the effect of these tokenizing mistakes. For practical purposes the goal is to get rid of those meaningless extra tokens so they are not considered during the generation process.\n",
    "\n",
    "That is why, from the third model on, we decided to work with a version of the corpus in which any token appearing only once, i.e. a hapax legomenon, was removed.\n",
    "The initial idea was to replace all of these by the \"UNKNOWN\" token. \n",
    "However, since searching and replacing items in such a big string was very inefficient (and \"UNKNOWN\" would have been a poor choice of flag token), we ended up adding the filtering into the trigram creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# reading in the corpus as one big text this time\n",
    "f = open('description-corpus-115k.txt', 'r', encoding='utf-8')\n",
    "whole_text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying now nltk's `FreqDist()` function to create a dictionary with token:frequency pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88331"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freq = nltk.FreqDist(whole_text.lower().split())\n",
    "len(token_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Before applying the cleaning step, our total vocabulary consists of ~88,000 tokens.\n",
    "\n",
    "Now that we have this dictionary we can get an idea of the frequency distribution of its values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAZBElEQVR4nO3dfbRddX3n8feHBAQsCJGHoYQYKBkrdSpCRFrbGRVFoNOGttriaoeUoaXjw7ROu6ZF2xnsgx3bWnVYdVBUFGirIlJlWihN8WnaykNQJChQUqASoASNAoKCCd/5Y/8uOSYnN4dkn3ty732/1jrr7v3dv73372w2fNgPZ+9UFZIk9Wm3SXdAkjT3GC6SpN4ZLpKk3hkukqTeGS6SpN4tnHQHdhUHHHBALV26dNLdkKRZ5YYbbvhqVR24Zd1waZYuXcrq1asn3Q1JmlWS/MuwuqfFJEm9M1wkSb0zXCRJvTNcJEm9M1wkSb0zXCRJvTNcJEm9M1x20t0bHuUz//TApLshSbsUf0S5k17ytk+z8Ynirrf+2KS7Ikm7DI9cdtLGJ3zZmiRtyXCRJPXOcJEk9c5wkST1znCRJPXOcJEk9c5wkST1znCRJPXOcJEk9c5wkST1znCRJPXOcJEk9W5s4ZLksCSfSnJLki8l+dVWX5RkVZLb29/9Wz1Jzk2yNslNSY4ZWNbK1v72JCsH6scmWdPmOTdJpluHJGlmjPPIZSPw61X1HOB44HVJjgLOBq6uqmXA1W0c4GRgWfucBZwHXVAA5wAvBI4DzhkIi/Na26n5Tmr1ba1DkjQDxhYuVXVfVX2+DT8M3AIcCqwALmzNLgRObcMrgIuqcw2wX5JDgFcAq6pqQ1V9HVgFnNSm7VtVn6uqAi7aYlnD1iFJmgEzcs0lyVLg+cC1wMFVdR90AQQc1JodCtw9MNu6Vpuuvm5InWnWsWW/zkqyOsnqBx7whV+S1Jexh0uS7wE+Bryhqh6arumQWu1AfWRVdX5VLa+q5QceeOBTmVWSNI2xhkuS3emC5c+r6rJWvr+d0qL9Xd/q64DDBmZfDNy7nfriIfXp1iFJmgHjvFsswPuBW6rq7QOTLgem7vhaCXxioH56u2vseODBdkrrKuDEJPu3C/knAle1aQ8nOb6t6/QtljVsHZKkGbBwjMt+EfCfgDVJbmy1NwFvBS5JcibwFeBVbdoVwCnAWuBR4AyAqtqQ5PeA61u7362qDW34NcAHgb2AK9uHadYhSZoBYwuXqvp7hl8XAThhSPsCXreNZV0AXDCkvhp47pD614atQ5I0M/yFviSpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd2MLlyQXJFmf5OaB2puT3JPkxvY5ZWDaG5OsTXJbklcM1E9qtbVJzh6oH57k2iS3J/lIkj1a/WltfG2bvnRc31GSNNw4j1w+CJw0pP6Oqjq6fa4ASHIUcBrwA22e/5NkQZIFwLuAk4GjgFe3tgB/2Ja1DPg6cGarnwl8vaqOBN7R2kmSZtDYwqWqPgtsGLH5CuDDVfVYVd0JrAWOa5+1VXVHVT0OfBhYkSTAS4FL2/wXAqcOLOvCNnwpcEJrL0maIZO45vL6JDe102b7t9qhwN0Dbda12rbqzwS+UVUbt6h/17La9Adb+60kOSvJ6iSrH3jggZ3/ZpIkYObD5Tzg+4CjgfuAP2n1YUcWtQP16Za1dbHq/KpaXlXLDzzwwOn6LUl6CmY0XKrq/qraVFVPAO+lO+0F3ZHHYQNNFwP3TlP/KrBfkoVb1L9rWW36Mxj99JwkqQczGi5JDhkY/Ulg6k6yy4HT2p1ehwPLgOuA64Fl7c6wPegu+l9eVQV8Cnhlm38l8ImBZa1sw68EPtnaS5JmyMLtN9kxST4EvBg4IMk64BzgxUmOpjtNdRfwywBV9aUklwBfBjYCr6uqTW05rweuAhYAF1TVl9oqfhP4cJLfB74AvL/V3w9cnGQt3RHLaeP6jpKk4cYWLlX16iHl9w+pTbV/C/CWIfUrgCuG1O9g82m1wfq3gVc9pc5KknrlL/QlSb0zXCRJvTNcJEm9M1wkSb0zXCRJvTNcJEm9M1wkSb0bKVySPHfcHZEkzR2jHrm8O8l1SV6bZL+x9kiSNOuNFC5V9SPAz9E9EHJ1kr9I8vKx9kySNGuNfM2lqm4HfpvumV7/ATg3ya1JfmpcnZMkzU6jXnP5wSTvAG6hewPkj1fVc9rwO8bYP0nSLDTqgyv/lO79K2+qqm9NFavq3iS/PZaeSZJmrVHD5RTgWwOPwd8N2LOqHq2qi8fWO0nSrDTqNZe/A/YaGN+71SRJ2sqo4bJnVX1zaqQN7z2eLkmSZrtRw+WRJMdMjSQ5FvjWNO0lSfPYqNdc3gB8NMm9bfwQ4GfH0yVJ0mw3UrhU1fVJvh94NhDg1qr6zlh7JkmatUY9cgF4AbC0zfP8JFTVRWPplSRpVhspXJJcDHwfcCOwqZULMFwkSVsZ9chlOXBUVdU4OyNJmhtGvVvsZuDfjLMjkqS5Y9QjlwOALye5DnhsqlhVPzGWXkmSZrVRw+XN4+yEJGluGfVW5M8keRawrKr+LsnewILxdk2SNFuN+sj9XwIuBd7TSocCHx9XpyRJs9uoF/RfB7wIeAiefHHYQePq1Gz0rcc3bb+RJM0To4bLY1X1+NRIkoV0v3NRc9fXHpl0FyRplzFquHwmyZuAvZK8HPgo8H/H1y1J0mw2aricDTwArAF+GbgC8A2UkqShRr1b7Am61xy/d7zdkSTNBaM+W+xOhlxjqaojeu+RJGnWeyrPFpuyJ/AqYFH/3ZEkzQUjXXOpqq8NfO6pqncCLx1z32aVZNI9kKRdx6inxY4ZGN2N7khmn7H0SJI06416t9ifDHz+F3As8DPTzZDkgiTrk9w8UFuUZFWS29vf/Vs9Sc5NsjbJTYNhlmRla397kpUD9WOTrGnznJt0xw7bWse4BQ9dJGnKqKfFXjLweXlV/VJV3bad2T4InLRF7Wzg6qpaBlzdxgFOBpa1z1nAedAFBXAO8ELgOOCcgbA4r7Wdmu+k7axDkjRDRj0t9mvTTa+qtw+pfTbJ0i3KK4AXt+ELgU8Dv9nqF7WXkV2TZL8kh7S2q6pqQ+vHKuCkJJ8G9q2qz7X6RcCpwJXTrEOSNEOeyt1iLwAub+M/DnwWuPspru/gqroPoKruSzL1fLJDt1jWulabrr5uSH26dWwlyVl0Rz8sWbLkKX4VSdK2PJWXhR1TVQ8DJHkz8NGq+sWe+jHsgkXtQP0pqarzgfMBli9fvlPPSvNuMUnabNQL+kuAxwfGHweW7sD67m+nu2h/17f6OuCwgXaLgXu3U188pD7dOiRJM2TUcLkYuC7Jm5OcA1wLXLQD67scmLrjayXwiYH66e2useOBB9uprauAE5Ps3y7knwhc1aY9nOT4dpfY6Vssa9g6JEkzZNRni70lyZXAj7bSGVX1henmSfIhugvrByRZR3fX11uBS5KcCXyF7pf+0D0I8xRgLfAocEZb74Ykvwdc39r97tTFfeA1dHek7UV3If/KVt/WOiRJM2TUay4AewMPVdUHkhyY5PCqunNbjavq1duYdMKQtkX3QrJhy7kAuGBIfTXw3CH1rw1bx7h5yUWSNhv1Ncfn0N3O+8ZW2h34s3F1ajbygr4kbTbqNZefBH4CeASgqu7Fx79IkrZh1HB5vJ26KoAkTx9flyRJs92o4XJJkvcA+yX5JeDv8MVhkqRtGPVusbcleTnwEPBs4H9W1aqx9kySNGttN1ySLKD7bcnLAANFkrRd2z0tVlWbgEeTPGMG+jOLebuYJE0Z9Xcu3wbWtKcSPzJVrKpfGUuvZiFvRZakzUYNl79uH0mStmvacEmypKq+UlUXzlSHJEmz3/auuXx8aiDJx8bcF0nSHLG9cBm8knDEODsy23nJRZI221641DaGJUnapu1d0H9ekofo/sd8rzZMG6+q2nesvZMkzUrThktVLZipjkiS5o5Rny2m7Yg/dJGkJxkukqTeGS498bhFkjYzXCRJvTNcJEm9M1wkSb0zXCRJvTNceuKdyJK0meHSk3i/mCQ9yXCRJPXOcJEk9c5wkST1znCRJPXOcOmJd4tJ0maGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hktPvFtMkjYzXCRJvZtIuCS5K8maJDcmWd1qi5KsSnJ7+7t/qyfJuUnWJrkpyTEDy1nZ2t+eZOVA/di2/LVtXo8rJGkGTfLI5SVVdXRVLW/jZwNXV9Uy4Oo2DnAysKx9zgLOgy6MgHOAFwLHAedMBVJrc9bAfCeN/+tIkqbsSqfFVgAXtuELgVMH6hdV5xpgvySHAK8AVlXVhqr6OrAKOKlN27eqPldVBVw0sKyx8eBIkjabVLgU8LdJbkhyVqsdXFX3AbS/B7X6ocDdA/Oua7Xp6uuG1CVJM2ThhNb7oqq6N8lBwKokt07TdtghQe1AfesFd8F2FsCSJUum7/F2eNwiSZtN5Milqu5tf9cDf0l3zeT+dkqL9nd9a74OOGxg9sXAvdupLx5SH9aP86tqeVUtP/DAA3fuO+3U3JI0t8x4uCR5epJ9poaBE4GbgcuBqTu+VgKfaMOXA6e3u8aOBx5sp82uAk5Msn+7kH8icFWb9nCS49tdYqcPLEuSNAMmcVrsYOAv2wXwhcBfVNXfJLkeuCTJmcBXgFe19lcApwBrgUeBMwCqakOS3wOub+1+t6o2tOHXAB8E9gKubB9J0gyZ8XCpqjuA5w2pfw04YUi9gNdtY1kXABcMqa8GnrvTnZUk7ZBd6VbkWc0L+pK0meHSE3/mIkmbGS49+do3H590FyRpl2G49ORV7/7cpLsgSbsMw6Un3/rOpkl3QZJ2GYaLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4aLJKl3hoskqXeGiySpd4ZLj9Y/9O1Jd0GSdgmGS4+O+4OrJ90FSdolGC6SpN4ZLpKk3hkukqTeGS6SpN4ZLpKk3hkuPXvw0e9MuguSNHGGS89+7ZIbJ90FSZo4w6Vn6x9+bNJdkKSJM1x6tuaeB/mFD1w36W5I0kQZLmPw6dseeHL4ijX3ceSbruDb39k0wR5J0swyXMZk0xMFwB9fdRsbnygu+Ic7ufOrj0y4V5I0MwyXMfnvl36RP7vmX9gt3fgf/c1tvORtn55onyRppiycdAfGJclJwP8GFgDvq6q3zuT6L/v8PVz2+XtmcpWStMuYk+GSZAHwLuDlwDrg+iSXV9WXJ9szWHr2Xw+t/8IPL+XIg76HPXdfwAnffxBrH/gme++xgGcfvA8bnyiqYPcFYeGC3Z485dYOiihgQTtEemzjJhbuttuTR0wASZCkmTQnwwU4DlhbVXcAJPkwsAKYeLhsywf/8a6xLv+wRXvx+MYnuP+h4bdK774gLH3m07n3G9/ikceH33zwrGfuzR4LPJMqzTV/8FP/jhcsXdTrMudquBwK3D0wvg544ZaNkpwFnAWwZMmSHVrRn7zqefz6R7+4Q/POpBc8axG7L9iNj6y+e+j0Iw/ah8MP2JvDFu3NJ29dD8CSRXvzlQ2PPtnmB7533xnpq6SZtdfuC3pf5lwNl2HngWqrQtX5wPkAy5cv32r6KH762MX89LGLd2TWifjDV/7gpLsgaR6Yq+c41gGHDYwvBu6dUF8kad6Zq+FyPbAsyeFJ9gBOAy6fcJ8kad6Yk6fFqmpjktcDV9HdinxBVX1pwt2SpHljToYLQFVdAVwx6X5I0nw0V0+LSZImyHCRJPXOcJEk9c5wkST1LlU79NvBOSfJA8C/7ODsBwBf7bE7c4HbZGtuk625TbY227bJs6rqwC2LhksPkqyuquWT7seuxG2yNbfJ1twmW5sr28TTYpKk3hkukqTeGS79OH/SHdgFuU225jbZmttka3Nim3jNRZLUO49cJEm9M1wkSb0zXHZSkpOS3JZkbZKzJ92fPiU5LMmnktyS5EtJfrXVFyVZleT29nf/Vk+Sc9u2uCnJMQPLWtna355k5UD92CRr2jznJhn2orddTpIFSb6Q5K/a+OFJrm3f7yPtVQ8keVobX9umLx1Yxhtb/bYkrxioz7p9Ksl+SS5NcmvbX35ovu8nSf5b+/fm5iQfSrLnvNpPqsrPDn7oHuf/z8ARwB7AF4GjJt2vHr/fIcAxbXgf4J+Ao4A/As5u9bOBP2zDpwBX0r0J9Hjg2lZfBNzR/u7fhvdv064DfqjNcyVw8qS/94jb5teAvwD+qo1fApzWht8NvKYNvxZ4dxs+DfhIGz6q7S9PAw5v+9GC2bpPARcCv9iG9wD2m8/7Cd2r1u8E9hrYP35hPu0nHrnsnOOAtVV1R1U9DnwYWDHhPvWmqu6rqs+34YeBW+j+pVlB9x8T2t9T2/AK4KLqXAPsl+QQ4BXAqqraUFVfB1YBJ7Vp+1bV56r7N+migWXtspIsBn4MeF8bD/BS4NLWZMttMrWtLgVOaO1XAB+uqseq6k5gLd3+NOv2qST7Av8eeD9AVT1eVd9gnu8ndK802SvJQmBv4D7m0X5iuOycQ4G7B8bXtdqc0w7Tnw9cCxxcVfdBF0DAQa3ZtrbHdPV1Q+q7uncCvwE80cafCXyjqja28cHv8eR3b9MfbO2f6rbalR0BPAB8oJ0qfF+SpzOP95Oqugd4G/AVulB5ELiBebSfGC47Z9h53zl3b3eS7wE+Bryhqh6arumQWu1AfZeV5D8C66vqhsHykKa1nWlzZpvQ/R/6McB5VfV84BG602DbMue3Sbu+tILuVNb3Ak8HTh7SdM7uJ4bLzlkHHDYwvhi4d0J9GYsku9MFy59X1WWtfH87VUH7u77Vt7U9pqsvHlLflb0I+Ikkd9Gdingp3ZHMfu30B3z393jyu7fpzwA28NS31a5sHbCuqq5t45fShc183k9eBtxZVQ9U1XeAy4AfZh7tJ4bLzrkeWNbuANmD7kLc5RPuU2/aOd/3A7dU1dsHJl0OTN3JsxL4xED99HY30PHAg+10yFXAiUn2b/9HdyJwVZv2cJLj27pOH1jWLqmq3lhVi6tqKd0/709W1c8BnwJe2ZptuU2mttUrW/tq9dPaXUKHA8voLlrPun2qqv4VuDvJs1vpBODLzOP9hO502PFJ9m59ntom82c/mfQdBbP9Q3fnyz/R3bnxW5PuT8/f7UfoDrVvAm5sn1PozgVfDdze/i5q7QO8q22LNcDygWX9Z7qLkWuBMwbqy4Gb2zx/SntqxGz4AC9m891iR9D9S78W+CjwtFbfs42vbdOPGJj/t9r3vo2Bu59m4z4FHA2sbvvKx+nu9prX+wnwO8Ctrd8X093xNW/2Ex//IknqnafFJEm9M1wkSb0zXCRJvTNcJEm9M1wkSb0zXCQgyTOT3Ng+/5rknoHxPYa0PzLJjWPox1uSvKTv5UozzVuRpS0keTPwzap62zRtjgQuraqjZ6xjPUuysDY/50rqlUcu0nYk+Y32To6bk/zXIdOPbA9sPCbJwiRvT3JduneV/GJr87IkVye5rL2D46JtrOvPkpzahtcleXNb9k1J/u2Q9guTvKP17aYkr231l7ejrjVJ3pvN7w1Zl+R/JPkH4CeT/H2Sdyb5XGu7vLX7/SRvGFjPrUkWJ9knyZVJvtjW+cot+yRB98A5SduQ5Djg5+gecb4AuC7JZ4BH2/Tn0L3X5fSqWtP+476+qo5L8jTgmiR/2xZ3DN37Oda3+vHVPXJ+OvdX1fOT/ArdO2T+yxbTX0P3YMTnVdWmdC/o2hu4AHhxVf1zkj8HzqL7ZTvAI1X1otb/X6X7lfgPJXkp3WsEpjsaOwW4q6pObvM/Yzv91zzlkYs0vR8FPlZVj1b3TpuP0z0WB+Bg4C+BV1fVmlY7ETijXY+5lu6lWcvatGuqe0fOJrpH6SwdYf1TDwu9YRvtX0b3kqlNAFW1AXgOcHtV/XNrcxHd+1amfGSLZXyozftJ4KB0T8Helpvo3rHy1iQvqqoHR/gOmocMF2l6071O9xvAPXRPSh5s/9qqOrp9Dq+qq9u0xwbabWK0MwdT82yrfdj6UevbewXwI1uMbzl/ARv57v8+7AlQVbfQPefrS8AfJ3nTdtalecpwkab3WbprE3u1/6NfAfy/Nu2xNn5mkp9ptauA16Y9Vj3Js5PsNcb+/S3wmiQL2voW0T19d1mSI1qbnwc+M80yfrbN+2K603CPAHcBx7b6cWx+HPyhdDc7XAy8ne5Un7QVr7lI06iq65J8iO4R59C9EGtNu1uMqvpmuheIrUryCPAeYAlwY/ekddYz3tfPvofutNtNSTa2/r07yZnAZS10rgXeO80yHkryj8A+wBmt9lHg55N8ge4pvXe0+vOAtyZ5Anicra8BSYC3IkvzWpK/B15fVb3/Zkfzm6fFJEm988hFktQ7j1wkSb0zXCRJvTNcJEm9M1wkSb0zXCRJvfv/MXoufeQ4z/AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_dist_dict = dict(token_freq)\n",
    "freqs = np.array([v for i, (k, v) in enumerate(freq_dist_dict.items())])\n",
    "\n",
    "plt.plot(freqs)\n",
    "plt.xlabel('Token in corpus')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the plot above and after computing some percentiles for the frequency of values, we concluded that in the context of our project it is justified to get rid of all tokens appearing only once throughout the corpus. This should result in better context-target relations in the network and increase the chances of getting meaningful sentences from the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41453"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting all the tokens that appear only once (i.e. hapax legomena)\n",
    "to_delete = [k for k,v in token_freq.items() if float(v) == 1]\n",
    "for k in to_delete:\n",
    "    del token_freq[k]\n",
    "len(token_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "By applying this filter we got rid of roughly half of our tokens. \n",
    "We will use this list of hapax legomena to create trigrams that don't contain any of the hapaxes in the next step.\n",
    "These trigrams will be used starting with the third model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 2: Implementing trigrams and setting up variables that will go into the network\n",
    "\n",
    "Here we create the trigrams, consisting of two preceding context tokens and the target token, that will be fed into the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#in case we are loading a particular set of trigrams, run this cell\n",
    "#if we do this we can then skip to the cell defining the vocabulary. It starts with voc = set()\n",
    "with open(\"trigrams_test6.txt\", \"rb\") as fp:\n",
    "    trigrams = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2921828"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_trigrams = []\n",
    "#getting trigrams sentence by sentence\n",
    "for sentence in sentences:\n",
    "    temp_trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]\n",
    "len(temp_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each trigram will have this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['it', 'does'], 'not')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_trigrams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Any training sessions we tried to implement using all trigrams without filtering resulted in kernel death. Our guess is that including all ~3,000,000 was too much. That is why for the first and second models we decided to draw 50,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "random.seed(163)\n",
    "temp_trigrams = random.sample(temp_trigrams, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As a consequence, our vocabulary length for the first and second models dropped from 88,331 tokens to about ~16,500. But we have to keep in mind that more than half of those ~88,000 tokens only appeared once in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "On a first approach we added the filtering step into the trigram generation step, but filtering $\\sim$3,000,000 objects was also very inefficient. That is why we decided to first do the sampling and then remove all trigrams that include any hapax legomena in to_delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trigrams = []\n",
    "#we will only accept those consisting only of tokens that appear at least twice\n",
    "for tri in temp_trigrams:\n",
    "    if tri[1] not in to_delete:\n",
    "        if tri[0][0] not in to_delete:\n",
    "            if tri[0][1] not in to_delete:\n",
    "                trigrams.append(tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Even with this solution, generating the trigrams takes a while, so we will use `pickle` to save the object as a `.txt` file. This will allow us to reproduce results faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#saving the current set of trigrams\n",
    "with open(\"trigrams_test6.txt\", \"wb\") as fp:\n",
    "    pickle.dump(trigrams, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the next cell we create a set containing all tokens found in our trigrams. From now on this will be our vocabulary, and its length will be the size of our network's input and output layers. We will also create a dictionary with token:frequency pairs that will allow us to assign numerical values to each trigram and therefore to have a way to make computations with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "voc = set()\n",
    "for tri in trigrams:\n",
    "    voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "    \n",
    "voc_length = len(voc)\n",
    "\n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The last preparation step is creating tensors containing the frequencies of each element and of each trigram. These tensors are the arguments we pass onto the training function and contain the context-target relationships from which our model will learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to cont\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Exploring the relation between number of trigrams and length of our vocabulary\n",
    "\n",
    "From the first two models, it was clear that, while including more trigrams leads to better text generation, it also meant having a larger vocabulary size, which in turn directly affects the number of nodes in our network's input and output layers.\n",
    "We were curious about the relationship between these two variables of trigrams and vocabulary size, so we plotted their relationship and used that plot to decide on an acceptable value for the number of trigrams used.\n",
    "\n",
    "Note that running the chunk below will modify the trigram and vocabulary sets from the previous parts. \n",
    "If that happens, please re-run the code from the trigrams implementation section above before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "random.seed(163)\n",
    "\n",
    "temp_trigrams = []\n",
    "#getting all trigrams in the corpus\n",
    "for sentence in sentences:\n",
    "    temp_trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)] #if (sentence[i] not in to_delete and sentence[i+1] not in to_delete) and sentence[i+2] not in to_delete]\n",
    "\n",
    "size_trigrams = np.arange(10000, 100000, 10000)\n",
    "size_vocab = np.zeros(len(size_trigrams))\n",
    "\n",
    "#for each sample size, cleaning the set of trigrams and getting its vocab size.\n",
    "for i in range(len(size_trigrams)):\n",
    "    sampled_trigrams_temp = random.sample(temp_trigrams, size_trigrams[i])\n",
    "    trigrams = []\n",
    "    for tri in sampled_trigrams_temp:\n",
    "        if tri[1] not in to_delete:\n",
    "            if tri[0][0] not in to_delete:\n",
    "                if tri[0][1] not in to_delete:\n",
    "                    trigrams.append(tri)\n",
    "    voc = set()\n",
    "    for tri in trigrams:\n",
    "        voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "    size_vocab[i] = len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(size_trigrams, size_vocab, size_trigrams, np.repeat(20500, len(size_trigrams)))\n",
    "plt.xlabel('Number of sampled and filtered trigrams')\n",
    "plt.ylabel('Vocabulary size')\n",
    "plt.savefig('trigrams_vs_vocab.png')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* The orange line has a value of 20,500 and represents half of the vocabulary size.\n",
    "* The blue line represents the number of trigrams we sampled versus the vocabulary size they contain.\n",
    "\n",
    "Because it seems that the marginal training cost (reflected by the vocabulary size) of including more trigrams is decreasing (we see this tendency in the slope), we decided to include ~90,000 trigrams in our third model. From the plot above we see that with this number of trigrams we are capturing relations representing about half of the total vocabulary size. Since we sampled randomly from the set of all trigrams, we also have a random sample of our vocabulary. And because of Zipf's law we can be certain that with a sample of about half the vocabulary size we are capturing most of the meaningful tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 3: Building the network and training it\n",
    "\n",
    "As mentioned above, this section was inspired by the PyTorch tutorial we had during the class period and by several papers and blog posts. They are all referenced in the project reports and at the end of both the demo and the walkthrough.\n",
    "\n",
    "As always, the first step is to check if we have a GPU to train our model on. \n",
    "It was not the case for any of our trials, but we will include the code for future reference and implementations. \n",
    "Note that since we knew that we would not have access to GPUs, our code does not include several .cuda() sections that would be necessary to run it in a GPU setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bad, training on CPU. Keep the number of epochs low!\n"
     ]
    }
   ],
   "source": [
    "#Cheking if we have a GPU to train our model on\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Fancy setup!')\n",
    "else: \n",
    "    print('Too bad, training on CPU. Keep the number of epochs low!')\n",
    "\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Then we defined our `torch.nn` class and its usual methods. We decided to implement a GRU-based network, rather than an LSTM. This choice is justified in the project report.\n",
    "\n",
    "Because of both time and computational power limitations, many of the features we decided to implement were the common choice among our sources: the decoder linear type, `Variable` as the autograd method, the step optimizer using `Adam`, and cross entropy loss as the loss criterion. \n",
    "\n",
    "The network parameters we did experiment with were the sizes of the input, output and hidden layers and also the number of hidden layers. Although the rule of thumb is generally to start with a number of nodes in the hidden layers that is on the same order of magnitude as the input or output layers, we estimated that doing so was going to result in training times longer than reasonable for the scope of the project.\n",
    "\n",
    "The main objective of this text generator was to output coherent and grammatically correct sentences. Of course, generating sequences of tokens related to a certain seed (so that the text generator is actually describing the main word) is also an important objective, but we attempted to brute-force this into the final output rather than incorporate it at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s #length of our vocab\n",
    "        self.hidden_s = hidden_s #to experiment with\n",
    "        self.output_s = output_s #length of our vocab\n",
    "        self.n_layers = n_layers #to experiment with\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))\n",
    "    \n",
    "def train(context, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(trigrams)):\n",
    "        output, hidden = decoder(context[t], hidden)\n",
    "        loss += criterion(output, target[t])\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data.item() / len(trigrams)\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next comes the training step. This set up corresponds to the third model we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#RUN THIS CELL ONLY IF YOU WANT TO TRAIN A NEW MODEL. THE MODELS WE ALREADY TRAINED CAN BE LOADED IN THE NEXT CELL\n",
    "n_epochs = 100\n",
    "print_every = 5\n",
    "plot_every = 10\n",
    "hidden_s = 50\n",
    "n_layers = 3\n",
    "lr = 0.015\n",
    "\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(cont,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#saving the current model's state_dict\n",
    "path = os.getcwd()+'/test6_trained_inference.pt'\n",
    "\n",
    "torch.save(decoder.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (encoder): Embedding(18665, 50)\n",
       "  (gru): GRU(100, 50, num_layers=3, batch_first=True)\n",
       "  (decoder): Linear(in_features=50, out_features=18665, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading an already trained decoder\n",
    "path = os.getcwd()+'/test6_trained_inference.pt'\n",
    "#make sure that the following structure coincides with the one of the model being loaded\n",
    "hidden_s = 50\n",
    "n_layers = 3\n",
    "lr = 0.015\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "decoder.load_state_dict(torch.load(path))\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 4: Generating our taboo player's descriptive sentence\n",
    "\n",
    "Now that we trained our decoder, we are able to generate sentences iteratively token by token. In order to do this, we will generate a distribution of possible next tokens based on a seed and choose the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def next_token_generator(seed, generation_length=100):\n",
    "    \"\"\"\n",
    "    Given a seed and a length, it returns a sequence of generated tokens. \n",
    "    It generates the tokens one by one, concatenating the new tokens to the seed and taking the seed's last two tokens as context for the next generation.\n",
    "\n",
    "    Arg:\n",
    "        seed: a string of minimal length = 2 which will serve as context for the first generation step.\n",
    "        generation_length: integer value representing the number of tokens we desire to generate.\n",
    "    Returns:\n",
    "        A string consisting of the concatenation of the seed and all generated tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(generation_length):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in seed.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).exp()\n",
    "        top_choice = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_choice)]\n",
    "        seed += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With a working sentence generator that in theory is able to produce coherent sequences, the next challenge was to use it in a way that would return a sequence with enough meaningful words related to our main word. \n",
    "\n",
    "First, we generated a set of input words that are related in some way to our main word. This set includes the main word's hypernyms, hyponyms, and synonyms (using the semantic relations encoded in WordNet and functions implemented for the card generator), as well as all taboo words. We are aiming to have a certain number of these words in our final sentence.\n",
    "\n",
    "Below is the code to generate these `input_words` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def gen_input_words(mw, model):\n",
    "    \"\"\"\n",
    "    Given a main word and a gensim model, it generates a set of input words that we are aiming to have in our descriptive sentence.\n",
    "    Arg:\n",
    "        mw: a string containing the main word that we are aiming to describe\n",
    "        model: gensim model from which we are retrieving words semanticaly related to the mw \n",
    "    Returns:\n",
    "        A set of strings consisting of the mw, its synonyms, hypernyms and hyponyms, and all its associated taboo words. All of them contained in the vocabulary.\n",
    "    \"\"\"\n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but they loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab.\n",
    "    input_words = [word for word in input_words if word in voc]    \n",
    "    return input_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The next step is the iterative process by which we expect to have a descriptive sequence as an output. Our approach was to not limit ourselves to using only one seed, but to try using several. Even though it was clear from the beginning that simply concatenating generated `sub_segments` would not result in a coherent sentence, having many `sub-segment`s with their own seeds trying to generate related words in parallel increases our chances of finding `input_words` inside the final sequence.\n",
    "\n",
    "The most frequent descriptive structures in our description corpus are \"X is\", \"X means\", and \"X can be found\". \n",
    "It is worth noting that the \"is\" seed appeared ~1000 times more often than \"means\", and \"means\" about ~100 times more often than \"can be found\". \n",
    "Our seed will always consist of our main word + \"is\" or \"means\" or \"can be found\". \n",
    "We limited the experiment to either 2 or 3 seeds in each sentence.\n",
    "\n",
    "After each iteration, we assign a success score to the concatenated final sequence. The score is an integer counting how many of the input words we have in the sequence, without taking into account the initial seeds. We agreed that a score of 2 was high enough to be considered a success, meaning that two of the `input_words` should appear in the sentence as an indicator that the text being generated is semantically related to the main word. \n",
    "\n",
    "In case one of the segments generates a `sub_sequence` containing one of the `input_words`, we \"block\" that segment. \n",
    "This means that that segment will be locked down and the iteration will stop, so that it will stay constant throughout the rest of the process. All other `sub_segment`s will keep generating until the desired score is reached. \n",
    "\n",
    "(If `sub_segment` X includes an input word and has been \"blocked\", the next `sub_segment` X+1 takes X into account as part of its seed. Theoretically, this should help by adding better context into the generating step. However, while this is a good idea, this is actually not a useful strategy because `next_token_generator()` actually only takes the last two segments of each seed into account. Nevertheless, this idea is still part of the implementation below.)\n",
    "\n",
    "For practical purposes we stop generating after some fixed number of iterations, should the score not be reached.\n",
    "\n",
    "The code below describes this iterative process for the cases in which we use two or three seeds. We recommend reading first the part of the code describing the process for only two seeds, as the version for three seeds works exactly the same but is messier because of we had to cover more cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    \"\"\"\n",
    "    Given the main word we are trying to describe, it will iteratively create sequences aiming to contain some of the input words generated by gen_input_words(mw, model).\n",
    "    If the desired score was not reached it will return the last generated sequence, after doing n_iterations of the process.\n",
    "    Arg:\n",
    "        mw = main word as string\n",
    "        model = embeddings used to generate the cards\n",
    "        n_seeds = int, if we are using 2 or 3 seeds during the sentence generation step\n",
    "        n_iterations = int, how many sequences we will test\n",
    "        debugging = Boolean, True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "        printing = Boolean, True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    Returns:\n",
    "        A string containing the last generated sequence.\n",
    "    \"\"\"\n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence, dividing the whole sequence into equally long sub_sequences\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 7), next_token_generator(mw+' is', 7), next_token_generator(mw+' can be found', 5)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that already contains an input word.\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence to know where input words are located and to which sub_sequence they belong.\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        #for practical purposes we stop generating after some fixed number of iterations in case the score was not reached.\n",
    "        while i < n_iterations and score <2 :\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change in the score. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index]\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                    \n",
    "                #keeping the segment in which the improvement took place, blocking it and continue the generating process\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 7) \n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) \n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    else:\n",
    "                        temp =  next_token_generator(sentence_parts[1]+' '+ mw+' can be found', 5) \n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            #choosing what to print\n",
    "            if i == 0:\n",
    "                print('The set of input words we are trying to introduce into our sequence is: '+str(input_words))\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                    if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 11), next_token_generator(mw+' is', 12)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 2\n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations and score <2:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 2\n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 12)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11) \n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 12)\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "                \n",
    "            #choosing what to print\n",
    "            if i == 0:\n",
    "                print('The set of input words we are trying to introduce into our sequence is: '+str(input_words))\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                    if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If we would let the generator iterate until a score of two had been reached, we would have a final sequence of tokens containing two of the input words that would ideally be meaningful for guessing the main word. In order to present that final \"sentence\", we have to make sure that it does not include the main word nor the taboo words.\n",
    "\n",
    "First, we replaced any instances of the main word by the string \"The main word\".\n",
    "\n",
    "Then, for the taboo words: following what a human taboo player might do, we decided to look for synonyms (that are themselves not taboo words or the main word) and choose one randomly. If there are no synonyms, we will look for hypernyms and paraphrase the sentence to transmit that change, saying instead \"is a type of [hypernym]\". If there also aren't any hypernyms, the player panics and says \"No idea!\" for that position of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def sentence_cleaner(sentence, mw, model):\n",
    "    '''\n",
    "    Makes sure that the generated sequence given by description_generator() follows taboo rules and does not contain the main word or any taboo word.\n",
    "    Arg: \n",
    "        sentence: string containing the sequence to clean\n",
    "        mw: string containing the main word we are playing with \n",
    "        model: gensim embeddings model we are retrieving semantic relations from\n",
    "    Returns:\n",
    "        A string containing a version of the descriptive sentence that follows taboo rules.\n",
    "    '''\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, '.The main word')\n",
    "    \n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            #if we have at least one\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                #choose one randomly\n",
    "                choice = np.random.choice(syns)\n",
    "                #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                while (choice in taboo_words or choice != mw) and len(syns) > 0:\n",
    "                    syns = syns.pop(syns.index(choice))\n",
    "                    choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "                #if all synonyms where taboo words or the mw\n",
    "                if choice in taboo_words or choice == mw:\n",
    "                    hypers = sr.get_hypernyms(tw)\n",
    "                    #if we have at least one\n",
    "                    if len(hypers) > 0:\n",
    "                        hypers = list(hypers)\n",
    "                        #choose one randomly\n",
    "                        choice = np.random.choice(hypers)\n",
    "                        #checking that the chosen one is not a taboo word either, or the main word + making sure that it doesn't loop indefinitely\n",
    "                        while (choice in taboo_words or choice != mw) and len(hypers) > 1:\n",
    "                            syns = syns.pop(syns.index(choice))\n",
    "                            choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'is a type of '+choice)\n",
    "            #in case no synonyms were found\n",
    "            else:\n",
    "                hypers = sr.get_hypernyms(tw)\n",
    "                #if we have at least one\n",
    "                if len(hypers) > 0:\n",
    "                    hypers = list(hypers)\n",
    "                    #choose one randomly\n",
    "                    choice = np.random.choice(hypers)\n",
    "                    #checking that the chosen one is not a taboo word either, or the main word + making sure that it doesn't loop indefinitely \n",
    "                    while (choice in taboo_words or choice != mw) and len(hypers) > 1:\n",
    "                        syns = syns.pop(syns.index(choice))\n",
    "                        choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'is a type of '+choice)\n",
    "                else: \n",
    "                    sentence = sentence.replace(tw, \"NO IDEA!\")\n",
    "    sentence = sentence[1:]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The final output function will implement all generating steps until either the desired score is reached (2 by default) or we iterate the given number of times (minimum 5). The printing and debugging arguments allow us to decide what we want to keep track of, as explained in `description generator()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def final_output(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = True):\n",
    "    mw = mw.lower()\n",
    "    sentence = description_generator(mw, model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, model)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['airliner', 'airplane', 'biplane', 'fighter', 'flight', 'jet']\n",
      "Sentence number: 1\n",
      "airplane means ericsson printing orientation columbia workplace persev airplane is lighter opt dwarfs considerably leases contribution aquife airplane can be found fertilized warlord wise asleep economist\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "airplane means trying cost-effective res observed wristwatch don hoste airplane is thematic economist wellington pickle dwarfs dire keep airplane can be found carp right-wing seasonal pope guthrie\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "airplane means bar on-road believable bsa jihad condiment outwardly  airplane is soil workplace arranged tram classic enlargement goals airplane can be found urge mates geometric boneless coerced\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "airplane means sleep obama weaponry classic underside demolition produ airplane is oklahoma marcello dictatorships influenced infinitely tumb airplane can be found garlands expressing tag choices leprechaun\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "airplane means smaller motorists moreland imss preventative couple cou airplane is injected montero rule bsa distinctive blaming ramen airplane can be found easier mate shallowness coast antiparticle\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "airplane means trusted microbicide aggregation gun productive pcn prev airplane is reformation feel find copious david motorists barry airplane can be found rival physicist thursday refusing unaided\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "airplane means hearings surrounds hears continued make-or-break produc airplane is fetched gumbo consumption solver erg backstop westminster airplane can be found arches proto-oncogene sellers trunk assemble\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "airplane means 80,000 bitch argument backstop enlargement wheel 1957  airplane is emperor compute equality textile censuses anon elemental airplane can be found reliability ecu killing elemental economist\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "airplane means copious borrow gun proletarian pre-existing hazards per airplane is xerox dead compare 02 gun bsa gun airplane can be found shed spaces exercise joked token\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "airplane means raping ponies jihad jihad classic normative chromecast  airplane is homemade transmit bacterium invading inundated billboard e airplane can be found processors burglary smashed elegantly stubbornly\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means raping ponies jihad jihad classic normative chromecast  .The main word is homemade transmit bacterium invading inundated billboard e .The main word can be found processors burglary smashed elegantly stubbornly'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output('airplane', model, n_seeds = 3, n_iterations = 10, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set of input words we are trying to introduce into our sequence is: ['airliner', 'airplane', 'biplane', 'fighter', 'flight', 'jet']\n",
      "Sentence number: 1\n",
      "airplane means surprising jihad causing jihad gun unfounded hoover  airplane is stitch bsa re classic deserving computation deny airplane can be found shout interestingly bsa attempt gun\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "airplane means meta -66 billboard motel persevered one-third rind  airplane is sleeps entertain dictatorships announced david layouts jihad airplane can be found programmed examined post-june juneau dog\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "airplane means quickest fourteen antithesis whig mame wherein bartender  airplane is professes forever outwardly face passover imac vid airplane can be found fearful most therapists similarity k.\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "airplane means al-qaeda grieving diplomatic obama low emergence outwardl airplane is thereof aids directory type leading contribution wren airplane can be found ime.i walls equinoxiall jason educational\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "airplane means coloured tap skipping dictatorships workplace bae product airplane is converse towel associates crying gun factors gun airplane can be found highlighted dealership psd haphazard harlot\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "airplane means widespread excluding units saloon oranges swale anti-abor airplane is ostomy restricts tyrannical greed cave bsa integrating airplane can be found eris agreed illustrations dartford ucl\n",
      "Covered vector: \n",
      "[0 0 0]\n",
      "Positions vector: \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "The final sentence got a score of: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means widespread excluding units saloon oranges swale anti-abor .The main word is ostomy restricts tyrannical greed cave bsa integrating .The main word can be found eris agreed illustrations dartford ucl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'Airplane', model = model, n_seeds=3, n_iterations = 6, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## References\n",
    "- Pytorch tutorials for the ANLP class WS19-20\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy\n",
    "- [Beginners Guide to Text Generation (Pytorch)](https://www.kaggle.com/ab971631/beginners-guide-to-text-generation-pytorch) by Abhishek Kumar \n",
    "- [Text Generation with Pytorch](https://machinetalk.org/2019/02/08/text-generation-with-pytorch/) by Trung Tran. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
