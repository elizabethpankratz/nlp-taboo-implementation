{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a gold standard from existing Taboo cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and formatting the cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Taboo cards that our gold standard will be based on belong to Elizabeth's Canadian edition of Taboo, produced sometime in the 1990s or early 2000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"taboo_cards.txt\"\n",
    "\n",
    "\n",
    "def read_in(filename):\n",
    "    \"\"\"\n",
    "    Reads in transcribed Taboo card words contained in the given file and returns them in an enumerated list.\n",
    "    \"\"\"\n",
    "    file_lines = []\n",
    "    \n",
    "    with open(filename, \"r\", encoding='utf-8') as myfile:\n",
    "        \n",
    "        # Go through every line, saving non-empty ones to the list file_lines.\n",
    "        for line in myfile:\n",
    "            if line.strip() != '':             \n",
    "                file_lines.append(line.strip()) \n",
    "                \n",
    "    return list(enumerate(file_lines))\n",
    "    \n",
    "            \n",
    "def format_cards(enum_list):\n",
    "    \"\"\"\n",
    "    Given an enumerated list (output of read_in()), formats the contents as a dictionary (key = MW, values = list of TWs)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise dictionary to contain card data.\n",
    "    card_dict = dict()\n",
    "    \n",
    "    # Assign MWs (every sixth word in the enumerated list) as dictionary keys, and create a list for the dict's\n",
    "    # value consisting of the five following words (the TWs); the word[1:] removes the dash from the beginning of\n",
    "    # each TW's string.\n",
    "\n",
    "    for enum, wd in enum_list:\n",
    "        if enum % 6 == 0:\n",
    "            card_dict[wd] = [word[1:] for num, word in enum_list[enum+1:enum+6]]\n",
    "        \n",
    "    return card_dict\n",
    "        \n",
    "\n",
    "enum_lines = read_in(FILENAME)\n",
    "cards = format_cards(enum_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maple', 'pancakes', 'trees', 'sap', 'sweet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: the five TWs from the MW 'syrup'\n",
    "cards['syrup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this dictionary format, it will be helpful to set up a pandas dataframe for easy addition of columns for semantic similarity and corpus-based collocation measures below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw\n",
       "0  huddle    gather\n",
       "1  huddle  football\n",
       "2  huddle     group\n",
       "3  huddle      play\n",
       "4  huddle  together"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas dataframe quickly based on a list of dictionaries, with each dictionary corresponding to a row in the \n",
    "# dataframe.\n",
    "# ( Source: https://stackoverflow.com/questions/10715965/add-one-row-to-pandas-dataframe/17496530#17496530 )\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "for mainwd, tabwds in cards.items():\n",
    "    for tabwd in tabwds:\n",
    "        \n",
    "        # Create a dictionary for each row of the dataframe (key = column name, value = row value for that column)\n",
    "        row = {\n",
    "            'mw': mainwd,\n",
    "            'tw': tabwd\n",
    "        }\n",
    "        \n",
    "        # Append to rows_list, and use that list as a basis for the new dataframe.\n",
    "        rows_list.append(row)\n",
    "\n",
    "data = pd.DataFrame(rows_list)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to csv for manual annotation\n",
    "#data.to_csv(r'gold-std-raw.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "      <th>semrel_synonym</th>\n",
       "      <th>semrel_antonym</th>\n",
       "      <th>semrel_hyponym</th>\n",
       "      <th>semrel_hypernym</th>\n",
       "      <th>collocation</th>\n",
       "      <th>cultural_ref</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw  semrel_synonym  semrel_antonym  semrel_hyponym  \\\n",
       "0  huddle    gather             0.0             0.0             0.0   \n",
       "1  huddle  football             0.0             0.0             0.0   \n",
       "2  huddle     group             0.0             0.0             0.0   \n",
       "3  huddle      play             0.0             0.0             0.0   \n",
       "4  huddle  together             0.0             0.0             0.0   \n",
       "\n",
       "   semrel_hypernym  collocation  cultural_ref  other  \n",
       "0              1.0          0.0           0.0    0.0  \n",
       "1              0.0          1.0           0.0    0.0  \n",
       "2              1.0          0.0           0.0    0.0  \n",
       "3              0.0          1.0           0.0    0.0  \n",
       "4              0.0          1.0           0.0    0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After manual annotation, read the csv back in and save as pandas dataframe, replacing NaNs with 0.\n",
    "GS_CSV_FILE = 'gold-std-categorised.csv'\n",
    "\n",
    "gs_df = pd.read_csv(GS_CSV_FILE, encoding='utf-8')\n",
    "gs_df.fillna(0,inplace=True)\n",
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count and plot the number of categories appearing in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the sum of each column as a series.\n",
    "gs_df_sum = gs_df.loc[:, 'semrel_synonym':'other'].sum()\n",
    "gs_df_sum.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# Create a bar plot object out of it.\n",
    "# (Plot y axis labels as integers by creating a list of integers based on the y range)\n",
    "yint = [x for x in range( int(gs_df_sum.max()) + 1 )]\n",
    "gs_df_sum_plot = gs_df_sum.plot.bar(x='category', y='count')\n",
    "\n",
    "# For readable x axis tick labels, set their rotation angle to 30 and the horizontal alignment to right.\n",
    "gs_df_sum_plot.set_xticklabels(gs_df_sum_plot.get_xticklabels(), rotation=30, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the figure, convert to a \"figure\" object and then export as pdf\n",
    "fig = gs_df_sum_plot.get_figure()\n",
    "fig.savefig('freq_plot.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert this frequency distribution to a probability distribution\n",
    "- associate each label with a given weight, based on its frequency\n",
    "- RV: semrel\n",
    "- values of semrel: the categories above (that we  care about -- ignore \"cultural_ref\" and \"other\")\n",
    "- return dict w labels and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before converting to a probability distribution, remove the categories we aren't interested in (\"cultural_ref\" and \"other\").\n",
    "gs_df_sum.drop(labels=['cultural_ref', 'other'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collocation        0.663428\n",
       "semrel_synonym     0.196996\n",
       "semrel_hypernym    0.101590\n",
       "semrel_hyponym     0.020318\n",
       "semrel_antonym     0.017668\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the frequency distribution to a probability distribution by dividing by the sum of all observations\n",
    "prob_dist = gs_df_sum / gs_df_sum.sum()\n",
    "prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collocation',\n",
       " 'collocation',\n",
       " 'semrel_synonym',\n",
       " 'semrel_synonym',\n",
       " 'semrel_hypernym']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dict = dict(prob_dist)\n",
    "\n",
    "\n",
    "def select_one_category(prob_dist_dict):\n",
    "    \"\"\"\n",
    "    Given a probability distribution of semantic relation labels, randomly returns one of them, weighted by probability.\n",
    "    \n",
    "    Arg:\n",
    "        prob_dist_dict: a dictionary with semantic relation labels as keys and their probability as values.\n",
    "    Returns:\n",
    "        A string corresponding to one semantic relation label.\n",
    "    \"\"\"\n",
    "    # For clarity, save keys as labels and values as probabilities.\n",
    "    labels = list( prob_dist_dict.keys() )\n",
    "    probs = list( prob_dist_dict.values() )\n",
    "    \n",
    "    # Use numpy's .choice() to return a label based on the given weight.\n",
    "    return np.random.choice(labels, p=probs) \n",
    "\n",
    "\n",
    "def select_five_categories(prob_dist_dict):\n",
    "    \"\"\"\n",
    "    Given a probability distribution of semantic relation labels, randomly returns a list of five of them, weighted \n",
    "    by probability.\n",
    "    \n",
    "    Arg:\n",
    "        prob_dist_dict: a dictionary with semantic relation labels as keys and their probability as values.\n",
    "    Returns:\n",
    "        A list containing five semantic relation labels (intended as the starting point for each card).\n",
    "    \"\"\"\n",
    "    # For clarity, save keys as labels and values as probabilities.\n",
    "    labels = list( prob_dist_dict.keys() )\n",
    "    probs = list( prob_dist_dict.values() )\n",
    "    \n",
    "    # Use numpy's .choice() to return a label based on the given weight.\n",
    "    return list( np.random.choice(labels, 5, p=probs) )\n",
    "\n",
    "\n",
    "select_five_categories(pd_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pooh', 'pooh', 'pooh', 'pooh', 'piglet']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n",
    "list( np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing semantic similarity with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# The following line of code requires that the large word embeddings file is in the current directory (not set up on\n",
    "# GitHub because it is too large to push around nicely, so for replication, have to add this file manually).\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('floral', 0.7493643760681152),\n",
       " ('flowers', 0.7488502264022827),\n",
       " ('roses', 0.697738766670227),\n",
       " ('orchid', 0.6928980350494385),\n",
       " ('tulip', 0.6629416346549988),\n",
       " ('peony', 0.6613221764564514),\n",
       " ('blooms', 0.6554170250892639),\n",
       " ('blossoms', 0.6527379751205444),\n",
       " ('chrysanthemum', 0.6500308513641357),\n",
       " ('anthurium', 0.6497662663459778)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = model.most_similar('flower')\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floral\n",
      "flowers\n",
      "roses\n",
      "orchid\n",
      "tulip\n",
      "peony\n",
      "blooms\n",
      "blossoms\n",
      "chrysanthemum\n",
      "anthurium\n"
     ]
    }
   ],
   "source": [
    "for tup in sim:\n",
    "    if 'cat' not in tup[0]:\n",
    "        print(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['floral',\n",
       " 'flowers',\n",
       " 'roses',\n",
       " 'orchid',\n",
       " 'tulip',\n",
       " 'peony',\n",
       " 'blooms',\n",
       " 'blossoms',\n",
       " 'chrysanthemum',\n",
       " 'anthurium']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tup[0] for tup in sim if 'cat' not in tup[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bovine', 'cow', 'goat', 'hog', 'rabbit', 'swine'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def get_collocations(word, forbidden_wds, gensim_model, num_collocates, num_to_check = 10):\n",
    "    \"\"\"\n",
    "    Returns minimum num_collocates most similar words to the given word based on gensim word embeddings.\n",
    "    \n",
    "    Arg:\n",
    "        word: A string representing the main word.\n",
    "        forbidden_wds: A set containing words as strings that may not be included as output.\n",
    "        gensim_model: The pre-trained word embeddings.\n",
    "        num_collocates: An integer, the number of collocates to generate.\n",
    "        num_to_check: (default 10) the number of most similar words to begin with.\n",
    "    Returns:\n",
    "        A list of collocated words as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    \n",
    "    # Use gensim's most_similar() function to get the (initially ten) words whose embeddings are most similar to the\n",
    "    # input word's. Lemmatise the words to remove plural/other inflections.\n",
    "    similar_tups = gensim_model.most_similar(word, topn=num_to_check)\n",
    "    similar_wds = [lemmatizer.lemmatize( tup[0] ) for tup in similar_tups]\n",
    "    \n",
    "    # Now save those words that do not contain the input word.\n",
    "    filtered = [wd for wd in similar_wds if (word not in wd.lower() and wd not in forbidden_wds)]\n",
    "    filtered = set(filtered)\n",
    "    \n",
    "    # Recursive bit: Check if there are at least num_collocates different words in filtered (base case).\n",
    "    # If not, increase the number of words to check in each recursive iteration by three and run the function again. \n",
    "    # Will stop once there are minimum num_collocates words in filtered.\n",
    "    \n",
    "    if len(filtered) >= num_collocates:\n",
    "        return filtered\n",
    "    else:\n",
    "        num_to_check += 3\n",
    "        return get_collocations(word, forbidden_wds, gensim_model, num_collocates, num_to_check)\n",
    "        \n",
    "\n",
    "get_collocations('pig', {'chicken', 'pup'}, model, num_collocates=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet semantic relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit worked well! Using WordNet, I created dictionaries for a given input word that contain all of that word's synonyms, antonyms, hypernyms, and hyponyms :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_to_synsets(word):\n",
    "    \"\"\"\n",
    "    Converts the given word to a synset object.\n",
    "    \n",
    "    Arg:\n",
    "        word: a string like 'cat'\n",
    "        pos: the desired part of speech (choices: wn.NOUN, wn.VERB, wn.ADJ, wn.ADV)\n",
    "    Returns:\n",
    "        A string containing the first synset ID, formatted according to WordNet's conventions, e.g. 'cat.n.01',\n",
    "        corresponding to that word.\n",
    "    \"\"\"\n",
    "    # Convert word string to the synset with the corresponding part of speech.\n",
    "    return wn.synsets(word)\n",
    "    \n",
    "\n",
    "def synset_to_word(synset):\n",
    "    \"\"\"\n",
    "    Converts the given synset to the actual word it represents.\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A string containing the word corresponding to that synset.\n",
    "    \"\"\"   \n",
    "    # Convert synset to lemma, since this is what name() is defined over.\n",
    "    return synset.lemmas()[0].name()\n",
    "\n",
    "\n",
    "def get_antonyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all antonyms for the given synset.\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of antonymic words as strings, if there are any, or else the empty list.\n",
    "    \"\"\"    \n",
    "    # Convert synset to lemma, since this is what the antonym relation is defined over, and get antonym(s).\n",
    "    ant_lemmas = synset.lemmas()[0].antonyms()\n",
    "    \n",
    "    # Convert each antonym in this list to a string and return list (empty if no antonyms).\n",
    "    return [ant_lemma.name() for ant_lemma in ant_lemmas]\n",
    "\n",
    "\n",
    "def get_hypernyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all immediate hypernyms for the given synset.\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of hypernymic words as strings.\n",
    "    \"\"\"\n",
    "    # Convert hypernyms of the synset to strings and return list.\n",
    "    return [synset_to_word(hyper) for hyper in synset.hypernyms()]\n",
    "\n",
    "\n",
    "def get_hyponyms(synset):\n",
    "    \"\"\"\n",
    "    Returns all immediate hyponyms for the given synset. (There are often many.)\n",
    "    \n",
    "    Arg:\n",
    "        synset: a WordNet Synset object\n",
    "    Returns:\n",
    "        A list of hyponymic words as strings.\n",
    "    \"\"\"\n",
    "    # Convert hypernyms of the synset to strings and return list.\n",
    "    return [synset_to_word(hypo) for hypo in synset.hyponyms()]\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Returns a set of synonyms, according to WordNet, for the given input word (using all of its senses, if\n",
    "    there are multiple).\n",
    "    \n",
    "    Arg:\n",
    "        word: a string representing the word whose synonyms we want.\n",
    "    Returns:\n",
    "        A set containing all of the other words in the same WordNet synset as the given word.\n",
    "    \"\"\"\n",
    "    # Initialise set that will collect the synonyms.\n",
    "    synonym_set = set()\n",
    "    \n",
    "    # Convert the word to a list of synsets.\n",
    "    synset_list = word_to_synsets(word)\n",
    "\n",
    "    # Get all the lemmas corresponding to the given word's synset.\n",
    "    synonym_lems = [x.lemmas() for x in synset_list]\n",
    "    \n",
    "    # Go through them, get the names from the lemma (lowercasing everything for consistency), and add\n",
    "    # to synonym_set.\n",
    "    for lemma_list in synonym_lems:\n",
    "        syn = lemma_list[0].name().lower()\n",
    "        synonym_set.update( [syn] )\n",
    "\n",
    "    # Remove from the synonym set the input word and any words that also contain the input word and return.\n",
    "    to_rm = set()\n",
    "    for synonym in synonym_set:\n",
    "        if synonym == word or word in synonym:\n",
    "            to_rm.update({synonym})\n",
    "\n",
    "    return synonym_set.difference(to_rm)\n",
    "\n",
    "\n",
    "def make_semrel_dict(word):\n",
    "    \"\"\"\n",
    "    Creates a dictionary that contains all words standing in the given semantic relation to the main word.\n",
    "    \n",
    "    Arg:\n",
    "        gensim_model: -----\n",
    "        word: a string like 'cat' (the main word)\n",
    "    Returns:\n",
    "        A dictionary with the semantic relations as keys and a set of words that have that relation to all senses\n",
    "        of the input word, according to WordNet, as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise dictionary (and we can get synonyms right away).\n",
    "    semrel_dict = {\n",
    "        'semrel_synonym': get_synonyms(word),\n",
    "        'semrel_antonym': set(),\n",
    "        'semrel_hypernym': set(),\n",
    "        'semrel_hyponym': set()\n",
    "    }\n",
    "    \n",
    "    # Convert the input word to all of its synsets.\n",
    "    ss = word_to_synsets(word)\n",
    "    \n",
    "    # Go through each synset, determining its antonyms, hypernyms, and hyponyms, and adding each to the set in the\n",
    "    # appropriate entry of the dictionary, as long as the main word does not appear as part of any of those strings.\n",
    "    for s in ss:\n",
    "        semrel_dict['semrel_antonym'].update( [w for w in get_antonyms(s) if word not in w] )\n",
    "        semrel_dict['semrel_hypernym'].update( [w for w in get_hypernyms(s) if word not in w] )\n",
    "        semrel_dict['semrel_hyponym'].update( [w for w in get_hyponyms(s) if word not in w] )\n",
    "        \n",
    "    return semrel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semrel_synonym': {'regretful'},\n",
       " 'semrel_antonym': {'good', 'unregretful'},\n",
       " 'semrel_hypernym': {'quality'},\n",
       " 'semrel_hyponym': {'evil',\n",
       "  'inadvisability',\n",
       "  'liability',\n",
       "  'undesirability',\n",
       "  'unsoundness',\n",
       "  'unworthiness',\n",
       "  'worse'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_semrel_dict('bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The real deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# -----\n",
    "\n",
    "# \n",
    "# five_semrels_list = ['semrel_antonym', 'semrel_hyponym', 'semrel_antonym', 'semrel_synonym', 'collocation'] # TESTER\n",
    "# five_semrels = pd.Series(five_semrels_list)\n",
    "# semrels_counts = dict( five_semrels.value_counts() )\n",
    "# srdict = make_semrel_dict('evil')\n",
    "\n",
    "\n",
    "\n",
    "# get_good_label_distrib(srdict, semrels_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'victory': ['checkmate', 'success', 'triumph', 'defeat', 'rout']}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_good_label_distrib(semrel_dict, semrel_counts):\n",
    "    \"\"\"\n",
    "    Finds a distribution of five labels that is compatible with the cardinality of the available semrel sets.\n",
    "    (Where any label is over-represented, it is replaced with 'collocation').\n",
    "    \n",
    "    Args:\n",
    "        semrel_dict: A dictionary containing the semantic relations for the given MW.\n",
    "        semrel_counts: A dictionary containing the number of each semantic relation the randomly-generated distribution wants.\n",
    "    Returns:\n",
    "        A list of five elements with the labels compatible with the number of words available in each semantic relation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do cardinality check on srdict first, to see if there are enough elements to fulfill the distribution in five_semrels.\n",
    "    srdict_counts = {key:len(value) for key, value in semrel_dict.items()}\n",
    "    \n",
    "    # This variable will hold the number of collocations to add to the distribution in place of unfulfillable other labels.\n",
    "    num_coll_to_add = 0\n",
    "    \n",
    "    # Go through all non-'collocation' labels in the dictionary that contains the requested number of labels.\n",
    "    for label, count in semrel_counts.items():\n",
    "        \n",
    "        if label != 'collocation':\n",
    "            \n",
    "            # Get the difference in cardinality between the available set and the requested set.\n",
    "            diffc = srdict_counts[label] - semrel_counts[label]\n",
    "\n",
    "            # If negative, i.e. if there are more requested than available, record the difference (this is how many instances\n",
    "            # of 'collocation' to add) and change the number of requested words to the number available.\n",
    "            if diffc < 0:\n",
    "                num_coll_to_add += abs(diffc)\n",
    "                semrel_counts[label] = srdict_counts[label]\n",
    "    \n",
    "    # Adjust the values in 'collocation' in the dictionary.\n",
    "    if 'collocation' in set(semrels_counts.keys()):\n",
    "        semrel_counts['collocation'] += num_coll_to_add\n",
    "    else:\n",
    "        semrel_counts['collocation'] = num_coll_to_add\n",
    "\n",
    "    return semrel_counts\n",
    "\n",
    "\n",
    "def card_generator(mw, prob_dist_dict, gensim_model):\n",
    "    \"\"\"\n",
    "    the big boi\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate five categories with the weighted probabilities based on their frequency in the gold standard data.\n",
    "    five_semrels_list = select_five_categories(prob_dist_dict)  # from gspd\n",
    "#     five_semrels_list = ['semrel_antonym', 'semrel_antonym', 'semrel_antonym', 'semrel_synonym', 'semrel_synonym'] # TESTER\n",
    "    five_semrels = pd.Series(five_semrels_list)\n",
    "    \n",
    "    # Count the number of instances of each semrel category in that list.\n",
    "    semrels_counts = dict( five_semrels.value_counts() )\n",
    "#     print(semrels_counts)\n",
    "#     print()\n",
    "    \n",
    "    # Generate the semantic relations dictionary.\n",
    "    srdict = make_semrel_dict(mw)  # from sr\n",
    "\n",
    "    # Rejig five_semrels_list, if need be, to one whose labels are compatible with the cardinality of the sets available\n",
    "    # in srdict.\n",
    "    good_five_labels = get_good_label_distrib(srdict, semrels_counts)\n",
    "#     print(good_five_labels, '\\n')\n",
    "\n",
    "    # Now we just populate a list with the required number of each kind of word!\n",
    "    # First, initialise list to contain the five final Taboo words (yay!)\n",
    "    tws = []\n",
    "    \n",
    "    # Go through good_five_labels and, for the labels that aren't 'collocation', access their list in the dictionary and\n",
    "    # randomly select however many out of it.\n",
    "    for label, count in good_five_labels.items():\n",
    "        if label != 'collocation':\n",
    "            tws.extend( rd.sample( tuple( srdict[label] ), count ) )\n",
    "            \n",
    "    # Now, take the number of collocations needed and return the most similar words according to gensim, removing the\n",
    "    # words that are forbidden (i.e. the main word and also the other words that are already in tws)\n",
    "    forbidden_words = set(tws + [mw])\n",
    "#     print(forbidden_words)\n",
    "    num_coll = good_five_labels['collocation']\n",
    "    \n",
    "    collocates = get_collocations(mw, forbidden_words, gensim_model, num_collocates =  num_coll)\n",
    "#     print(collocates)\n",
    "    \n",
    "    # If there are more collocates than needed, randomly select num_coll of them and add to tws. Else just add list to tws.\n",
    "    if len(collocates) > num_coll:\n",
    "        tws.extend( rd.sample( tuple(collocates), num_coll ) )\n",
    "    else:\n",
    "        tws.extend(collocates)\n",
    "    \n",
    "    return {mw: tws}\n",
    "\n",
    "\n",
    "probdist = dict(prob_dist)\n",
    "card_generator('victory', probdist, model)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
