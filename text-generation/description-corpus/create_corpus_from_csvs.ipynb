{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this file is for developing a script to reformat all of the individual csv files generated for each descriptive construction into one corpus file\n",
    "- the corpus queries are tab-separated and span three columns, so that has to be tidied up\n",
    "- then all sentences are combined into one list, which can then be exported\n",
    "- V1 uses exported files from ENCOW16A-NANO, which might be sufficient? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sents_from_csv(subdir, filename):\n",
    "    \"\"\"\n",
    "    Reformats the tab-separated outputs of SeaCOW queries for each descriptive construction into a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        subdir: a string with the subdirectory containing the CSV file.\n",
    "        filename: a string ending in '.csv'; the file to read in.\n",
    "    Returns:\n",
    "        A list of sentences as strings from the given concordance file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise empty list to fill as we go through the concordance file.\n",
    "    sent_list = []\n",
    "    \n",
    "    path = str(subdir + '/' + filename)\n",
    "\n",
    "    # Open desired file and go through it line by line.\n",
    "    with open( path , encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "\n",
    "            # We only care about the actual concordance, so, the lines in output file not beginning with # or the header 'doc'.\n",
    "            if line[0] != \"#\" and line[0:3] != \"doc\":\n",
    "\n",
    "                # Remove newlines and split by tabs (since the output is actually tab-separated), saving all but the first\n",
    "                # element of the resulting list (the original URL of the sentence) as a string to the concordance's list.\n",
    "                split_by_tab = line.strip('\\n').split('\\t')\n",
    "                sent_as_str = \" \".join( split_by_tab[1:] )                \n",
    "                sent_list.append(sent_as_str)\n",
    "        \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be_another_word_for.csv',\n",
       " 'be_det.csv',\n",
       " 'be_not_det.csv',\n",
       " 'be_part_of.csv',\n",
       " 'be_related_to.csv',\n",
       " 'be_something_that_you.csv',\n",
       " 'be_usually_used.csv',\n",
       " 'can_be_found.csv',\n",
       " 'feel_like.csv',\n",
       " 'look_like.csv',\n",
       " 'means.csv',\n",
       " 'smell_like.csv',\n",
       " 'sound_like.csv']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBDIR = 'encow16a-nano'\n",
    "\n",
    "conc_files = [file for file in os.listdir(SUBDIR) if file[-3:] == 'csv']\n",
    "conc_files\n",
    "# smell_like = get_sents_from_csv(SUBDIR, 'smell_like.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
