{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this file is for developing a script to reformat all of the individual csv files generated for each descriptive construction into one corpus file\n",
    "- the corpus queries are tab-separated and span three columns, so that has to be tidied up\n",
    "- then all sentences are combined into one list, which can then be exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sents_from_csv(subdir, filename):\n",
    "    \"\"\"\n",
    "    Reformats the tab-separated outputs of SeaCOW queries for each descriptive construction into a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        subdir: a string with the subdirectory containing the CSV file.\n",
    "        filename: a string ending in '.csv'; the file to read in.\n",
    "    Returns:\n",
    "        A list of sentences as strings from the given concordance file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise empty list to fill as we go through the concordance file.\n",
    "    sent_list = []\n",
    "    \n",
    "    path = str(subdir + '/' + filename)\n",
    "\n",
    "    # Open desired file and go through it line by line.\n",
    "    with open( path , encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "\n",
    "            # We only care about the actual concordance, so, the lines in output file not beginning with # or the header 'doc'.\n",
    "            if line[0] != \"#\" and line[0:3] != \"doc\":\n",
    "\n",
    "                # Remove newlines and split by tabs (since the output is actually tab-separated), saving all but the first\n",
    "                # element of the resulting list (the original URL of the sentence) as a string to the concordance's list.\n",
    "                split_by_tab = line.strip('\\n').split('\\t')\n",
    "                sent_as_str = \" \".join( split_by_tab[1:] )                \n",
    "                \n",
    "                # If this joining results in a space being the first character of the line, remove that space.\n",
    "                if sent_as_str[0] == \" \":\n",
    "                    sent_as_str = sent_as_str[1:]\n",
    "                \n",
    "                # Add to list.\n",
    "                sent_list.append(sent_as_str)\n",
    "        \n",
    "    return sent_list\n",
    "\n",
    "\n",
    "def create_total_sent_list(subdir):\n",
    "    \"\"\"\n",
    "    Goes through each CSV file in the given subdirectory and combines all of their concordances into one single list.\n",
    "    \n",
    "    Args:\n",
    "        subdir: a string, the name of the subdirectory to gather concordance files from.\n",
    "    Returns:\n",
    "        A list containing all sentences as strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all csv files in given subdir.\n",
    "    conc_files = [file for file in os.listdir(subdir) if file[-3:] == 'csv']\n",
    "\n",
    "    # Initialise empty list to contain sentences.\n",
    "    all_sents = []\n",
    "    \n",
    "    # Go through all files in subdir and extract the sentences from them, extending all_sents for each one and returning.\n",
    "    for conc_file in conc_files:\n",
    "        all_sents.extend( get_sents_from_csv(subdir, conc_file) )\n",
    "        \n",
    "    return all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBDIR_NANO = 'encow16a-nano'  # The smaller version of the corpus; results in 20,680 sentences\n",
    "SUBDIR_FULL = 'encow16a'       # The larger version; results in 115,735 sentences\n",
    "\n",
    "all_sents_nano = create_total_sent_list(SUBDIR_NANO)\n",
    "# len(all_sents_nano)\n",
    "\n",
    "all_sents_full = create_total_sent_list(SUBDIR_FULL)\n",
    "# len(all_sents_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A cookie is a text file that is placed on your hard disk by a Web page server .', 'A listing is a product placed in the directory for sale .', 'After all , a home is the largest ( and most emotional ) investment most people will ever make .', \"While emotions are probably in high gear once you 've found a home you love , it 's important to remember that a home is an investment .\", \"With so many questions , a consultation is the start of contributing positively to one 's future .\", 'A classroom is a small community in and of itself .', \"The recent studies like studies before them for 30 years showed cancer , heart attacks , strokes , obesity , diabetes , etc. It is said where there is smoke there is fire , and the aspartame studies that continually show aspartame as a killer are a blazing conflagration that EFSA does n't know how to deal with .\", 'An NTD is an opening in the spinal cord or brain that occurs very early in human development .', 'A pinhole is a small hole used in Confocal Microscopes at the detection side of the lenses to get rid of out-of-focus light , thus allowing to record real 3 D images ...', \"An analemma is the figure-of-eight shape that results if the Sun 's position in the sky is recorded at the same time of day throughout the year ...\"]\n"
     ]
    }
   ],
   "source": [
    "print(all_sents_nano[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write these lists to new text files.\n",
    "\n",
    "def write_to_corpus_file(sents_list, out_filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    file = open(out_filename, 'w+')\n",
    "    for sent in sents_list:\n",
    "        file.write(sent+'\\n')\n",
    "    file.close()\n",
    "    \n",
    "# write_to_corpus_file(all_sents_nano, 'description-corpus-20k.txt')\n",
    "write_to_corpus_file(all_sents_nano[:10], 'test.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
