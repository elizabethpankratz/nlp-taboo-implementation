{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Card generator walkthrough\n",
    "\n",
    "In this notebook, we walk the reader through how we developed a gold standard for our Taboo card generator based on existing Taboo cards. Then, we show how we applied this gold standard to implement our card generator, and how this card generator can be used to create brand new Taboo-style cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 1: Developing a gold standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "It was important to have some gold standard on which to base our work, because otherwise, there would be no way to know how faithful the words that we generated to serve as taboo words (TWs) for any given main word (MW) are to the style of TWs found in the actual game.\n",
    "\n",
    "This section presents our workflow for developing this gold standard, which consists of an annotated dataset of the types of semantic relations that hold between MWs and TWs on actual Taboo cards.\n",
    "We show how we used this dataset to generate a distribution of the probabilities of each of the five semantic relations we care about replicating: collocations, synonyms, antonyms, hypernyms, and hyponyms (more detail on this below).\n",
    "\n",
    "The reason we wanted a probability distribution is so that we could sample from it for each of the five slots on a card, in order to decide which semantic relation should populate each slot.\n",
    "Then, we can choose some TW that stands in the corresponding semantic relationship to the MW.\n",
    "This results in cards that are true to the semantic relation distribution seen on actual Taboo cards.\n",
    "\n",
    "The 240 Taboo cards that our gold standard is based on belong to Elizabeth's family's Canadian edition of Taboo, produced sometime in the 1990s or early 2000s.\n",
    "These cards were transcribed into the text file `taboo_cards.txt` (found in the current directory).\n",
    "\n",
    "The module `gs_probdist` contains the functions needed for this section.\n",
    "(Each function is outfitted with a docstring detailing the parameters and returned values, in case the reader wants more information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import gs_probdist as gspd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To start, the function `get_card_dicts()` (using helper functions `read_in()` and `format_cards()`) reads in the transcribed Taboo cards and produces a dictionary, each entry of which represents one card. \n",
    "For example, the MW *huddle* is assigned as one of the dictionary's keys, and the corresponding value is a list of *huddle*'s five TWs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gather', 'football', 'group', 'play', 'together']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_dict = gspd.get_card_dicts()\n",
    "card_dict['huddle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "`get_card_dicts()` provides the input for the function `cards_to_df()`, which converts this dictionary to a `pandas` dataframe.\n",
    "In this dataframe, each row is a pairing of the MW with each of its TWs.\n",
    "This format allows for easy annotation of the semantic relationship between each MW/TW pair.\n",
    "The following cell shows the top five rows of the resulting dataframe, corresponding to one card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw\n",
       "0  huddle    gather\n",
       "1  huddle  football\n",
       "2  huddle     group\n",
       "3  huddle      play\n",
       "4  huddle  together"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gspd.cards_to_df(card_dict).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "At this point, we exported this dataframe to a CSV file and manually categorised the following types of semantic relationship between each TW and its MW (annotation guidelines can be found in `gs-annotation-guidelines.txt`):\n",
    "\n",
    "- **collocations** (i.e. combinations of words at rates more frequent than chance; see Evert 2009)\n",
    "- **synonyms** (words meaning the same thing)\n",
    "- **antonyms** (words with opposite meanings)\n",
    "- **hyponyms** (a subset of a word's meaning, i.e. a more specific version)\n",
    "- **hypernyms** (a superset of a word's meaning, i.e. a more general version)\n",
    "\n",
    "We also had a category for cultural references---MWs and TWs relating in a way that requires cultural or world knowledge---and a catch-all \"other\" category.\n",
    "We did not try to replicate these two categories, since our focus was on the linguistic aspect of this project.\n",
    "\n",
    "We chose these five semantic relations since they are fairly easy to operationalise using two tools that we have gotten to know this semester: word2vec word embeddings (for the collocations, since word embeddings represent textual co-occurrence; see Mikolov et al. 2013) and WordNet (for the other four categories; see Fellbaum 2010).\n",
    "\n",
    "The rest of the functions in `gspd` are used to process the annotated gold standard dataset, which is saved as `gold-std-categorised.csv` in the current directory.\n",
    "First, `read_in_categorised()` simply processes the csv file into a `pandas` dataframe with one row per MW/TW pairing, a 1 in the column of the category that that pair belongs to, and zeroes everywhere else, as illustrated in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mw</th>\n",
       "      <th>tw</th>\n",
       "      <th>semrel_synonym</th>\n",
       "      <th>semrel_antonym</th>\n",
       "      <th>semrel_hyponym</th>\n",
       "      <th>semrel_hypernym</th>\n",
       "      <th>collocation</th>\n",
       "      <th>cultural_ref</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>huddle</td>\n",
       "      <td>gather</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>huddle</td>\n",
       "      <td>football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>huddle</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>huddle</td>\n",
       "      <td>play</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>huddle</td>\n",
       "      <td>together</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mw        tw  semrel_synonym  semrel_antonym  semrel_hyponym  \\\n",
       "0  huddle    gather             0.0             0.0             0.0   \n",
       "1  huddle  football             0.0             0.0             0.0   \n",
       "2  huddle     group             0.0             0.0             0.0   \n",
       "3  huddle      play             0.0             0.0             0.0   \n",
       "4  huddle  together             0.0             0.0             0.0   \n",
       "\n",
       "   semrel_hypernym  collocation  cultural_ref  other  \n",
       "0              1.0          0.0           0.0    0.0  \n",
       "1              0.0          1.0           0.0    0.0  \n",
       "2              1.0          0.0           0.0    0.0  \n",
       "3              0.0          1.0           0.0    0.0  \n",
       "4              0.0          1.0           0.0    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldstd_data = gspd.read_in_categorised()\n",
    "goldstd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This dataframe is used as input for the final two functions of `gspd`: `plot_category_freqs()`, to create the bar plot used in our presentation at the beginning of February (this function also exports the plot to a PDF in the current directory), and the crucial `freq_dist_to_prob_dist()`, which converts the relative frequencies of each category into a probability distribution.\n",
    "This probability distribution was then used to randomly supply a category for each of the five TW slots, in proportion to that category's actual frequency in the real Taboo cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gspd.plot_category_freqs(goldstd_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collocation': 0.6634275618374559,\n",
       " 'semrel_synonym': 0.19699646643109542,\n",
       " 'semrel_hypernym': 0.10159010600706714,\n",
       " 'semrel_hyponym': 0.020318021201413426,\n",
       " 'semrel_antonym': 0.0176678445229682}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_prob_dist = gspd.freq_dist_to_prob_dist(goldstd_data)\n",
    "category_prob_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 2: Determining semantic relations for the given word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "At this point, we know the probability of each category being assigned to one TW slot.\n",
    "However, we still need to actually find the words that belong to these categories for any given MW, so that in the final step, these words can be selected and combined into one card.\n",
    "This is where the functions in the module `semrel` come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import semrel as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "One major pillar of this module is `make_semrel_dict()`, which takes in the given MW and uses the NLTK interface with WordNet to populate a dictionary with the MW's synonyms, antonyms, hypernyms, and hyponyms.\n",
    "It uses other functions defined in this module to do this: `get_synonyms()`,  `get_antonyms()`, `get_hypernyms()`, and `get_hyponyms()`.\n",
    "Two other functions were created for this module, as well: `wd_to_synsets()` (used in `get_synonyms()`), and `synset_to_wd()` (used in `get_hypernyms()` and `get_hyponyms()`).\n",
    "\n",
    "Two examples of the results of  `make_semrel_dict()` are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semrel_synonym': set(),\n",
       " 'semrel_antonym': set(),\n",
       " 'semrel_hypernym': {'bovid', 'follower', 'simpleton'},\n",
       " 'semrel_hyponym': {'ewe', 'ram', 'wether'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.make_semrel_dict('sheep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semrel_synonym': {'adept',\n",
       "  'beneficial',\n",
       "  'commodity',\n",
       "  'dear',\n",
       "  'dependable',\n",
       "  'effective',\n",
       "  'estimable',\n",
       "  'full',\n",
       "  'thoroughly',\n",
       "  'well'},\n",
       " 'semrel_antonym': {'bad', 'evil', 'ill'},\n",
       " 'semrel_hypernym': {'advantage', 'artifact', 'morality', 'quality'},\n",
       " 'semrel_hyponym': {'basic',\n",
       "  'beneficence',\n",
       "  'benefit',\n",
       "  'benignity',\n",
       "  'better',\n",
       "  'desirability',\n",
       "  'entrant',\n",
       "  'export',\n",
       "  'fungible',\n",
       "  'future',\n",
       "  'import',\n",
       "  'kindness',\n",
       "  'merchandise',\n",
       "  'middling',\n",
       "  'optimum',\n",
       "  'saintliness',\n",
       "  'salvage',\n",
       "  'shopping',\n",
       "  'summum_bonum',\n",
       "  'virtue',\n",
       "  'wisdom',\n",
       "  'worldly_possession',\n",
       "  'worthiness'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr.make_semrel_dict('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Two remarks about the above:\n",
    "\n",
    "1. WordNet might not contain any words that stand in some semantic relation to the MW (e.g. there are no synonyms or antonyms for *sheep*).\n",
    "This means that, if one of the five TW slots on a card for *sheep* happens to be assigned the category \"synonym\" or \"antonym\", this assignment will be unfulfillable.\n",
    "We will return to how we deal with these cases in a little while.\n",
    "\n",
    "2. The word *good* can be an adjective or a noun (singular of *goods*; uncommon, but it exists).\n",
    "The function `make_semrel_dict()` was designed to capture all parts of speech and all senses of the input word, which is why we have synonyms like *commodity* and also *beneficial*.\n",
    "We made this decision because, in Taboo, you are not told what part of speech your MW belongs to; you can use any of its senses to try to make your team members guess it (and often this is a very good strategy for English Taboo, where many of the words are homonymous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The other pillar of this module is `get_collocations()`.\n",
    "This is a recursive function that uses the word2vec word embeddings to return some number of suitable collocates, which we operationalise as words that word2vec considers to be similar to the main word, due to their frequent use in the same contexts.\n",
    "\n",
    "\"Suitable\" is an important word here, since some post-processing has to be done to the words that are returned by gensim's `most_similar()` function.\n",
    "Words that we do not want on the cards, but that `most_similar()` might unearth, are those that\n",
    "\n",
    "- contain the string of the main word (e.g. inflectional variations of the MW),\n",
    "- have a Levenshtein distance of 3 or fewer to the main word (meaning that they are probably typos),\n",
    "- contain an underscore, which indicates a multi-word unit; these can sometimes get quite strange (e.g. for the MW *kitchen*, one of the results is *virtuoso_violinist_walked* (???)), or\n",
    "- are already contained in the semantic relations dictionary.\n",
    "\n",
    "All of these cases are taken care of by `get_collocations()`, which returns a set of suitable collocates whose cardinality is larger than the passed-in integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anthurium', 'blossom', 'chrysanthemum', 'orchid', 'peony', 'tulip'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No \"forbidden words\" (the set in the second argument is empty)\n",
    "sr.get_collocations('flower', set(), model, num_collocates = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blossom', 'chrysanthemum', 'peony', 'tulip'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forbidding \"orchid\" and \"anthurium\" (e.g. if they were already in the semantic relations dictionary)\n",
    "sr.get_collocations('flower', {'orchid', 'anthurium'}, model, num_collocates = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "*Side note:* The reasoning behind returning (at minimum) some number of suitable collocates is the following.\n",
    "\n",
    "We sample from the probability distribution from above to decide which semantic relation should hold for each of the five slots on a card.\n",
    "We do not know in advance how many collocations we will need for each MW; this depends on the number of other semantic relations that this sampling procedure gives us.\n",
    "We would need zero collocates if all of the sampled semantic relations were synonyms, antonyms, hypernyms, or hyponyms (which is unlikely, but possible), but we would need five if all five of the sampled semantic relations were collocations.\n",
    "\n",
    "Conceivably, `get_collocations()` could be specified to always return five collocates, no matter what, since that is the maximum number that could be needed for each card.\n",
    "However, generating more collocations than necessary is inefficient, since for some MWs, even reaching five collocates that pass all the tests above is a laborious job for `most_similar()`.\n",
    "Therefore, this threshold ensures that the function does as little work as possible.\n",
    "\n",
    "But where does the recursion come in?\n",
    "Imagine that we need five collocations, but `most_similar()` returns only three viable options, because the other seven (it returns the top ten by default) were all unsuitable.\n",
    "Then, we would need to extend the range of most similar words that `most_similar()` returns to beyond the top ten, so that we have more words to choose from.\n",
    "So, `get_collocations()` checks whether the number of suitable collocates provided in the first iteration is more than the passed-in cardinality threshold (which, in the above example, it is not).\n",
    "If there are too few suitable collocates in the first bunch, `get_collocations()` increases the number of most similar words to return and calls itself again, checking to see how many words in this extended range pass the tests.\n",
    "This recursion continues until until the cardinality of the resulting set of collocates surpasses the minimum threshold provided in the `num_collocates` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "So, to recap: at this point, we have functions that make a dictionary that contains all of the synonyms, antonyms, hypernyms, and hyponyms from WordNet, as well as a set of collocated words based on the word2vec embeddings.\n",
    "We also have a probability distribution that tells us how probable it is to find any one of these semantic relations in one TW slot in the final card.\n",
    "Now all that remains is to put this information together and generate the Taboo cards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 3: Putting it all together -- Card generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The module containing the functions for the culmination of the card generator is `cardgen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import cardgen as cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Its workhorse is `card_generator()`, which takes in the desired MW, the dictionary containing the probability distribution from the gold standard, and the gensim model.\n",
    "It uses the function `select_five_categories()` to generate a list of five semantic relations, one for each slot on the card.\n",
    "Then, `get_good_label_distrib()` assesses the cardinality of the sets in the semantic relation dictionaries to see if the MW has enough synonyms, say, to fulfill the number of synonyms that `select_five_categories()` has generated.\n",
    "If not, this label is just replaced by \"collocation\", since this is the most frequent category, and we can generate as many collocations as we need using `sr.get_collocations()`. \n",
    "\n",
    "For example, we saw a case above where *sheep* had no synonyms or antonyms.\n",
    "If the relation \"synonym\" were sampled for *sheep*, because this assignment cannot be fulfilled, `get_good_label_distrib()` would replace \"synonym\" with \"collocation\".\n",
    "(It would have been more sophisticated to resample a category according to the probability distribution instead of just assigning \"collocation\", but this method was much more straightforward, and the word2vec words contain more than just strict collocations in the linguistic sense, anyway.)\n",
    "\n",
    "At this point, we have a list of categories to populate with TWs, and we know that these categories are fulfillable for our given MW.\n",
    "Now we randomly select words with the desired semantic relation from the dictionary and fill in the rest with new collocates (this is why the forbidden-words parameter in `sr.get_collocations()` is important: to block already-selected TWs that we chose from the semantic relations dictionary).\n",
    "The card is returned as a single-entry dictionary, with the MW as a key and the generated list of TWs as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hat': ['wear', 'millinery', 'stetson', 'fedora', 'beanie']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg.card_generator('hat', category_prob_dist, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There is also a function to pretty-print these cards in a more Taboo-like format.\n",
    "Together, `card_generator()` and `pretty_print()` are wrapped up in `draw_card()`, which simulates drawing a Taboo card from the playing deck.\n",
    "\n",
    "One of the fun parts of this card generator is that, because of the randomness in the probability distribution and the fact that there are generally more words to choose from than will fit on a card, the cards generated for a given MW are different almost every time.\n",
    "Let's see an example with the word *hat* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------------\n",
      " |    hat         |\n",
      " ------------------\n",
      " |    function    |\n",
      " |    stetson     |\n",
      " |    fedora      |\n",
      " |    beanie      |\n",
      " |    chapeau     |\n",
      " ------------------\n"
     ]
    }
   ],
   "source": [
    "cg.draw_card('hat', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally, we also built in handling for the case in which the requested main word is not in word2vec's vocabulary; in that case, the following message will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, no card can be generated for this word! Please try another one.\n"
     ]
    }
   ],
   "source": [
    "cg.draw_card('ignominiousness', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## References\n",
    "\n",
    "Evert, Stefan. 2009. Corpora and collocations. In Anke Lüdeling & Merja Kytö (eds.), *Corpus linguistics. An international handbook*, 1211–1248. Berlin: De Gruyter.\n",
    "\n",
    "Fellbaum, Christiane. 2010. WordNet. In *Theory and applications of ontology: Computer applications*, 231–243. Dordrecht: Springer.\n",
    "\n",
    "Mikolov, Tomas, Kai Chen, Greg Corrado & Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. *ICLR 2013.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
