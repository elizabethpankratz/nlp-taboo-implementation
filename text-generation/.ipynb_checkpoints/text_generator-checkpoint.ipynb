{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Remarks\n",
    "* Three steps for cleaning probably provide better word2word meanings but since for us the main goal is to generate a coherent sentence from the input and meaning WE are feeding, then we can't leave out punctuation symbols and stop words. Maybe it is still good to lemmatize? We'll see\n",
    "* Network currently trained with  5000 trigrams, 50 epochs, hidden_s = 100, lr = 0.015. Took 17min to train. Losses went from 7.9 to 2.5\n",
    "* clean corpus 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Generating our descriptive sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Importing everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#is this the best choice of autograd?\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Getting out text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and readisng the corpus\n",
    "#we will be using the small version of the descriptive corpus we made\n",
    "f = open('description-corpus-20k.txt', 'r')\n",
    "#text = f.read()\n",
    "text = f.readlines() #if we want a list with sentences as elements\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#getting the avg length of a descriptive sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Cleaning (NOT USED)\n",
    "* Removing stop words, punctuation symbols and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# found that in some tutorials they do 3 extra cleaning steps before applying N-grams\n",
    "\n",
    "# getting rid of stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_free = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "# justification --> it's like getting rid of the long tail in the word frequency plot. Oder?\n",
    "\n",
    "# getting rid of punctuation\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "#punct_free = \"\".join(word for word in stop_free if word not in punctuation_symbols)\n",
    "# makes sense.. I think\n",
    "\n",
    "# lemmatizing?\n",
    "#lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#normalized = ' '.join(lemma.lemmatize(word) for word in punct_free.split())\n",
    "#if it is what I think it is, then it makes sense too\n",
    "\n",
    "#last step, lower case and splitting\n",
    "#cleaned_text = normalized.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stop_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "punct_free[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "normalized[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Only normalizing (NOT USED)\n",
    "* is it worth it having different tokens for cookie and cookies (in terms of having a correct sentence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Implementing trigrams and setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating trigram sets EASY WAY\n",
    "#trigrams = [([cleaned_text[i], cleaned_text[i+1]], cleaned_text[i+2])for i in range(len(cleaned_text) - 2)]\n",
    "\n",
    "# this structure allows us to create context/target sets for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# better way: sentence by sentence, not the whole text in one go\n",
    "trigrams = []\n",
    "for sentence in sentences:\n",
    "    trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'cookie'], 'is')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[0]\n",
    "#context for target word 'text' --> 'a' and 'cookie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#to prototype we will only take 5000 trigrams out of the ~500000\n",
    "trigrams=trigrams[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting set of words in vocab, it's length and the frequency of each word\n",
    "voc = set()\n",
    "for sentence in sentences:\n",
    "    voc = voc.union(set(sentence))\n",
    "voc_length = len(voc) #34174\n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to inp\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bad, training on CPU; dont exagerate with number of epochs.\n"
     ]
    }
   ],
   "source": [
    "#Cheking if we have access to training on GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Fancy setup!')\n",
    "else: \n",
    "    print('Too bad, training on CPU; dont exagerate with number of epochs.')\n",
    "\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def train(context, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(trigrams)):\n",
    "        output, hidden = decoder(context[t], hidden)\n",
    "        loss += criterion(output, target[t])\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data.item() / len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 48s (10 16%) 5.4674]\n",
      "[1m 37s (20 33%) 5.3654]\n",
      "[2m 24s (30 50%) 5.2835]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "print_every = 10\n",
    "plot_every = 10\n",
    "hidden_s = 50\n",
    "n_layers = 3\n",
    "lr = 0.015\n",
    "\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(cont,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a3a3ad290>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in prime_str.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main year a in criminal a individual found to edition a\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('the main', 10, temperature = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Generating descriptive sentence\n",
    "* input = main word + taboo words\n",
    "\n",
    "The main idea here is that we will seed always with the main word and replace at the end.\n",
    "The descriptive sentence will be accepted into the cleaning step (where we will make sure that no tws or the mw were used and if so, we'll replace them) if it contains at least two of the input words, besides the seeds. As long as the sentence is not accepted, we will keep generating.\n",
    "\n",
    "To help reach a high score: once an input word has been used and its score_vector value increases, we will leave that segment in our sentence. Not sure if this hurts more than it helps, but adding the input word as a seed is VERY complicated. And not sure it would make sense either.\n",
    "\n",
    "For a first prototype we will assume that maximum one new word is gonna be added per iteration. \n",
    "\n",
    "Pretty fragile with many assumptions?\n",
    "\n",
    "It would be useful to generate a list of synonyms of all input words to expand the input_words set and have higher acceptance rate. We thought about cleaning the input words set from the start but those are words with high probability of occurence so we better leave them and clean in the end.\n",
    "\n",
    "* maybe more seeds?\n",
    "* maybe require a higher score?\n",
    "* how to properly connect end of sentences and seeds? Is it better to have fewer seeds but longer auto-generated text?\n",
    "* add time it to report results and \"quantify\" results/improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 3 seeds (means, is, refers to). i=10\n",
    "Final score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# set of words that we hope will appear in the description\n",
    "input_words = np.array(['happy', 'pleased', 'thrilled', 'excited', 'ecstatic', 'overjoyed', 'joy'])\n",
    "#extend!\n",
    "#probably good idea to check if those words are even in our vocab. jeje\n",
    "# filtering out the ones that are not. Shouldn't be a thing when using larger corpus\n",
    "input_words = [word for word in input_words if word in voc]\n",
    "\n",
    "#create the first sentence\n",
    "#on average a descriptive sentence had 27 words/symbols.\n",
    "# we will equally divide them between our seeds\n",
    "\n",
    "sentence_parts = np.array([evaluate('happy means', 7, temperature = 1), evaluate('happy is', 7, temperature = 1), evaluate('happy refers to', 6, temperature = 1)])\n",
    "\n",
    "sentence =  \" \".join(sentence_parts)\n",
    "\n",
    "eval_sentence = sentence.split()\n",
    "# first score vector and score\n",
    "score_vector = np.array([eval_sentence.count(input_words[x]) for x in range(len(input_words))])\n",
    "score_vector[0] -= 3\n",
    "score = np.sum(score_vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy means out cold be found of realizing emergency happy is the diabetes the can three the contributing happy refers to the before diameter your peice side'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# iterate until nice sentence comes up\n",
    "# we will add safety measure to not break everything\n",
    "i = 0\n",
    "n = 10\n",
    "index_in_sentence = -1\n",
    "# to keep track of scores\n",
    "scores = np.zeros(n)\n",
    "# the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "covered = np.array([0,0,0])\n",
    "changes = np.zeros(len(score_vector))\n",
    "\n",
    "#known positions of input words in our sentence\n",
    "positions = np.zeros(len(eval_sentence))\n",
    "#we know the positions of the seeds\n",
    "positions[0] = 1\n",
    "positions[9] = 1\n",
    "positions[18] = 1\n",
    "\n",
    "while i < n:\n",
    "    #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "    \n",
    "    #checking if score improved\n",
    "    new_score_vector = np.array([eval_sentence.count(input_words[x]) for x in range(len(input_words))])\n",
    "    new_score_vector[0] -= 3\n",
    "    changes = new_score_vector - score_vector\n",
    "    \n",
    "    if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "        index = np.where(changes != 0)[0][0] #looking for the word that was added\n",
    "        word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "        #finding in which segment that new added word is in order to leave the segment untouched\n",
    "        \n",
    "        #how to detect the index of the word that just came up if it was already in the sentence somewhere else?\n",
    "        #this should do\n",
    "        indices_in_sentence = np.where(eval_sentence == word_that_was_added)[0]\n",
    "        if len(indices_in_sentence) >1: #word appears at least twice\n",
    "            for d in indices_in_sentence:\n",
    "                if positions[d] != 1:\n",
    "                    index_in_sentence = d\n",
    "                    positions[d] = 1\n",
    "        else:\n",
    "            index_in_sentence = indices_in_sentence[0]\n",
    "            positions[index_in_sentence] = 1\n",
    "        #keeping the segment in which the improvement took place\n",
    "        if index_in_sentence in range(9) & covered[0]!=1:\n",
    "            sentence_parts[1] = evaluate('happy is', 7, temperature = 1)\n",
    "            sentence_parts[2] = evaluate('happy refers to', 6, temperature = 1)\n",
    "            sentence = ' '.join(sentence_parts)\n",
    "            covered[0] = 1\n",
    "        elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "            sentence_parts[0] = evaluate('happy means', 7, temperature = 1)\n",
    "            sentence_parts[2] = evaluate('happy refers to', 6, temperature = 1)\n",
    "            sentence = ' '.join(sentence_parts)\n",
    "            covered[1] = 1\n",
    "        elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "            sentence_parts[1] = evaluate('happy is', 7, temperature = 1)\n",
    "            sentence_parts[0] = evaluate('happy means', 7, temperature = 1)\n",
    "            sentence = ' '.join(sentence_parts)\n",
    "            covered[2] = 1\n",
    "        eval_sentence = sentence.split()\n",
    "        changes = np.zeros(len(score_vector))\n",
    "        index_in_sentence = 0\n",
    "        score_vector = new_score_vector\n",
    "        score = np.sum(score_vector)\n",
    "    \n",
    "    #if there was no change\n",
    "    else: #based on what is already covered\n",
    "        for r in range(len(covered)):\n",
    "            if covered[r] !=1 & r==0:\n",
    "                sentence_parts[r] = evaluate('happy means', 7, temperature = 1) +' '\n",
    "            if covered[r] !=1 & r==1:\n",
    "                sentence_parts[r] = evaluate('happy is', 7, temperature = 1) +' '\n",
    "            if covered[r] !=1 & r==2:\n",
    "                sentence_parts[r] = evaluate('happy refers to', 6, temperature = 1)\n",
    "        sentence = ' '.join(sentence_parts)\n",
    "        eval_sentence = sentence.split()\n",
    "        score_vector = new_score_vector\n",
    "        score = np.sum(score_vector)\n",
    "        \n",
    "            \n",
    "    \n",
    "    scores[i] = score\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -3., -3., -3., -3., -3., -3., -3., -3., -3.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
