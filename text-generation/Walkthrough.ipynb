{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Descriptive sentence generator walkthrough\n",
    "In this notebook we intend to walk the reader through how we developed the second part of our taboo implementation project; the taboo player. The ultimate goal of the player is to receive as input one taboo card with the main word and its taboo words, and output a sentence describing the main word without using the taboo words. \n",
    "\n",
    "Here will be presented the different steps we took in order to tackle the challenge. \n",
    "\n",
    "Simply put(?), our strategy is the following:\n",
    "\n",
    "* Train the GRU-based neural network with trigrams from our corpus. This step was inspired on several implementations we found online attempting to achieve similar text generation task. Those papers and blog posts are referenced in the project reports.\n",
    "* Use the main word as grounding point or seed for the final sentence. Using multiple seeds should allow us to generate sentences related to the main word without deviating too much as the distance from the seed increases. Our whole sentence will be the concatenation of as many smaller generated sentences as we have seeds.\n",
    "* Start an iterative process in which each segment of the sentence keeps generating until we detect some meaningful words in it.\n",
    "* Clean the final sentence to avoid using the main word and the taboo words.\n",
    "\n",
    "For a more detailed description see the project reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 0: Importing all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable \n",
    "import math\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Setting up the gensim model used for card generation and semantic relations mining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 1: Creating a useful corpus and loading it\n",
    "Since we wanted to use this project as a chance to gain experience implementing neural network learning methods, and more specifically as a first approach to text generation, it was essential to get our hands on a fitting corpus that would result in meaningful inference after using it for training. \n",
    "\n",
    "For this purpose we built a corpus using web corpora consisting of ~115k sentences with the desired descriptive structures. (For more detail on how the corpus was created, see /text-generation/description-corpus/create_corpus_from_csvs.ipynb).\n",
    "\n",
    "We prototype using the smaller version of the corpus (~20k sentences), but any final training and text generation was implemented with the complete version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "f = open('description-corpus-115k.txt', 'r')\n",
    "text = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# getting lower case and splitting each sentence\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]\n",
    "\n",
    "#getting the average length of a descriptive sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We will aim to generate sentences of the same length as the average sentence length in our corpus, namely 27 words or symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 1b: Cleaning the corpus\n",
    "Before dividing our sentences into trigram relations we experimented with some basic cleaning and normalizing techniques:\n",
    "* Removing stop words\n",
    "* Removing punctuation symbols\n",
    "* Lemmatizing\n",
    "\n",
    "Below we present the code for each one of these steps. We decided NOT to implement any of them in the final version of our code since for the strategy we have in mind the goal of the GRU-based text generator is to produce grammatically correct sentences, for which we need both \"stop\" words and punctuation. Once we have such a sentence, we will implement an iterative strategy to introduce the semantic/definitive meaning into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting rid of stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "#stop_free = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# getting rid of punctuation\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "#punct_free = \"\".join(word for word in stop_free if word not in punctuation_symbols)\n",
    "\n",
    "# lemmatizing\n",
    "#lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "#normalized = ' '.join(lemma.lemmatize(word) for word in punct_free.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Modifying the corpus to account for Zipf's law\n",
    "While examining our corpus we also found some weirdly tokenized words, \"binoculars\" was imported as \"bi -no -cu -lars\" per example. That is why from the 3rd model on we decided to work with a version of the corpus in which any token appearing only once was replaced by the \"UNKNOWN\" token. Since searching and replacing items in such a big string was very inefficient, we ended up adding the filtering into the trigram creation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "f = open('description-corpus-115k.txt', 'r')\n",
    "whole_text = f.read()\n",
    "f.close()\n",
    "\n",
    "token_freq = nltk.FreqDist(whole_text.lower().split())\n",
    "len(token_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Before applying the cleaning step, our total vocab consisted of ~88000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "percent = [5, 10, 20, 30, 40, 50, 60, 70, 75, 80, 85, 90, 92, 94, 96, 97, 98]\n",
    "np.percentile(list(token_freq.values()), percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Above we can clearly see Zipf's law in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "to_delete = [k for k,v in token_freq.items() if float(v) == 1]\n",
    "for k in to_delete:\n",
    "    del token_freq[k]\n",
    "len(token_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "By applying this filter we got rid of roughly half of our tokens. We will use this list of hapax legomena to filter our trigrams in the next step. \n",
    "\n",
    "The code below shows a very inefficient way of replacing those tokens in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# spl = text.split()\n",
    "# for i in range(len(spl)):\n",
    "#     if spl[i] in to_delete:\n",
    "#         spl[i] = 'UNKNOWN'\n",
    "# text = ' '.join(spl)\n",
    "# f = open('description-corpus-115k_with_replacements.txt', 'w+')\n",
    "# f.write(text)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 2: Implementing trigrams and setting up variables that will go into the network\n",
    "Here we create the context:target triplets that will be fed into the neural network.\n",
    "\n",
    "We tried limiting here the creation of trigrams by adding only the ones NOT containing any of the to_delete words, but it also proved to too inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#in case we are loading a particular set of trigrams\n",
    "#if we do this we can then skip to the cell defining the vocabulary. It starts with voc = set()\n",
    "with open(\"trigrams_test6.txt\", \"rb\") as fp:\n",
    "    trigrams = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "temp_trigrams = []\n",
    "for sentence in sentences:\n",
    "    temp_trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)] #if (sentence[i] not in to_delete and sentence[i+1] not in to_delete) and sentence[i+2] not in to_delete]\n",
    "len(temp_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Each trigram will have this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "temp_trigrams[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Any training sessions we tried to implement using all trigrams without filtering resulted in kernel death. Our guess is that including all ~3,000,000 was too much. That is why for the 1st and 2nd models we decided to draw 50000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "random.seed(163)\n",
    "temp_trigrams = random.sample(temp_trigrams, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As a consequence, our vocabulary length for the 1st and 2nd models dropped from 88331 tokens to about ~16500. But we have to keep in mind that more than half of those ~88000 tokens only appeared once in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that we sampled the desired number of trigrams, we will filter them as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "trigrams = []\n",
    "#we will only accept those consisting only of tokens that appear at least twice\n",
    "for tri in temp_trigrams:\n",
    "    if tri[1] not in to_delete:\n",
    "        if tri[0][0] not in to_delete:\n",
    "            if tri[0][1] not in to_delete:\n",
    "                trigrams.append(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Note that we only got rid of around ~2000 trigrams. \n",
    "\n",
    "Because even with this solution generating the trigrams takes a while, we will use pickle to save the object as a .txt file. This will allow us to reproduce results faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#saving the current set\n",
    "with open(\"trigrams_test6.txt\", \"wb\") as fp:\n",
    "    pickle.dump(trigrams, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the next cell we create a set containing all tokens found in our trigrams, retrieve its length and create a token:frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "voc = set()\n",
    "for tri in trigrams:\n",
    "    voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "    \n",
    "voc_length = len(voc)\n",
    "\n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The last preparation step is creating the context and target tensors containing for each trigram its frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to cont\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Exploring the relation between number of trigrams and length of our vocabulary\n",
    "From the first two models it was clear that while including more trigrams leads to better text generation, it also meant having a larger vocabulary size, which in turn affects directly the number of nodes in our network's input and output layers. That is why in the cell below we explore the relation between both variables and used that plot to decide on a good enough trade off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "random.seed(163)\n",
    "size_trigrams = np.arange(10000, 100000, 10000)\n",
    "size_vocab = np.zeros(len(size_trigrams))\n",
    "\n",
    "for i in range(len(size_trigrams)):\n",
    "    sampled_trigrams_temp = random.sample(temp_trigrams, size_trigrams[i])\n",
    "    trigrams = []\n",
    "    for tri in sampled_trigrams_temp:\n",
    "        if tri[1] not in to_delete:\n",
    "            if tri[0][0] not in to_delete:\n",
    "                if tri[0][1] not in to_delete:\n",
    "                    trigrams.append(tri)\n",
    "    voc = set()\n",
    "    for tri in trigrams:\n",
    "        voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "    size_vocab[i] = len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.plot(size_trigrams, size_vocab, size_trigrams, np.repeat(20500, len(size_trigrams)))\n",
    "plt.xlabel('Number of sampled and filtered trigrams')\n",
    "plt.ylabel('Vocabulary size')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Because it seems that the marginal training cost (reflected by the vocabulary size) of including more trigrams is decreasing, we decided to include ~90,000 trigrams in our 3rd model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 3: Building the network and training it\n",
    "As mentioned above, this section was inspired by the pytorch tutorial we had during the class period and by several papers and blog-posts. They are all referenced in the project reports and at the end of both the demo and the walkthrough.\n",
    "\n",
    "As always, the first step is to check if we have a GPU to train our model on. It was not the case for any of our trials, but we will include the code for future reference and implementations. Note that since we knew that we would not have access to GPUs, our code does not include several .cuda() sections that would be necessary to run it in a GPU setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too bad, training on CPU. Keep the number of epochs low!\n"
     ]
    }
   ],
   "source": [
    "#Cheking if we have a GPU to train our model on\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Fancy setup!')\n",
    "else: \n",
    "    print('Too bad, training on CPU. Keep the number of epochs low!')\n",
    "\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Then we defined our torch.nn class and its usual methods. We decided to implement a GRU-based network over an LSTM. This choice is justified in the project report.\n",
    "\n",
    "Because of both time and computational power limitations, many of the features we decided to implement were the common choice among our sources: the decoder linear type, Variable as the Autograd method, the step optimizer using Adam and cross entropy loss as criterion. \n",
    "\n",
    "The network parameters we did experiment with were the sizes of the input, output and hidden layers and also the number of hidden layers. Although many times the rule of thumb is to start with hidden layers containing a number of nodes of the same order of magnitude as the input or output layers, we estimated that doing so was going to result in training times longer than reasonable for the scope of the project.\n",
    "\n",
    "The main objective of this text generator is to output coherent and grammatically correct sentences. Of course, generating sequences of tokens related to a certain seed is also a second objective, but in our approach we brute-forced this into the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s #length of our vocab\n",
    "        self.hidden_s = hidden_s #to experiment with\n",
    "        self.output_s = output_s #length of our vocab\n",
    "        self.n_layers = n_layers #to experiment with\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))\n",
    "    \n",
    "def train(context, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(trigrams)):\n",
    "        output, hidden = decoder(context[t], hidden)\n",
    "        loss += criterion(output, target[t])\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data.item() / len(trigrams)\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m*60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next comes the training step. This set up corresponds to the 3rd model we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "print_every = 5\n",
    "plot_every = 10\n",
    "hidden_s = 50\n",
    "n_layers = 3\n",
    "lr = 0.015\n",
    "\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(cont,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#saving the model's state_dict\n",
    "path = os.getcwd()+'/test6_trained_inference.pt'\n",
    "\n",
    "torch.save(decoder.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (encoder): Embedding(18665, 50)\n",
       "  (gru): GRU(100, 50, num_layers=3, batch_first=True)\n",
       "  (decoder): Linear(in_features=50, out_features=18665, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXAMPLE: to load\n",
    "path = os.getcwd()+'/test6_trained_inference.pt'\n",
    "hidden_s = 50\n",
    "n_layers = 3\n",
    "lr = 0.015\n",
    "decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "decoder.load_state_dict(torch.load(path))\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Section 4: Generating our taboo player's descriptive sentence\n",
    "Now that we trained our decoder, we are able to generate sentences iteratively token by token. In order to do this we will generate a distribution of possible next tokens based on a seed and choose the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def next_token_generator(seed, generation_length=100):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(generation_length):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in seed.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).exp()\n",
    "        top_choice = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_choice)]\n",
    "        seed += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With a working sentence generator that in theory is able to produce coherent sequences, the next challenge was to use it in a way that would return a sequence with enough meaningful words related to our main word. \n",
    "\n",
    "First we generated a set of input words providing useful information about our main word. This set includes the main word, all its hypernyms, hyponyms and synonyms and all taboo words. We are aiming to have a certain number of these words in our final sentence.\n",
    "\n",
    "Below is the code to generate these input_words sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def gen_input_words(mw, model):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    \n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but they loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab.\n",
    "    input_words = [word for word in input_words if word in voc]    \n",
    "    return input_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After a quick analysis of our descriptive corpus we found that the most frequent descriptive seeds are \"X is\", \"X means\" and \"X can be found\". It is worth noting that the \"is\" seed appeared ~1000 times more often than \"means\" and \"means\" about ~100 times more often than \"can be found\". \n",
    "\n",
    "The next step is the iterative process by which we expect to have a descriptive sequence as an output. Our approach was to not limit ourselves to using only one seed but to try using several. Even though it was clear from the beginning that simply concatenating generated sub_segments would not result in a coherent sentence, having many sub-segments with their own seeds trying to generate related words in parallel increases our chances of finding input_words inside the final sequence.\n",
    "\n",
    "We mentioned already that our seed will always consist of our main word + \"is\" or \"means\" or \"can be found\". We limited the experiment to 2 or 3 seeds.\n",
    "\n",
    "After each iteration we will assign a success score to the concatenated final sequence. The score is an integer counting how many of the input words we have in the sequence, without taking into account the initial seeds. After some experimenting with the game of taboo we agreed that a score of 2 was high enough to be considered a success. \n",
    "\n",
    "In case one of the segments generates a sub_sequence containing one of the input_words, we block that segment. This means it will stop iterating and will stay constant throughout the rest of the process. All other sub_segments will keep generating until the desired score is reached. If sub_segment X already includes an input word and is already blocked, the next sub_segment (X+1) could take X into account as part of its seed. This should help by adding better context into the generating step. But since only take into account the last two segments of each seed in next_token_generator, this is not a useful strategy right now (although the implementation below is already adapted to take advantage of this). \n",
    "\n",
    "For practical purposes we stop generating after some fixed number of iterations in case the score was not reached.\n",
    "\n",
    "The code below describes this iterative process for the cases in which we use 2 or 3 seeds. I suggest reading first the part of the code describing the process for only 2 seeds, as the version for 3 seeds works exactly the same but is messier because of we had to cover more cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    #n_seeds = if we are using 2 or 3 seeds during the sentence generation step\n",
    "    #n_iterations = how many iterations we will do in the generation step\n",
    "    #debugging = True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "    #printing = True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    \n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence, dividing the whole sequence into equally long sub_sequences\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 7), next_token_generator(mw+' is', 7), next_token_generator(mw+' can be found', 5)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that already contains an input word.\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence to know where input words are located and to which sub_sequence they belong.\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        #for practical purposes we stop generating after some fixed number of iterations in case the score was not reached.\n",
    "        while i < n_iterations and score <2 :\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change in the score. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index]\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                    \n",
    "                #keeping the segment in which the improvement took place, blocking it and continue the generating process\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 7) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) +' '\n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    else:\n",
    "                        temp =  next_token_generator(sentence_parts[1]+' '+ mw+' can be found', 5) +' '\n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            #choosing what to print\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 11), next_token_generator(mw+' is', 12)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 12)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 12) +' '\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) +' '\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(changes)\n",
    "                    print(covered)\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that we have a sequence containing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def sentence_cleaner(sentence, mw, model):\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, 'The main word')\n",
    "    \n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def final_output(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    sentence = description_generator(mw, model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, model)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number: 1\n",
      "cake means yall blog line quickest tumbler sod al-jabr much memorandum couples layouts  cake is none sow bach intrigue skier workmanship cre \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means typos cannot interviewer saved beta massive lean requirements requirements labeled mean  cake is multi-user methodically manage mean reform lean 13 \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means retrieving lean notably dining pharmacist parenthood taxpayer lean libertarian requirements cake is holocaust demons neurotic parenthood scoby multi-user appointments \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means knocking roster propagate jamaica photographer skier monitor detrás labeled 31 memorandum  cake is gazebo cycle koreans sacrificing requirements welsh advised \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means ions manage septenary beta pedestrian arise reform lean manage cascading brig  cake is deadlock worms assertion terrorism reform comprise c.w. \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main word means ions manage septenary beta pedestrian arise reform lean manage cascading brig  The main word is deadlock worms assertion terrorism reform comprise c.w. '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output(mw = 'cake', model = model, n_seeds=2, n_iterations = 5, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
