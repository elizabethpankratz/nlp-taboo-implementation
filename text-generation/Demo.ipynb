{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Demo version of the descriptive sentence generator for the taboo implementation project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gs_probdist as gspd\n",
    "import semrel as sr\n",
    "import gensim\n",
    "import cardgen as cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading gensim model to use the card generator and the semantic relations finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "card_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Reading and structuring corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#opening and reading the corpus\n",
    "#we will be using the full version of the descriptive corpus we made ~115k sentences\n",
    "f = open('description-corpus-115k.txt', 'r', encoding='utf-8')\n",
    "text = f.readlines() # List with sentences as elements\n",
    "f.close()\n",
    "\n",
    "# getting lower case and splitting it\n",
    "sentences = [text[i].lower().split() for i in range(len(text))]\n",
    "\n",
    "#getting the avg length of a sentence\n",
    "lengths = [len(sent) for sent in sentences]\n",
    "avg_sent_length = sum(lengths)/len(lengths) # ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Setting up trigrams, context and target tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Sentence by sentence\n",
    "# this structure allows us to create context/target sets for each word. \n",
    "trigrams = []\n",
    "for sentence in sentences:\n",
    "    trigrams += [([sentence[i], sentence[i+1]], sentence[i+2]) for i in range(len(sentence) - 2)]\n",
    "\n",
    "\n",
    "#using all trigrams led to kernel death every time\n",
    "# we will randomly sample 50000 of them\n",
    "random.seed(163)\n",
    "trigrams = random.sample(trigrams, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# getting set of words in vocab, it's length and the frequency of each word\n",
    "# our vocab consists of the words appearing in trigrams, so no need to take the vocab over the whole text if we are not using all trigrams.\n",
    "voc = set()\n",
    "for tri in trigrams:\n",
    "    voc = voc.union(set(np.union1d(np.array(tri[0]), np.asarray(tri[1]))))\n",
    "voc_length = len(voc) \n",
    "word_to_freq = {word: i for i, word in enumerate(voc)}\n",
    "\n",
    "#creating lists where we will store the input tensors\n",
    "cont = []\n",
    "tar = []\n",
    "for context, target in trigrams:\n",
    "    #creates a tensor with the frequency of both current context words\n",
    "    context_freqs = torch.tensor([word_to_freq[word] for word in context], dtype = torch.long)\n",
    "    #adds the tensor to inp\n",
    "    cont.append(context_freqs)\n",
    "    # does the same for the target and its frequency\n",
    "    target_freq = torch.tensor([word_to_freq[target]], dtype = torch.long)\n",
    "    tar.append(target_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Defining GRU class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    #init for input size, hidden size, output size and number of hidden layers.\n",
    "    def __init__(self, input_s, hidden_s, output_s,n_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_s = input_s\n",
    "        self.hidden_s = hidden_s\n",
    "        self.output_s = output_s\n",
    "        self.n_layers = n_layers\n",
    "        # our encoder will be nn.Embedding\n",
    "        # reminder: the encoder takes the input and outputs a feature tensor holding the information representing the input.\n",
    "        self.encoder = nn.Embedding(input_s, hidden_s)\n",
    "        #defining the GRU cell, still have to determine which parameters work best\n",
    "        self.gru = nn.GRU(2*hidden_s, hidden_s, n_layers, batch_first=True, bidirectional=False)\n",
    "        # defining linear decoder\n",
    "        self.decoder = nn.Linear(hidden_s, output_s)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #making sure that the input is a row vector\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1,-1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading trained model\n",
    "Choosing which trained model to load. They were both trained on CPU over 100 epochs using 50000 trigrams sampled randomly. \n",
    "* 1:\n",
    "    * GRU model with 1 hidden layer consisting of 150 nodes. Note that since this model was trained before adding the random seed to the trigram sampling step, it is necessary to load its corresponding set of trigrams.\n",
    "* 2: \n",
    "    * GRU model with 2 hidden layers consisting of 75 nodes each. Unfortunately we didn't include a random seed for this trial either, and we did not save the corresponding set of trigrams. Although the generation step might work, it is not advised to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Liz (Casual)\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "def model_selection(x):\n",
    "    if x ==1:\n",
    "        path = os.getcwd()+'/test5_trained_inference.pt'\n",
    "        hidden_s = 150\n",
    "        n_layers = 1\n",
    "        lr = 0.015\n",
    "    if x==2:\n",
    "        path = os.getcwd()+'/test4_trained_inference.pt'\n",
    "        hidden_s = 75\n",
    "        n_layers = 2\n",
    "        lr = 0.015\n",
    "    decoder = GRU(voc_length, hidden_s, voc_length, n_layers)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    decoder = torch.load(path)\n",
    "    decoder.eval()\n",
    "    return decoder\n",
    "\n",
    "decoder = model_selection(1)\n",
    "with open(\"trigrams_test5.txt\", \"rb\") as fp:\n",
    "    trigrams = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading description generation scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def next_token_generator(seed, generation_length=100):\n",
    "    hidden = decoder.init_hidden()\n",
    "\n",
    "    for p in range(generation_length):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_freq[w] for w in seed.split()], dtype=torch.long)\n",
    "        cont = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(cont, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).exp()\n",
    "        top_choice = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_freq.keys())[list(word_to_freq.values()).index(top_choice)]\n",
    "        seed += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return seed\n",
    "\n",
    "def gen_input_words(mw, model):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "\n",
    "    #generating the corresponding taboo card\n",
    "    card_words = cg.card_generator(mw, cg.get_gold_probdist(), model)\n",
    "    #set of words that we hope will appear in the description\n",
    "    input_words = card_words[mw] + [mw]\n",
    "\n",
    "    # extending the input_words set using semantic relations. Bigger set --> better chances of generating an approved word!\n",
    "    # we will use the make_semrel_dict function to get synonyms, hyponyms and hypernyms of the MW.\n",
    "    # we considered adding also semrel words from the tw, but the loose connection to the MW very fast\n",
    "    # we will leave out antonyms as they might make they are \"riskier\" to use in a description.\n",
    "\n",
    "    adds = []\n",
    "    temp = sr.make_semrel_dict(mw)\n",
    "    for k in temp.keys():\n",
    "        if k != 'semrel_antonym':\n",
    "            new = list(temp[k])\n",
    "            adds += new\n",
    "    adds = np.unique(adds)\n",
    "    adds = [x.lower() for x in adds]\n",
    "    input_words = np.unique(input_words + adds)\n",
    "\n",
    "    # filtering out the input words that are not in our vocab. Shouldn't be a thing when using larger corpus\n",
    "    input_words = [word for word in input_words if word in voc]\n",
    "    return input_words\n",
    "\n",
    "def description_generator(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    #mw = main word\n",
    "    #model = embeddings used to generate the cards\n",
    "    #n_seeds = if we are using 2 or 3 seeds during the sentence generation step\n",
    "    #n_iterations = how many iterations we will do in the generation step\n",
    "    #debugging = True if we want to print some statistics about the process. False if we only want the last 5 generated sentences.\n",
    "    #printing = True will print something, based on debugging. If false, it will only return the final sentence\n",
    "    \n",
    "    #generating the input_words we are aiming to include in our description\n",
    "    input_words = gen_input_words(mw, model)    \n",
    "    #on average a descriptive sentence had 27 words/symbols.\n",
    "    # we will equally divide them between our seeds\n",
    "    \n",
    "    \n",
    "    # iterate until nice sentence comes up\n",
    "    # we will add safety measure to not break everything\n",
    "    i = 0\n",
    "    index_in_sentence = -1\n",
    "    \n",
    "    \n",
    "    #if we are using 3 seeds\n",
    "    #the 3 most frequent ones in our corpus were \"x is\", 'x means' and \"x can be found\"\n",
    "    if n_seeds == 3:\n",
    "        #create the first sentence, dividing the whole sequence into equally long sub_sequences\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 7), next_token_generator(mw+' is', 7), next_token_generator(mw+' can be found', 5)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 3 \n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that already contains an input word.\n",
    "        covered = np.array([0,0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence to know where input words are located and to which sub_sequence they belong.\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[9] = 1\n",
    "        positions[18] = 1\n",
    "        \n",
    "        #for practical purposes we stop generating after some fixed number of iterations in case the score was not reached.\n",
    "        while i < n_iterations and score <2 :\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 3 \n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change in the score. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index]\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                    \n",
    "                #keeping the segment in which the improvement took place, blocking it and continue the generating process\n",
    "                if index_in_sentence in range(9) & covered[0]!=1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(9, 18) & covered[1] !=1:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                elif index_in_sentence in range(18, 27) & covered[2] != 1:\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[2] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 7) +' '\n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 7) \n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[9:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7) \n",
    "                # same logic for the third part.\n",
    "                if covered[2] == 0:\n",
    "                    if covered[1] == 0:\n",
    "                        sentence_parts[2] = next_token_generator(mw+' can be found', 5)\n",
    "                    else:\n",
    "                        temp =  next_token_generator(sentence_parts[1]+' '+ mw+' can be found', 5) \n",
    "                        #taking off the second part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[2] = \" \".join(temp[9:])\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            #choosing what to print\n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "            \n",
    "    #if we are using 2 seeds\n",
    "    #the 2 most frequent ones in our corpus were \"x is\" and 'x means'\n",
    "    if n_seeds == 2:\n",
    "        #create the first sentence\n",
    "        sentence_parts = np.array([next_token_generator(mw+' means', 11), next_token_generator(mw+' is', 12)])\n",
    "        sentence =  \" \".join(sentence_parts)\n",
    "        eval_sentence = sentence.split()   \n",
    "    \n",
    "        # to keep track of scores\n",
    "        scores = np.zeros(n_iterations)\n",
    "        #first score vector and score\n",
    "        #and accounting for the 3 times the MW appears already in the seeds\n",
    "        score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "        score_vector[input_words.index(mw)] -= 2\n",
    "        score = np.sum(score_vector)  \n",
    "\n",
    "        # the covered vector will take care that we don't replace a segment that we already \"like\"\n",
    "        covered = np.array([0,0])\n",
    "        changes = np.zeros(len(score_vector))\n",
    "\n",
    "        #known positions of input words in our sentence\n",
    "        positions = np.zeros(len(eval_sentence))\n",
    "\n",
    "        #we know the positions of the seeds\n",
    "        positions[0] = 1\n",
    "        positions[14] = 1\n",
    "        \n",
    "        while i < n_iterations and score <2:\n",
    "            #aware that with this flow we are doing one iteration after reaching the desired score, but it's no big deal because score is designed to only go up.\n",
    "\n",
    "            #checking if score improved\n",
    "            new_score_vector = np.array([eval_sentence.count(word) for word in input_words])\n",
    "            new_score_vector[input_words.index(mw)] -= 2\n",
    "            changes = new_score_vector - score_vector\n",
    "\n",
    "            if True in (changes>0): #there was a change. Assuming there is max 1 change per iteration from now on\n",
    "                index = np.where(changes == 1)[0][0] #looking for the position in which an input_word was added\n",
    "                word_that_was_added = input_words[index] #if we stop assuming that, here we have to keep track of location and magnitude of changes\n",
    "                \n",
    "                #finding in which segment that new added word is in order to leave the segment untouched\n",
    "\n",
    "                #this detects the index of the word that just came up in case that word was already in our sentence\n",
    "                indices_in_sentence = np.where(np.array(eval_sentence) == word_that_was_added)[0]\n",
    "                if len(indices_in_sentence) >1: #word appears at least twice\n",
    "                    for d in indices_in_sentence:\n",
    "                        if positions[d] != 1:\n",
    "                            index_in_sentence = d\n",
    "                            positions[d] = 1\n",
    "                else:\n",
    "                    index_in_sentence = indices_in_sentence[0]\n",
    "                    positions[index_in_sentence] = 1\n",
    "                #keeping the segment in which the improvement took place\n",
    "                if index_in_sentence in range(14):\n",
    "                    sentence_parts[1] = next_token_generator(mw+' is', 12)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[0] = 1\n",
    "                elif index_in_sentence in range(14, 27):\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11)\n",
    "                    sentence = ' '.join(sentence_parts)\n",
    "                    covered[1] = 1\n",
    "                eval_sentence = sentence.split()\n",
    "                changes = np.zeros(len(score_vector))\n",
    "                index_in_sentence = 0\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "\n",
    "            #if there was no change\n",
    "            else: #based on what is already covered\n",
    "                if covered[0] ==0:\n",
    "                    sentence_parts[0] = next_token_generator(mw+' means', 11) \n",
    "                #if the first part is already covered we can add it as input to generate the second\n",
    "                if covered[1] ==0:\n",
    "                    if covered[0]==1:\n",
    "                        temp =  next_token_generator(sentence_parts[0]+' '+ mw+' is', 12)\n",
    "                        #taking off the first part from it\n",
    "                        temp = temp.split()\n",
    "                        sentence_parts[1] = \" \".join(temp[12:])   \n",
    "                    else:\n",
    "                        sentence_parts[1] = next_token_generator(mw+' is', 7)\n",
    "                sentence = ' '.join(sentence_parts)\n",
    "                eval_sentence = sentence.split()\n",
    "                score_vector = new_score_vector\n",
    "                score = np.sum(score_vector)\n",
    "            \n",
    "            if printing == True:\n",
    "                if debugging ==True:\n",
    "                    print(\"Sentence number: \" + str(i+1))\n",
    "                    print(sentence)\n",
    "                    if True in (changes>0):\n",
    "                        print(\"Changes vector: \")\n",
    "                        print(changes)\n",
    "                    print(\"Covered vector: \")\n",
    "                    print(covered)\n",
    "                    print(\"Positions vector: \")\n",
    "                    print(positions)\n",
    "                else:\n",
    "                    if i in range(n_iterations-5, n_iterations):\n",
    "                        print(\"Sentence number: \" + str(i+1))\n",
    "                        print(sentence)\n",
    "                        if i == n_iterations-1:\n",
    "                            print('The final sentence got a score of: '+str(score))\n",
    "            scores[i] = score\n",
    "            i +=1\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def sentence_cleaner(sentence, mw, model):\n",
    "    #replacing MW with \"the main word\" and TWs appearing in the sentence with one of their synonyms\n",
    "    sentence = sentence.replace(mw, '.The main word')\n",
    "    \n",
    "    #replacing any TWs appearing in our sentence with some allowed synonym\n",
    "    taboo_words = cg.card_generator(mw, cg.get_gold_probdist(), model)[mw]\n",
    "\n",
    "    spl = np.array(sentence.split())\n",
    "    for tw in taboo_words:\n",
    "        if tw in spl:\n",
    "           #getting synonyms of detected tw\n",
    "            syns = sr.get_synonyms(tw)\n",
    "            #if we have at least one\n",
    "            if len(syns) > 0:\n",
    "                syns = list(syns)\n",
    "                #choose one randomly\n",
    "                choice = np.random.choice(syns)\n",
    "                #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                while (choice in taboo_words or choice != mw) and len(syns) > 0:\n",
    "                    syns = syns.pop(syns.index(choice))\n",
    "                    choice = np.random.choice(syns)\n",
    "                sentence = sentence.replace(tw, choice)\n",
    "                #if all synonyms where taboo words or the mw\n",
    "                if choice in taboo_words or choice == mw:\n",
    "                    hypers = sr.get_hypernyms(tw)\n",
    "                    #if we have at least one\n",
    "                    if len(hypers) > 0:\n",
    "                        hypers = list(hypers)\n",
    "                        #choose one randomly\n",
    "                        choice = np.random.choice(hypers)\n",
    "                        #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                        while (choice in taboo_words or choice != mw) and len(hypers) > 0:\n",
    "                            syns = syns.pop(syns.index(choice))\n",
    "                            choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'Is a type of '+choice)\n",
    "            #in case no synonyms were found\n",
    "            else:\n",
    "                hypers = sr.get_hypernyms(tw)\n",
    "                #if we have at least one\n",
    "                if len(hypers) > 0:\n",
    "                    hypers = list(hypers)\n",
    "                    #choose one randomly\n",
    "                    choice = np.random.choice(hypers)\n",
    "                    #checking that the choosen one it not a taboo word either, or the main word + making sure that it doesn't loop\n",
    "                    while (choice in taboo_words or choice != mw) and len(hypers) > 0:\n",
    "                        syns = syns.pop(syns.index(choice))\n",
    "                        choice = np.random.choice(syns)\n",
    "                        #replacing in order to point the reader to think of this word as a hypernym \n",
    "                        sentence = sentence.replace(tw, choice)\n",
    "                        #if all synonyms where taboo words or the mw\n",
    "                        if choice in taboo_words or choice == mw:\n",
    "                            sentence = sentence.replace(choice, \"ERROR, NO IDEA!\")  #panicking as a real player would.\n",
    "                        else:\n",
    "                            sentence = sentence.replace(choice,'Is a type of '+choice)\n",
    "                else: \n",
    "                    sentence = sentence.replace(tw, \"NO IDEA!\")\n",
    "    sentence = sentence[1:]\n",
    "    return sentence\n",
    "\n",
    "def final_output(mw, model, n_seeds = 3, n_iterations = 10, debugging = False, printing = False):\n",
    "    sentence = description_generator(mw, model, n_seeds, n_iterations, debugging, printing)\n",
    "    output = sentence_cleaner(sentence, mw, model)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'cake' as main word, 3 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number: 1\n",
      "cake means weighing-up innubens bid fragment fia hubs die  cake is bosnia books geeky preexisting shouted pathetic starfish  cake can be found anecdote inline-axis mound tavern prospects\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means -200 charting tony mataconis stream -1206 sand  cake is polli serial up-ended obstacle unknot 533.030 complimenti cake can be found inflexible team inhabited reveal fia\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means .. unknot wart fallen magicians navy-developed somewhe cake is circuits actuator sps horômenos frightened calf fish  cake can be found he keeping innings advise clone\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means deadline knife magicians he submissive unions fia  cake is flattery heavens off-road roll fia uttered die  cake can be found couturier intrapreneur professionally 150,000 u\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means hd terrific bulldozer mflops team magicians centenar cake is mpw towels eurasian -1206 provisions heroin leninist  cake can be found horseshit die nationwide thursday chap\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means unknot beaded mataconis funda playground straightforwa cake is positive respray prefer shoots memorial knife keeper  cake can be found industry oracle magicians billy magicians\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means ~ indicating -200 by-product tony mataconis crapload  cake is lbs abnormalities match dies doorway pastor advocates  cake can be found viewing lobster cd companionship -1206\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means storythat stool ongoing diagonal unknot boys duplicate cake is brôsin scope scooby clumsy cd quadriceps fault  cake can be found postage peculiar bins shoots intrapreneur\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means -200 resettlement based chap cheating ∨ town  cake is largest incomes longlasting olympics separator low-barrie cake can be found nobler unfortunately eurasian -200 earnout\n",
      "[0 0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "tensor(16458) is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a5e7c6c5b2a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cake'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcard_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mfinal_output\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescription_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mdescription_generator\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m    153\u001b[0m                         \u001b[0msentence_parts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                         \u001b[0msentence_parts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_token_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m                 \u001b[1;31m# same logic for the third part.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcovered\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mnext_token_generator\u001b[1;34m(seed, generation_length)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Add predicted word to string and use as next input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpredicted_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_choice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpredicted_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: tensor(16458) is not in list"
     ]
    }
   ],
   "source": [
    "final_output(mw = 'cake', model = card_model, n_seeds=3, n_iterations = 10, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'cake' as main word, 2 seeds, debugging mode on to show covered and position vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number: 1\n",
      "cake means buyer inline-axis influential glorious pleasure bondage honorary brick cheating waterw cake is flattery academic string a. mcmanus chiropractic -200 \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 2\n",
      "cake means decides bureaucratic tastes spas unconnected persons imprisoned resolve resolve chargi cake is flattery disney verse ignoring fia services mataconis \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 3\n",
      "cake means ejb indicating -200 impregnation conveyancing cd rifle hadcrut raisin minors mataconis cake is flattery doable pillar blew dramas -1206 navy-developed \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 4\n",
      "cake means toward unknot guitar slam granite magicians physicists shouted inline-axis tony die  cake is shortly guns jackal department panic gnome 80,000 \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 5\n",
      "cake means brutally inhabited navy-developed introducing unknot mataconis keeping rises decadence cake is fide dig up-ended gradle alton lineage bondage \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 6\n",
      "cake means decides bureaucratic ive blessed unconnected divisions keeping fia charging team unkno cake is megawatt present-day fide samaritan target ellipse mechanism \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 7\n",
      "cake means buyer inline-axis influential magicians he griddle fia are fiddly prospecting tony  cake is flattery à objected-to ginsberg leafless powers indicating \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 8\n",
      "cake means storythat valid inflation intrapreneur factories prefer bulldozer traces inline-axis q cake is griddle ed reduced transform hands clippings non-northwesterners \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 9\n",
      "cake means stream actuallity resolve -1206 -200 libation tony -200 emulator 1975 fringe  cake is unifies backup awareness comps lamb colorado appointed \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 10\n",
      "cake means quoc inhabited clippings heroin usually consumable emphasis evident roux inhabited inf cake is brutally placing drawings blade claude alignment divided \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 11\n",
      "cake means company indicating -200 instrument magicians tony -200 feasible chained fallabout indi cake is largest buildup regained ui percptual 1990 preceding \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 12\n",
      "cake means buyer charting syka -1206 eurasian authorship inhabited parivena fia tastes cameras  cake is calf peer fia die plummet bulldozer vaccine \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 13\n",
      "cake means intruders intrapreneur decadence at δ inline-axis inf chiropractic catheter inhabited  cake is 840 too magicians orchid chiropractic came examining \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 14\n",
      "cake means zero-sum booker importing granite wasnt regent discovered evident inline-axis innubens cake is hated riddle bailey tests flattery wrongful articulation \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 15\n",
      "cake means buyer inline-axis influential -1206 navy-developed assimilate fia harming physicists m cake is stove charged naming peano sentry fia alcoholic \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Sentence number: 16\n",
      "cake means car indicating -200 mids tony -200 feasible inspector terrorism magicians shouted  cake is expression artifacts educating reified steep assert abandoning \n",
      "[0 0]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "tensor(16496) is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3ddd03aebb86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cake'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcard_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mfinal_output\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescription_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mdescription_generator\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m    260\u001b[0m                         \u001b[0msentence_parts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                         \u001b[0msentence_parts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_token_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_parts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                 \u001b[0meval_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mnext_token_generator\u001b[1;34m(seed, generation_length)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Add predicted word to string and use as next input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpredicted_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_choice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpredicted_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: tensor(16496) is not in list"
     ]
    }
   ],
   "source": [
    "final_output(mw = 'cake', model = card_model, n_seeds=2, n_iterations = 20, debugging = True, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'airplane' as main word, 3 seeds, simple printing mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'airplane'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4f8ae8b166c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'airplane'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcard_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mfinal_output\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescription_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mdescription_generator\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_seeds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m#create the first sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0msentence_parts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_token_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' means'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_token_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_token_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' can be found'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_parts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0meval_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mnext_token_generator\u001b[1;34m(seed, generation_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneration_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprime_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprime_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#last two words as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneration_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprime_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprime_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#last two words as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'airplane'"
     ]
    }
   ],
   "source": [
    "final_output(mw = 'airplane', model = card_model, n_seeds=3, n_iterations = 150, debugging = False, printing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Example with 'airplane' as main word, 2 seeds, only final output is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'airplane'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dcc8d8b3cd19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'airplane'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcard_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mfinal_output\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdescription_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprinting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mdescription_generator\u001b[1;34m(mw, model, n_seeds, n_iterations, debugging, printing)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_seeds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m#create the first sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0msentence_parts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_token_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' means'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_token_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_parts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0meval_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36mnext_token_generator\u001b[1;34m(seed, generation_length)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneration_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprime_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprime_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#last two words as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-72870529718d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneration_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprime_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprime_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#last two words as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'airplane'"
     ]
    }
   ],
   "source": [
    "final_output(mw = 'airplane', model = card_model, n_seeds=2, n_iterations = 10, debugging = False, printing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
